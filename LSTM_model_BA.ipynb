{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Developing an LSTM model to predict future stock prices involves several steps. \n",
    "These steps include data preprocessing, model building, hyperparameter tuning, and evaluation. \n",
    "\n",
    "Below, I'll outline the steps and provide code snippets for each part.\n",
    "\n",
    "\n",
    "Check for varying Timeframe (50 Years - 20 Years - 10 years )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to collect and preprocess the historical stock price data. \n",
    "\n",
    "This typically involves normalizing the data and creating sequences for the LSTM model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Deep Learning\" by Ian Goodfellow, Yoshua Bengio, and Aaron Courville"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hypothesis: How the best parameters for these 4 stocks work for 4 tech stocks (AAPL, AMZN, GOOG, MSFT) and see the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>BA</th>\n",
       "      <th>DE</th>\n",
       "      <th>MSI</th>\n",
       "      <th>SPGI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1974-01-02</td>\n",
       "      <td>0.130799</td>\n",
       "      <td>1.094406</td>\n",
       "      <td>2.354811</td>\n",
       "      <td>0.191056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1974-01-03</td>\n",
       "      <td>0.132121</td>\n",
       "      <td>1.148318</td>\n",
       "      <td>2.440219</td>\n",
       "      <td>0.200771</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date        BA        DE       MSI      SPGI\n",
       "0  1974-01-02  0.130799  1.094406  2.354811  0.191056\n",
       "1  1974-01-03  0.132121  1.148318  2.440219  0.200771"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "# Added data since 1974-01-01\n",
    "\n",
    "data = pd.read_csv('stocks_data.csv', index_col = 0)\n",
    "data.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        1974-01-02\n",
       "1        1974-01-03\n",
       "2        1974-01-04\n",
       "3        1974-01-07\n",
       "4        1974-01-08\n",
       "            ...    \n",
       "12604    2023-12-22\n",
       "12605    2023-12-26\n",
       "12606    2023-12-27\n",
       "12607    2023-12-28\n",
       "12608    2023-12-29\n",
       "Name: Date, Length: 12609, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming the data has a 'Date' column and 'Close' price column\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data.set_index('Date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BA</th>\n",
       "      <th>DE</th>\n",
       "      <th>MSI</th>\n",
       "      <th>SPGI</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1974-01-02</th>\n",
       "      <td>0.130799</td>\n",
       "      <td>1.094406</td>\n",
       "      <td>2.354811</td>\n",
       "      <td>0.191056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1974-01-03</th>\n",
       "      <td>0.132121</td>\n",
       "      <td>1.148318</td>\n",
       "      <td>2.440219</td>\n",
       "      <td>0.200771</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  BA        DE       MSI      SPGI\n",
       "Date                                              \n",
       "1974-01-02  0.130799  1.094406  2.354811  0.191056\n",
       "1974-01-03  0.132121  1.148318  2.440219  0.200771"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking for one by one Stocks, starting with BA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature scaling\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_data = scaler.fit_transform(data['BA'].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.84278157e-05],\n",
       "       [2.14995167e-05],\n",
       "       [1.84278157e-05],\n",
       "       ...,\n",
       "       [6.08998348e-01],\n",
       "       [6.04930255e-01],\n",
       "       [6.05650883e-01]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaled_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In time series forecasting, particularly with LSTM models, a \"window\" refers to a sequence of consecutive time points used as input to predict future values. A 60-day window means we are using the stock prices from the past 60 days to predict the stock price on the 61st day. This approach captures temporal dependencies and patterns in the data, which are crucial for making accurate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The choice of a 60-day window (or sequence length) in time series forecasting, particularly for stock price prediction using LSTM models, is based on several considerations:\n",
    "\n",
    "- Historical Trends and Patterns: Stock prices often exhibit trends, cycles, and patterns over time. A 60-day window allows the model to capture these medium-term patterns, such as monthly trends, which are significant for making predictions.\n",
    "\n",
    "- Balance Between Data Availability and Model Complexity: A longer sequence provides more historical context to the model, potentially improving its ability to learn patterns. However, too long a sequence can increase the model complexity and computational requirements, and might also lead to overfitting. A 60-day window strikes a balance by providing sufficient historical data without overwhelming the model.\n",
    "\n",
    "- Empirical Studies and Industry Practice: Many empirical studies and industry practices in financial modeling use a range of 30 to 90 days for sequence lengths. The 60-day window is a common and practical choice within this range.\n",
    "\n",
    "- Market Behavior: Stock markets often show significant changes within a 60-day period due to quarterly earnings reports, economic data releases, and other macroeconomic events. Capturing these dynamics can be beneficial for prediction accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create sequences\n",
    "def create_sequences(data, sequence_length):\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        sequences.append(data[i:i+sequence_length])\n",
    "        labels.append(data[i+sequence_length])\n",
    "    return np.array(sequences), np.array(labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 60  # 60 days look back\n",
    "X, y = create_sequences(scaled_data, sequence_length)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10039"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12609, 4)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2510"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10039"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(units, dropout_rate, learning_rate):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=units, return_sequences=True, input_shape=(X_train.shape[1], 1)))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(LSTM(units=units))\n",
    "    model.add(Dropout(dropout_rate))\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    optimizer = Adam(learning_rate=learning_rate)\n",
    "    model.compile(optimizer=optimizer, loss='mean_squared_error')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the best combination of hyperparameters, we can use Keras Tuner or a custom grid search.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choosing a minimum value of 50 instead of 10 for hp_units likely reflects a balance between computational efficiency and model complexity, as well as practical considerations based on the specific problem. Here's why a minimum value of 50 might be preferred over 10:\n",
    "\n",
    "- Sufficient Model Capacity:\n",
    "\n",
    "A minimum value of 50 units ensures that the model has enough capacity to capture meaningful patterns in the data. In many practical scenarios, 10 units might be insufficient, leading to underfitting where the model fails to learn the underlying data distribution effectively.\n",
    "Avoiding Underfitting:\n",
    "\n",
    "With only 10 units, the model might be too simplistic, especially for complex datasets. This could result in poor performance because the model cannot adequately represent the complexity of the data.\n",
    "Empirical Evidence:\n",
    "\n",
    "Often, hyperparameter ranges are informed by empirical evidence and prior experiments. If past experiments or domain knowledge suggest that configurations with fewer than 50 units generally perform poorly, it makes sense to set the lower bound at 50.\n",
    "\n",
    "- Computational Efficiency:\n",
    "\n",
    "Exploring configurations with very low units (like 10, 20, 30) may not be an efficient use of computational resources if these configurations are likely to perform poorly. Starting at 50 helps focus the search on more promising areas of the hyperparameter space.\n",
    "Practical Experience:\n",
    "\n",
    "Practical experience with similar models or datasets might indicate that starting with 50 units is a reasonable baseline that balances model complexity and training time. It avoids the pitfalls of too small networks while not being overly complex.\n",
    "\n",
    "- Step Size Consideration:\n",
    "\n",
    "Given a step size of 50, starting from 10 would result in a sequence of (10, 60, 110, 160, 210). This is less intuitive and potentially less useful than (50, 100, 150, 200), where each step represents a significant and meaningful increase in model capacity.\n",
    "Search Space Efficiency:\n",
    "\n",
    "Hyperparameter tuning often involves a trade-off between the breadth of the search space and the granularity of the search. By starting at 50 and using a step of 50, you ensure a more focused and manageable search space, improving the likelihood of finding optimal configurations without excessive computational cost.\n",
    "By setting the minimum value to 50, the tuning process starts with a model complexity that is more likely to be effective, avoiding the inefficiency and potential performance issues associated with very small networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 4 Complete [00h 06m 21s]\n",
      "val_loss: 0.00019311531893132874\n",
      "\n",
      "Best val_loss So Far: 5.732391218771227e-05\n",
      "Total elapsed time: 02h 32m 11s\n",
      "\n",
      "Search: Running Trial #5\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "50                |100               |units\n",
      "0.1               |0.1               |dropout_rate\n",
      "0.0001            |0.01              |learning_rate\n",
      "\n",
      "Epoch 1/50\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 46ms/step - loss: 0.0146 - val_loss: 3.1579e-04\n",
      "Epoch 2/50\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 45ms/step - loss: 6.2736e-04 - val_loss: 2.5279e-04\n",
      "Epoch 3/50\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 48ms/step - loss: 5.5764e-04 - val_loss: 2.8740e-04\n",
      "Epoch 4/50\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 46ms/step - loss: 4.8172e-04 - val_loss: 2.1121e-04\n",
      "Epoch 5/50\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 46ms/step - loss: 4.4417e-04 - val_loss: 2.2066e-04\n",
      "Epoch 6/50\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 45ms/step - loss: 4.4622e-04 - val_loss: 1.9145e-04\n",
      "Epoch 7/50\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 45ms/step - loss: 4.2975e-04 - val_loss: 1.8292e-04\n",
      "Epoch 8/50\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 45ms/step - loss: 3.9899e-04 - val_loss: 1.6907e-04\n",
      "Epoch 9/50\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 46ms/step - loss: 3.2836e-04 - val_loss: 1.9170e-04\n",
      "Epoch 10/50\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 44ms/step - loss: 4.0505e-04 - val_loss: 2.0773e-04\n",
      "Epoch 11/50\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 45ms/step - loss: 3.3498e-04 - val_loss: 2.2900e-04\n",
      "Epoch 1/50\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 45ms/step - loss: 0.0221 - val_loss: 2.7108e-04\n",
      "Epoch 2/50\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 45ms/step - loss: 9.1045e-04 - val_loss: 2.3252e-04\n",
      "Epoch 3/50\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 44ms/step - loss: 6.4298e-04 - val_loss: 2.2738e-04\n",
      "Epoch 4/50\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 46ms/step - loss: 5.8877e-04 - val_loss: 2.2534e-04\n",
      "Epoch 5/50\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 44ms/step - loss: 5.2285e-04 - val_loss: 1.9124e-04\n",
      "Epoch 6/50\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 44ms/step - loss: 4.9546e-04 - val_loss: 2.7086e-04\n",
      "Epoch 7/50\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 46ms/step - loss: 4.5504e-04 - val_loss: 1.8325e-04\n",
      "Epoch 8/50\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 46ms/step - loss: 4.2135e-04 - val_loss: 1.9833e-04\n",
      "Epoch 9/50\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 45ms/step - loss: 4.0077e-04 - val_loss: 1.5941e-04\n",
      "Epoch 10/50\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 44ms/step - loss: 4.2117e-04 - val_loss: 1.5969e-04\n",
      "Epoch 11/50\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 46ms/step - loss: 3.8827e-04 - val_loss: 1.5731e-04\n",
      "Epoch 12/50\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 3.5552e-04 - val_loss: 1.4425e-04\n",
      "Epoch 13/50\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 44ms/step - loss: 3.1393e-04 - val_loss: 1.4151e-04\n",
      "Epoch 14/50\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 45ms/step - loss: 3.3395e-04 - val_loss: 1.5369e-04\n",
      "Epoch 15/50\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 43ms/step - loss: 3.1590e-04 - val_loss: 1.2950e-04\n",
      "Epoch 16/50\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 43ms/step - loss: 2.9475e-04 - val_loss: 1.3978e-04\n",
      "Epoch 17/50\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 43ms/step - loss: 3.1610e-04 - val_loss: 1.2192e-04\n",
      "Epoch 18/50\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 43ms/step - loss: 3.1024e-04 - val_loss: 1.3184e-04\n",
      "Epoch 19/50\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 43ms/step - loss: 2.8791e-04 - val_loss: 1.1873e-04\n",
      "Epoch 20/50\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 43ms/step - loss: 2.8031e-04 - val_loss: 1.2848e-04\n",
      "Epoch 21/50\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 43ms/step - loss: 3.1440e-04 - val_loss: 1.1374e-04\n",
      "Epoch 22/50\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 43ms/step - loss: 2.8417e-04 - val_loss: 1.1327e-04\n",
      "Epoch 23/50\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 47ms/step - loss: 2.7728e-04 - val_loss: 1.2495e-04\n",
      "Epoch 24/50\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 51ms/step - loss: 2.8822e-04 - val_loss: 1.0650e-04\n",
      "Epoch 25/50\n",
      "\u001b[1m124/251\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 48ms/step - loss: 2.5605e-04"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 27\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[0;32m     18\u001b[0m tuner \u001b[38;5;241m=\u001b[39m RandomSearch(\n\u001b[0;32m     19\u001b[0m     model_builder,\n\u001b[0;32m     20\u001b[0m     objective\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     24\u001b[0m     project_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstock_price_prediction\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     25\u001b[0m )\n\u001b[1;32m---> 27\u001b[0m \u001b[43mtuner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mEarlyStopping\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmonitor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mval_loss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m best_hps \u001b[38;5;241m=\u001b[39m tuner\u001b[38;5;241m.\u001b[39mget_best_hyperparameters(num_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;124mThe hyperparameter search is complete. The optimal number of units in the LSTM layer is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_hps\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124munits\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;124mThe optimal dropout rate is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_hps\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdropout_rate\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;124mThe optimal learning rate is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_hps\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\SHEKHAR\\.conda\\envs\\aws\\lib\\site-packages\\keras_tuner\\src\\engine\\base_tuner.py:234\u001b[0m, in \u001b[0;36mBaseTuner.search\u001b[1;34m(self, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[0;32m    231\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_trial_begin(trial)\n\u001b[1;32m--> 234\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_run_and_update_trial(trial, \u001b[38;5;241m*\u001b[39mfit_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_kwargs)\n\u001b[0;32m    235\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_trial_end(trial)\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_search_end()\n",
      "File \u001b[1;32mc:\\Users\\SHEKHAR\\.conda\\envs\\aws\\lib\\site-packages\\keras_tuner\\src\\engine\\base_tuner.py:274\u001b[0m, in \u001b[0;36mBaseTuner._try_run_and_update_trial\u001b[1;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[0;32m    272\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_run_and_update_trial\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, \u001b[38;5;241m*\u001b[39mfit_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_kwargs):\n\u001b[0;32m    273\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 274\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_run_and_update_trial(trial, \u001b[38;5;241m*\u001b[39mfit_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_kwargs)\n\u001b[0;32m    275\u001b[0m         trial\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m trial_module\u001b[38;5;241m.\u001b[39mTrialStatus\u001b[38;5;241m.\u001b[39mCOMPLETED\n\u001b[0;32m    276\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\SHEKHAR\\.conda\\envs\\aws\\lib\\site-packages\\keras_tuner\\src\\engine\\base_tuner.py:239\u001b[0m, in \u001b[0;36mBaseTuner._run_and_update_trial\u001b[1;34m(self, trial, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_and_update_trial\u001b[39m(\u001b[38;5;28mself\u001b[39m, trial, \u001b[38;5;241m*\u001b[39mfit_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_kwargs):\n\u001b[1;32m--> 239\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_trial(trial, \u001b[38;5;241m*\u001b[39mfit_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_kwargs)\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moracle\u001b[38;5;241m.\u001b[39mget_trial(trial\u001b[38;5;241m.\u001b[39mtrial_id)\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mexists(\n\u001b[0;32m    241\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moracle\u001b[38;5;241m.\u001b[39mobjective\u001b[38;5;241m.\u001b[39mname\n\u001b[0;32m    242\u001b[0m     ):\n\u001b[0;32m    243\u001b[0m         \u001b[38;5;66;03m# The oracle is updated by calling `self.oracle.update_trial()` in\u001b[39;00m\n\u001b[0;32m    244\u001b[0m         \u001b[38;5;66;03m# `Tuner.run_trial()`. For backward compatibility, we support this\u001b[39;00m\n\u001b[0;32m    245\u001b[0m         \u001b[38;5;66;03m# use case. No further action needed in this case.\u001b[39;00m\n\u001b[0;32m    246\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    247\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe use case of calling \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    248\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`self.oracle.update_trial(trial_id, metrics)` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    254\u001b[0m             stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m    255\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\SHEKHAR\\.conda\\envs\\aws\\lib\\site-packages\\keras_tuner\\src\\engine\\tuner.py:314\u001b[0m, in \u001b[0;36mTuner.run_trial\u001b[1;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[0;32m    312\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mappend(model_checkpoint)\n\u001b[0;32m    313\u001b[0m     copied_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m callbacks\n\u001b[1;32m--> 314\u001b[0m     obj_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_and_fit_model(trial, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcopied_kwargs)\n\u001b[0;32m    316\u001b[0m     histories\u001b[38;5;241m.\u001b[39mappend(obj_value)\n\u001b[0;32m    317\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m histories\n",
      "File \u001b[1;32mc:\\Users\\SHEKHAR\\.conda\\envs\\aws\\lib\\site-packages\\keras_tuner\\src\\engine\\tuner.py:233\u001b[0m, in \u001b[0;36mTuner._build_and_fit_model\u001b[1;34m(self, trial, *args, **kwargs)\u001b[0m\n\u001b[0;32m    231\u001b[0m hp \u001b[38;5;241m=\u001b[39m trial\u001b[38;5;241m.\u001b[39mhyperparameters\n\u001b[0;32m    232\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_build(hp)\n\u001b[1;32m--> 233\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhypermodel\u001b[38;5;241m.\u001b[39mfit(hp, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    235\u001b[0m \u001b[38;5;66;03m# Save the build config for model loading later.\u001b[39;00m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mmulti_backend():\n",
      "File \u001b[1;32mc:\\Users\\SHEKHAR\\.conda\\envs\\aws\\lib\\site-packages\\keras_tuner\\src\\engine\\hypermodel.py:149\u001b[0m, in \u001b[0;36mHyperModel.fit\u001b[1;34m(self, hp, model, *args, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, hp, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    126\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Train the model.\u001b[39;00m\n\u001b[0;32m    127\u001b[0m \n\u001b[0;32m    128\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;124;03m        If return a float, it should be the `objective` value.\u001b[39;00m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 149\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\u001b[38;5;241m.\u001b[39mfit(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\SHEKHAR\\.conda\\envs\\aws\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\SHEKHAR\\.conda\\envs\\aws\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:314\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[0;32m    312\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39menumerate_epoch():\n\u001b[0;32m    313\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m--> 314\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    315\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pythonify_logs(logs)\n\u001b[0;32m    316\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n",
      "File \u001b[1;32mc:\\Users\\SHEKHAR\\.conda\\envs\\aws\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\SHEKHAR\\.conda\\envs\\aws\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\SHEKHAR\\.conda\\envs\\aws\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[0;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\SHEKHAR\\.conda\\envs\\aws\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\SHEKHAR\\.conda\\envs\\aws\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1324\u001b[0m     args,\n\u001b[0;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1326\u001b[0m     executing_eagerly)\n\u001b[0;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\SHEKHAR\\.conda\\envs\\aws\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32mc:\\Users\\SHEKHAR\\.conda\\envs\\aws\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\SHEKHAR\\.conda\\envs\\aws\\lib\\site-packages\\tensorflow\\python\\eager\\context.py:1500\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1498\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1500\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1501\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1503\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1504\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1505\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1506\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1507\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1508\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1509\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1510\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1514\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1515\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\SHEKHAR\\.conda\\envs\\aws\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras_tuner import RandomSearch\n",
    "\n",
    "def model_builder(hp):\n",
    "    model = Sequential()\n",
    "    hp_units = hp.Int('units', min_value=50, max_value=200, step=50)\n",
    "    hp_dropout = hp.Float('dropout_rate', min_value=0.1, max_value=0.5, step=0.1)\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "\n",
    "    model.add(LSTM(units=hp_units, return_sequences=True, input_shape=(X_train.shape[1], 1)))\n",
    "    model.add(Dropout(hp_dropout))\n",
    "    model.add(LSTM(units=hp_units))\n",
    "    model.add(Dropout(hp_dropout))\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=hp_learning_rate), loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "tuner = RandomSearch(\n",
    "    model_builder,\n",
    "    objective='val_loss',\n",
    "    max_trials=5,\n",
    "    executions_per_trial=3,\n",
    "    directory='stock_price_tuning',\n",
    "    project_name='stock_price_prediction'\n",
    ")\n",
    "\n",
    "tuner.search(X_train, y_train, epochs=50, validation_split=0.2, callbacks=[EarlyStopping(monitor='val_loss', patience=3)])\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(f\"\"\"\n",
    "The hyperparameter search is complete. The optimal number of units in the LSTM layer is {best_hps.get('units')}.\n",
    "The optimal dropout rate is {best_hps.get('dropout_rate')}.\n",
    "The optimal learning rate is {best_hps.get('learning_rate')}.\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayesian Optimization is a probabilistic model-based optimization method that is particularly effective for tuning hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 Complete [00h 24m 06s]\n",
      "val_loss: 0.00014155186606027806\n",
      "\n",
      "Best val_loss So Far: 8.370360592380166e-05\n",
      "Total elapsed time: 02h 40m 52s\n",
      "\n",
      "The hyperparameter search is complete. The optimal number of units in the LSTM layer is 100.\n",
      "The optimal dropout rate is 0.1.\n",
      "The optimal learning rate is 0.001.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras_tuner import BayesianOptimization\n",
    "\n",
    "def model_builder(hp):\n",
    "    model = Sequential()\n",
    "    hp_units = hp.Int('units', min_value=50, max_value=200, step=50)\n",
    "    hp_dropout = hp.Float('dropout_rate', min_value=0.1, max_value=0.5, step=0.1)\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "\n",
    "    model.add(LSTM(units=hp_units, return_sequences=True, input_shape=(X_train.shape[1], 1)))\n",
    "    model.add(Dropout(hp_dropout))\n",
    "    model.add(LSTM(units=hp_units))\n",
    "    model.add(Dropout(hp_dropout))\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=hp_learning_rate), loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "tuner = BayesianOptimization(\n",
    "    model_builder,\n",
    "    objective='val_loss',\n",
    "    max_trials=10,\n",
    "    executions_per_trial=3,\n",
    "    directory='stock_price_tuning_bayesian',\n",
    "    project_name='stock_price_prediction'\n",
    ")\n",
    "\n",
    "tuner.search(X_train, y_train, epochs=50, validation_split=0.2, callbacks=[EarlyStopping(monitor='val_loss', patience=3)])\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(f\"\"\"\n",
    "The hyperparameter search is complete. The optimal number of units in the LSTM layer is {best_hps.get('units')}.\n",
    "The optimal dropout rate is {best_hps.get('dropout_rate')}.\n",
    "The optimal learning rate is {best_hps.get('learning_rate')}.\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyperband is an optimization algorithm that uses adaptive resource allocation and early-stopping to find the best hyperparameters quickly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 54 Complete [00h 05m 35s]\n",
      "val_loss: 0.00025037495167149854\n",
      "\n",
      "Best val_loss So Far: 4.937402385015351e-05\n",
      "Total elapsed time: 05h 23m 20s\n",
      "\n",
      "Search: Running Trial #55\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "100               |100               |units\n",
      "0.1               |0.1               |dropout_rate\n",
      "0.0001            |0.001             |learning_rate\n",
      "6                 |50                |tuner/epochs\n",
      "0                 |17                |tuner/initial_epoch\n",
      "2                 |3                 |tuner/bracket\n",
      "0                 |3                 |tuner/round\n",
      "\n",
      "Epoch 1/6\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 137ms/step - loss: 0.0138 - val_loss: 2.7039e-04\n",
      "Epoch 2/6\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 141ms/step - loss: 5.3305e-04 - val_loss: 2.2664e-04\n",
      "Epoch 3/6\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 120ms/step - loss: 4.4237e-04 - val_loss: 2.1036e-04\n",
      "Epoch 4/6\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 115ms/step - loss: 3.9550e-04 - val_loss: 1.9376e-04\n",
      "Epoch 5/6\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 140ms/step - loss: 3.7381e-04 - val_loss: 1.8111e-04\n",
      "Epoch 6/6\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 139ms/step - loss: 3.5080e-04 - val_loss: 2.7294e-04\n",
      "Epoch 1/6\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 118ms/step - loss: 0.0146 - val_loss: 2.5092e-04\n",
      "Epoch 2/6\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 127ms/step - loss: 4.6083e-04 - val_loss: 2.3373e-04\n",
      "Epoch 3/6\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 113ms/step - loss: 4.1438e-04"
     ]
    }
   ],
   "source": [
    "from keras_tuner import Hyperband\n",
    "\n",
    "def model_builder(hp):\n",
    "    model = Sequential()\n",
    "    hp_units = hp.Int('units', min_value=50, max_value=200, step=50)\n",
    "    hp_dropout = hp.Float('dropout_rate', min_value=0.1, max_value=0.5, step=0.1)\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "\n",
    "    model.add(LSTM(units=hp_units, return_sequences=True, input_shape=(X_train.shape[1], 1)))\n",
    "    model.add(Dropout(hp_dropout))\n",
    "    model.add(LSTM(units=hp_units))\n",
    "    model.add(Dropout(hp_dropout))\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=hp_learning_rate), loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "tuner = Hyperband(\n",
    "    model_builder,\n",
    "    objective='val_loss',\n",
    "    max_epochs=50,\n",
    "    executions_per_trial=3,\n",
    "    directory='stock_price_tuning_hyperband',\n",
    "    project_name='stock_price_prediction'\n",
    ")\n",
    "\n",
    "tuner.search(X_train, y_train, epochs=50, validation_split=0.2, callbacks=[EarlyStopping(monitor='val_loss', patience=3)])\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(f\"\"\"\n",
    "The hyperparameter search is complete. The optimal number of units in the LSTM layer is {best_hps.get('units')}.\n",
    "The optimal dropout rate is {best_hps.get('dropout_rate')}.\n",
    "The optimal learning rate is {best_hps.get('learning_rate')}.\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid Search is an exhaustive search method that evaluates all possible combinations of hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SHEKHAR\\.conda\\envs\\aws\\lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Search: Running Trial #1\n",
      "\n",
      "Value             |Best Value So Far |Hyperparameter\n",
      "50                |50                |units\n",
      "0.1               |0.1               |dropout_rate\n",
      "0.01              |0.01              |learning_rate\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\SHEKHAR\\.conda\\envs\\aws\\lib\\site-packages\\keras\\src\\backend\\common\\global_state.py:82: The name tf.reset_default_graph is deprecated. Please use tf.compat.v1.reset_default_graph instead.\n",
      "\n",
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SHEKHAR\\.conda\\envs\\aws\\lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m 68/251\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 38ms/step - loss: 0.0161"
     ]
    }
   ],
   "source": [
    "from keras_tuner import GridSearch\n",
    "\n",
    "def model_builder(hp):\n",
    "    model = Sequential()\n",
    "    hp_units = hp.Int('units', min_value=50, max_value=200, step=50)\n",
    "    hp_dropout = hp.Float('dropout_rate', min_value=0.1, max_value=0.5, step=0.1)\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "\n",
    "    model.add(LSTM(units=hp_units, return_sequences=True, input_shape=(X_train.shape[1], 1)))\n",
    "    model.add(Dropout(hp_dropout))\n",
    "    model.add(LSTM(units=hp_units))\n",
    "    model.add(Dropout(hp_dropout))\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    model.compile(optimizer=Adam(learning_rate=hp_learning_rate), loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "tuner = GridSearch(\n",
    "    model_builder,\n",
    "    objective='val_loss',\n",
    "    max_trials=5,\n",
    "    executions_per_trial=3,\n",
    "    directory='stock_price_tuning_grid',\n",
    "    project_name='stock_price_prediction'\n",
    ")\n",
    "\n",
    "tuner.search(X_train, y_train, epochs=50, validation_split=0.2, callbacks=[EarlyStopping(monitor='val_loss', patience=3)])\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "\n",
    "print(f\"\"\"\n",
    "The hyperparameter search is complete. The optimal number of units in the LSTM layer is {best_hps.get('units')}.\n",
    "The optimal dropout rate is {best_hps.get('dropout_rate')}.\n",
    "The optimal learning rate is {best_hps.get('learning_rate')}.\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build the model with the optimal hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\SHEKHAR\\.conda\\envs\\aws\\lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 74ms/step - loss: 0.0072 - val_loss: 3.7733e-04\n",
      "Epoch 2/50\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 74ms/step - loss: 6.8205e-04 - val_loss: 1.5135e-04\n",
      "Epoch 3/50\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 86ms/step - loss: 7.5720e-04 - val_loss: 3.0886e-04\n",
      "Epoch 4/50\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 81ms/step - loss: 5.6399e-04 - val_loss: 1.5983e-04\n",
      "Epoch 5/50\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 82ms/step - loss: 4.9024e-04 - val_loss: 1.0233e-04\n",
      "Epoch 6/50\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 71ms/step - loss: 4.9874e-04 - val_loss: 2.5507e-04\n",
      "Epoch 7/50\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 70ms/step - loss: 6.4332e-04 - val_loss: 5.3823e-05\n",
      "Epoch 8/50\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 73ms/step - loss: 5.2622e-04 - val_loss: 7.4072e-05\n",
      "Epoch 9/50\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 68ms/step - loss: 4.4472e-04 - val_loss: 1.7008e-04\n",
      "Epoch 10/50\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 76ms/step - loss: 5.4639e-04 - val_loss: 0.0011\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 9.0132e-04\n",
      "Test Loss: 0.0008998233242891729\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Build the model with the optimal hyperparameters\n",
    "model = build_model(best_hps.get('units'), best_hps.get('dropout_rate'), best_hps.get('learning_rate'))\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2, callbacks=[EarlyStopping(monitor='val_loss', patience=3)])\n",
    "\n",
    "# Evaluate the model\n",
    "loss = model.evaluate(X_test, y_test)\n",
    "print(f'Test Loss: {loss}')\n",
    "\n",
    "# Plot training history\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='validation')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring Different Sequence Lengths\n",
    "\n",
    "\n",
    "While 60 days is a reasonable starting point, it's essential to experiment with different sequence lengths to determine the optimal window for your specific dataset and model. Here's how you can do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(data, sequence_length):\n",
    "    sequences = []\n",
    "    labels = []\n",
    "    for i in range(len(data) - sequence_length):\n",
    "        sequences.append(data[i:i+sequence_length])\n",
    "        labels.append(data[i+sequence_length])\n",
    "    return np.array(sequences), np.array(labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment with Different Sequence Lengths\n",
    "You can loop through different sequence lengths to find the best performing one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 32ms/step - loss: 0.0038 - val_loss: 2.3653e-04\n",
      "Epoch 2/50\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 26ms/step - loss: 7.5396e-04 - val_loss: 8.2970e-04\n",
      "Epoch 3/50\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 27ms/step - loss: 6.9557e-04 - val_loss: 1.1085e-04\n",
      "Epoch 4/50\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 26ms/step - loss: 7.4271e-04 - val_loss: 6.7913e-04\n",
      "Epoch 5/50\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 40ms/step - loss: 6.7993e-04 - val_loss: 8.8390e-05\n",
      "Epoch 6/50\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 43ms/step - loss: 5.6105e-04 - val_loss: 5.9823e-05\n",
      "Epoch 7/50\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 41ms/step - loss: 5.5899e-04 - val_loss: 7.5940e-04\n",
      "Epoch 8/50\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 5.8496e-04 - val_loss: 5.5559e-04\n",
      "Epoch 9/50\n",
      "\u001b[1m252/252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 39ms/step - loss: 6.7450e-04 - val_loss: 8.0509e-05\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - loss: 6.2607e-05\n",
      "Sequence Length: 30, Test Loss: 6.713426409987733e-05\n",
      "Epoch 1/50\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 83ms/step - loss: 0.0050 - val_loss: 7.7491e-04\n",
      "Epoch 2/50\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 74ms/step - loss: 7.0755e-04 - val_loss: 3.3276e-04\n",
      "Epoch 3/50\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 72ms/step - loss: 6.8839e-04 - val_loss: 9.3357e-05\n",
      "Epoch 4/50\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 69ms/step - loss: 5.6899e-04 - val_loss: 1.6857e-04\n",
      "Epoch 5/50\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 74ms/step - loss: 6.1082e-04 - val_loss: 1.3228e-04\n",
      "Epoch 6/50\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 74ms/step - loss: 5.2556e-04 - val_loss: 4.9406e-04\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - loss: 3.9606e-04\n",
      "Sequence Length: 60, Test Loss: 0.0003984233771916479\n",
      "Epoch 1/50\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 100ms/step - loss: 0.0076 - val_loss: 3.6742e-04\n",
      "Epoch 2/50\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 102ms/step - loss: 7.6389e-04 - val_loss: 4.0810e-04\n",
      "Epoch 3/50\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 105ms/step - loss: 6.4896e-04 - val_loss: 2.0193e-04\n",
      "Epoch 4/50\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 107ms/step - loss: 5.6712e-04 - val_loss: 2.3531e-04\n",
      "Epoch 5/50\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 111ms/step - loss: 6.2879e-04 - val_loss: 1.2658e-04\n",
      "Epoch 6/50\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 111ms/step - loss: 5.5032e-04 - val_loss: 2.6438e-04\n",
      "Epoch 7/50\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 116ms/step - loss: 4.4725e-04 - val_loss: 5.5615e-04\n",
      "Epoch 8/50\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 105ms/step - loss: 6.6766e-04 - val_loss: 5.8519e-05\n",
      "Epoch 9/50\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 114ms/step - loss: 4.6931e-04 - val_loss: 1.6589e-04\n",
      "Epoch 10/50\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 131ms/step - loss: 4.8831e-04 - val_loss: 1.2582e-04\n",
      "Epoch 11/50\n",
      "\u001b[1m251/251\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 121ms/step - loss: 5.4668e-04 - val_loss: 3.4422e-04\n",
      "\u001b[1m79/79\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 40ms/step - loss: 3.3042e-04\n",
      "Sequence Length: 90, Test Loss: 0.0003241041558794677\n",
      "Epoch 1/50\n",
      "\u001b[1m249/249\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 251ms/step - loss: 0.0048 - val_loss: 1.4543e-04\n",
      "Epoch 2/50\n",
      "\u001b[1m249/249\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m64s\u001b[0m 255ms/step - loss: 5.2860e-04 - val_loss: 1.3580e-04\n",
      "Epoch 3/50\n",
      "\u001b[1m249/249\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 208ms/step - loss: 6.1835e-04 - val_loss: 2.6960e-04\n",
      "Epoch 4/50\n",
      "\u001b[1m249/249\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 214ms/step - loss: 5.6118e-04 - val_loss: 3.7625e-04\n",
      "Epoch 5/50\n",
      "\u001b[1m249/249\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 218ms/step - loss: 6.2791e-04 - val_loss: 1.3778e-04\n",
      "\u001b[1m78/78\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 71ms/step - loss: 1.1070e-04\n",
      "Sequence Length: 180, Test Loss: 0.00011436480417614803\n",
      "Epoch 1/50\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m110s\u001b[0m 423ms/step - loss: 0.0039 - val_loss: 7.5335e-04\n",
      "Epoch 2/50\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m92s\u001b[0m 374ms/step - loss: 8.3563e-04 - val_loss: 3.3695e-04\n",
      "Epoch 3/50\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m94s\u001b[0m 386ms/step - loss: 7.0939e-04 - val_loss: 1.4898e-04\n",
      "Epoch 4/50\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 403ms/step - loss: 6.2152e-04 - val_loss: 7.3645e-05\n",
      "Epoch 5/50\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 368ms/step - loss: 5.3189e-04 - val_loss: 1.4602e-04\n",
      "Epoch 6/50\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 394ms/step - loss: 5.4637e-04 - val_loss: 2.1974e-04\n",
      "Epoch 7/50\n",
      "\u001b[1m245/245\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m95s\u001b[0m 386ms/step - loss: 7.0854e-04 - val_loss: 2.4974e-04\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 130ms/step - loss: 2.3471e-04\n",
      "Sequence Length: 360, Test Loss: 0.00024939104332588613\n",
      "Best Sequence Length: 30\n"
     ]
    }
   ],
   "source": [
    "sequence_lengths = [30, 60, 90, 180, 360]\n",
    "results = {}\n",
    "\n",
    "for seq_len in sequence_lengths:\n",
    "    X, y = create_sequences(scaled_data, seq_len)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    model = build_model(best_hps.get('units'), best_hps.get('dropout_rate'), best_hps.get('learning_rate'))\n",
    "    history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2, callbacks=[EarlyStopping(monitor='val_loss', patience=3)])\n",
    "    \n",
    "    loss = model.evaluate(X_test, y_test)\n",
    "    results[seq_len] = loss\n",
    "    print(f'Sequence Length: {seq_len}, Test Loss: {loss}')\n",
    "\n",
    "# Find the best sequence length\n",
    "best_seq_len = min(results, key=results.get)\n",
    "print(f'Best Sequence Length: {best_seq_len}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sequence Length: 90, Test Loss: 0.00033690148848108947\n",
    "Best Sequence Length: 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aws",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
