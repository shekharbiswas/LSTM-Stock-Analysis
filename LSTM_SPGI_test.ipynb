{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1h2cxODjgJy"
      },
      "source": [
        "# **Detailed Analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eu79nL0XsKPv"
      },
      "source": [
        "( The description of figure will change- it is just an sample to mention the concepts)\n",
        "\n",
        "- Best Configuration\n",
        "- Best Configuration: 30 years, batch size 32, epochs 80\n",
        "- MAE: 3.2587\n",
        "\n",
        "\n",
        "- Training Loss: 2.3098\n",
        "- Validation Loss: 34.2010\n",
        "\n",
        "**Observations and Implications**\n",
        "\n",
        "- **Training Loss:**\n",
        "\n",
        "Lowest Training Loss: Generally observed with configurations involving 30 years of data and higher epochs (e.g., epochs 60 or 80).\n",
        "Trends: Training loss decreases as the number of epochs increases, which indicates that the model is fitting the training data better with more training time. However, very high training loss values in configurations with fewer epochs or different batch sizes suggest that the model may not have enough time or capacity to learn effectively.\n",
        "Validation Loss:\n",
        "\n",
        "- **Lowest Validation Loss:**\n",
        "\n",
        "Achieved with the configuration of 30 years, batch size 32, and 80 epochs, which is the same as the configuration with the best MAE.\n",
        "\n",
        "- **Trends:** Validation loss also tends to decrease with more epochs up to a point, showing that the model generalizes better with more training. However, the validation loss does not decrease as much in some cases where the training loss is very low. This could suggest overfitting, where the model performs well on training data but less well on unseen data.\n",
        "\n",
        "- **Comparison of Training Loss and Validation Loss:**\n",
        "\n",
        "- **Overfitting:** When training loss is significantly lower than validation loss, it often indicates overfitting. The model learns to perform well on the training data but struggles with new, unseen data. For example, in the configuration with 30 years of data and a batch size of 64, the training loss is quite low, but the validation loss is relatively higher, suggesting overfitting.\n",
        "\n",
        "- **Generalization:** The best configuration (30 years, batch size 32, epochs 80) shows relatively balanced training and validation losses, indicating good generalization. The validation loss is higher than the training loss but not excessively so, which is a positive sign.\n",
        "\n",
        "**Summary**\n",
        "\n",
        "\n",
        "- **Best Overall Performance:**\n",
        "\n",
        "- The configuration with 30 years of data, batch size 32, and 80 epochs is the best based on MAE, training loss, and validation loss. This setup provides a good balance between fitting the training data and generalizing to unseen data.\n",
        "Training vs. Validation Loss:\n",
        "\n",
        "- A lower training loss compared to validation loss suggests that while the model fits the training data well, there might be some overfitting. The goal is to minimize both losses and ensure that they are as close as possible, which indicates the model's ability to generalize.\n",
        "Practical Recommendations:\n",
        "\n",
        "- Monitor Overfitting: Keep an eye on the gap between training and validation loss. Consider regularization techniques or early stopping to mitigate overfitting if necessary.\n",
        "\n",
        "- Continue Experimenting: Further experiment with different configurations or techniques (e.g., more data, different architectures) if we aim to improve performance even more.\n",
        "\n",
        "\n",
        "- By considering both training and validation losses along with MAE, we can get a more comprehensive understanding of our model’s performance and make more informed decisions about adjustments and improvements."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-OplbrRcapY"
      },
      "source": [
        "![image.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2aRtOlQfstm"
      },
      "source": [
        "We included a parameter to check for overfitting in the code by monitoring the training and validation losses during the model training process. Overfitting occurs when the model performs well on the training data but poorly on the validation data, indicating that it has learned the training data too well (including noise) and is not generalizing to unseen data.\n",
        "\n",
        "Steps :\n",
        "\n",
        "- Split the Training Data: Split the training data into training and validation sets.\n",
        "- Track Training and Validation Losses: Use these sets to track the training and validation losses during the model fitting.\n",
        "- Early Stopping (Optional): Not implemented as it is not required and can alter the actual results by stopping early."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltwjP-jDc7kJ"
      },
      "source": [
        "## Random Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 412
        },
        "id": "tUdAslA2jSRS",
        "outputId": "ed7a4bf1-a7a3-46dc-ceda-c4f5171b0aa2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[*********************100%%**********************]  4 of 4 completed\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2013-12-31 00:00:00 2023-12-31 00:00:00\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\SHEKHAR\\.conda\\envs\\aws\\lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 266ms/step\n",
            "Training with 10 years, batch_size 32, epochs 20 - MAE: 205.46692572699652\n",
            "Training Loss: 34.897178649902344, Validation Loss: 109.51943969726562\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\SHEKHAR\\.conda\\envs\\aws\\lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 130\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_size \u001b[38;5;129;01min\u001b[39;00m batch_sizes:\n\u001b[0;32m    129\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m epochs \u001b[38;5;129;01min\u001b[39;00m epochs_list:\n\u001b[1;32m--> 130\u001b[0m         mae, predictions, train_loss, val_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_and_evaluate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    131\u001b[0m \u001b[43m            \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    133\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m mae \u001b[38;5;241m<\u001b[39m best_mae:\n\u001b[0;32m    134\u001b[0m             best_mae \u001b[38;5;241m=\u001b[39m mae\n",
            "Cell \u001b[1;32mIn[1], line 67\u001b[0m, in \u001b[0;36mtrain_and_evaluate\u001b[1;34m(X_train, y_train, X_val, y_val, X_test, y_test, hyperparams, epochs, batch_size)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# Adding EarlyStopping callback to prevent overfitting\u001b[39;00m\n\u001b[0;32m     65\u001b[0m early_stopping \u001b[38;5;241m=\u001b[39m EarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, restore_best_weights\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 67\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\n\u001b[0;32m     73\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     75\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     76\u001b[0m val_loss \u001b[38;5;241m=\u001b[39m history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
            "File \u001b[1;32mc:\\Users\\SHEKHAR\\.conda\\envs\\aws\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
            "File \u001b[1;32mc:\\Users\\SHEKHAR\\.conda\\envs\\aws\\lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:314\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[0;32m    312\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39menumerate_epoch():\n\u001b[0;32m    313\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[1;32m--> 314\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    315\u001b[0m     logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pythonify_logs(logs)\n\u001b[0;32m    316\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n",
            "File \u001b[1;32mc:\\Users\\SHEKHAR\\.conda\\envs\\aws\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
            "File \u001b[1;32mc:\\Users\\SHEKHAR\\.conda\\envs\\aws\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
            "File \u001b[1;32mc:\\Users\\SHEKHAR\\.conda\\envs\\aws\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:878\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    875\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 878\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[0;32m    880\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    881\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_created_variables:\n\u001b[0;32m    882\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating variables on a non-first call to a function\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    883\u001b[0m                    \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m decorated with tf.function.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[1;32mc:\\Users\\SHEKHAR\\.conda\\envs\\aws\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32mc:\\Users\\SHEKHAR\\.conda\\envs\\aws\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1324\u001b[0m     args,\n\u001b[0;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1326\u001b[0m     executing_eagerly)\n\u001b[0;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
            "File \u001b[1;32mc:\\Users\\SHEKHAR\\.conda\\envs\\aws\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
            "File \u001b[1;32mc:\\Users\\SHEKHAR\\.conda\\envs\\aws\\lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
            "File \u001b[1;32mc:\\Users\\SHEKHAR\\.conda\\envs\\aws\\lib\\site-packages\\tensorflow\\python\\eager\\context.py:1500\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1498\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1500\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1501\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1503\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1504\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1505\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1506\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1507\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1508\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1509\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1510\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1514\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1515\u001b[0m   )\n",
            "File \u001b[1;32mc:\\Users\\SHEKHAR\\.conda\\envs\\aws\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Include overfitting parameter\n",
        "# No EARLY STOP\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "##########################################################################\n",
        "# Configuration for the LSTM model (Random Search - ******IMPORTANT******)\n",
        "##########################################################################\n",
        "config = {'units': 50, 'dropout': 0.1, 'learning_rate': 0.01}\n",
        "\n",
        "\n",
        "\n",
        "# Download the data\n",
        "start_date = '1974-01-01'\n",
        "end_date = '2024-08-10'\n",
        "all_stocks = ['BA', 'MSI', 'DE', 'SPGI']\n",
        "df = yf.download(all_stocks, start=start_date, end=end_date)['Adj Close']\n",
        "df = df.reset_index()\n",
        "\n",
        "# Prepare the BA stock data\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "df.set_index('Date', inplace=True)\n",
        "df = df['SPGI']\n",
        "\n",
        "# Preprocess Data\n",
        "def preprocess_data(df, window_size):\n",
        "    scaled_data = df.values.reshape(-1, 1)\n",
        "\n",
        "    X, y, dates = [], [], []\n",
        "    for i in range(window_size, len(df)):\n",
        "        X.append(scaled_data[i-window_size:i, 0])\n",
        "        y.append(scaled_data[i, 0])\n",
        "        dates.append(df.index[i])  # Collect dates for plotting\n",
        "\n",
        "    X, y = np.array(X), np.array(y)\n",
        "    X = np.reshape(X, (X.shape[0], X.shape[1], 1))  # Reshape for LSTM [samples, time steps, features]\n",
        "\n",
        "    return X, y, None, dates\n",
        "\n",
        "# Define LSTM Model\n",
        "def create_lstm_model(units, dropout, learning_rate):\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(units=units, return_sequences=False, input_shape=(90, 1)))\n",
        "    model.add(Dropout(dropout))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_absolute_error')\n",
        "    return model\n",
        "\n",
        "# Train and evaluate each model configuration\n",
        "def train_and_evaluate(X_train, y_train, X_val, y_val, X_test, y_test, hyperparams, epochs, batch_size):\n",
        "        model = create_lstm_model(hyperparams['units'], hyperparams['dropout'], hyperparams['learning_rate'])\n",
        "\n",
        "        # Adding EarlyStopping callback to prevent overfitting\n",
        "        early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "        history = model.fit(\n",
        "            X_train, y_train,\n",
        "            validation_data=(X_val, y_val),\n",
        "            epochs=epochs,\n",
        "            batch_size=batch_size,\n",
        "            verbose=0\n",
        "        )\n",
        "\n",
        "        train_loss = history.history['loss']\n",
        "        val_loss = history.history['val_loss']\n",
        "        predictions = model.predict(X_test)\n",
        "\n",
        "        mae = mean_absolute_error(y_test, predictions)\n",
        "\n",
        "        return mae, predictions, train_loss, val_loss\n",
        "\n",
        "# Possible values for epochs and batch size\n",
        "batch_sizes = [32, 64]\n",
        "# batch_sizes = [32, 64, 128, 256]\n",
        "# epochs_list = [20, 40, 50, 75]\n",
        "epochs_list = [20, 40, 60, 80]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Varying training periods\n",
        "#training_years = [50, 40, 30, 20, 10]\n",
        "\n",
        "training_years = [10,5, 4, 3, 2]\n",
        "results = {}\n",
        "\n",
        "# Test data (2024)\n",
        "test_start = '2024-01-01'\n",
        "test_end = '2024-08-10'\n",
        "df_test = df[test_start:test_end]\n",
        "\n",
        "# Grid Search for best batch size and epochs\n",
        "best_config = None\n",
        "best_mae = float('inf')\n",
        "best_predictions = None\n",
        "best_dates = None\n",
        "\n",
        "for years in training_years:\n",
        "    train_end = pd.to_datetime('2023-12-31')\n",
        "    train_start = train_end - pd.DateOffset(years=years)\n",
        "    print(train_start, train_end)\n",
        "    df_train = df[train_start:train_end]\n",
        "\n",
        "\n",
        "    # Split into training and validation sets (80/20 split)\n",
        "    val_size = int(len(df_train) * 0.2)\n",
        "    df_val = df_train[-val_size:]\n",
        "    df_train = df_train[:-val_size]\n",
        "\n",
        "    window_size = 90\n",
        "    X_train, y_train, scaler, train_dates = preprocess_data(df_train, window_size)\n",
        "    X_val, y_val, _, val_dates = preprocess_data(df_val, window_size)\n",
        "    X_test, y_test, _, test_dates = preprocess_data(df_test, window_size)\n",
        "\n",
        "    for batch_size in batch_sizes:\n",
        "        for epochs in epochs_list:\n",
        "            mae, predictions, train_loss, val_loss = train_and_evaluate(\n",
        "                X_train, y_train, X_val, y_val, X_test, y_test, config, epochs, batch_size)\n",
        "\n",
        "            if mae < best_mae:\n",
        "                best_mae = mae\n",
        "                best_config = {'years': years, 'batch_size': batch_size, 'epochs': epochs}\n",
        "                best_predictions = predictions\n",
        "                best_dates = test_dates\n",
        "            print(f\"Training with {years} years, batch_size {batch_size}, epochs {epochs} - MAE: {mae}\")\n",
        "            print(f\"Training Loss: {train_loss[-1]}, Validation Loss: {val_loss[-1]}\")\n",
        "\n",
        "print(f\"Best Configuration - Years: {best_config['years']}, Batch Size: {best_config['batch_size']}, Epochs: {best_config['epochs']} - MAE: {best_mae}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dr1B9zj9Uhbn"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YoLTpJn1gzzU",
        "outputId": "7086a597-00c8-47ec-b20b-90bb076d0f57"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[*********************100%***********************]  4 of 4 completed"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2023-12-31 00:00:00\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "Cannot compare tz-naive and tz-aware datetime-like objects",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-ffb0c176820f>\u001b[0m in \u001b[0;36m<cell line: 86>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_end\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0mtrain_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_end\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDateOffset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myears\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbest_years\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m \u001b[0mdf_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_start\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtrain_end\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0mdf_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_start\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtest_end\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1063\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1064\u001b[0m             \u001b[0;31m# Do slice check before somewhat-costly is_bool_indexer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1065\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_slice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1066\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1067\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_bool_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_getitem_slice\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4282\u001b[0m         \u001b[0;31m# _convert_slice_indexer to determine if this slice is positional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4283\u001b[0m         \u001b[0;31m#  or label based, and if the latter, convert to positional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4284\u001b[0;31m         \u001b[0mslobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_slice_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"getitem\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4285\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4286\u001b[0m             \u001b[0;31m# reachable with DatetimeIndex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_convert_slice_indexer\u001b[0;34m(self, key, kind)\u001b[0m\n\u001b[1;32m   4284\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4285\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4286\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslice_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4288\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/datetimes.py\u001b[0m in \u001b[0;36mslice_indexer\u001b[0;34m(self, start, end, step)\u001b[0m\n\u001b[1;32m    681\u001b[0m             \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_monotonic_increasing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m         ):\n\u001b[0;32m--> 683\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mIndex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslice_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    684\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m         \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mslice_indexer\u001b[0;34m(self, start, end, step)\u001b[0m\n\u001b[1;32m   6600\u001b[0m         \u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6601\u001b[0m         \"\"\"\n\u001b[0;32m-> 6602\u001b[0;31m         \u001b[0mstart_slice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_slice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslice_locs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6604\u001b[0m         \u001b[0;31m# return a slice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mslice_locs\u001b[0;34m(self, start, end, step)\u001b[0m\n\u001b[1;32m   6817\u001b[0m         \u001b[0mstart_slice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6818\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6819\u001b[0;31m             \u001b[0mstart_slice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_slice_bound\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"left\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6820\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstart_slice\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6821\u001b[0m             \u001b[0mstart_slice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_slice_bound\u001b[0;34m(self, label, side)\u001b[0m\n\u001b[1;32m   6732\u001b[0m         \u001b[0;31m# For datetime indices label may be a string that has to be converted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6733\u001b[0m         \u001b[0;31m# to datetime boundary according to its resolution.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6734\u001b[0;31m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_slice_bound\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mside\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6736\u001b[0m         \u001b[0;31m# we need to look up the label\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/datetimes.py\u001b[0m in \u001b[0;36m_maybe_cast_slice_bound\u001b[0;34m(self, label, side)\u001b[0m\n\u001b[1;32m    642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_slice_bound\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mside\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_assert_tzawareness_compat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mTimestamp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/arrays/datetimes.py\u001b[0m in \u001b[0;36m_assert_tzawareness_compat\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    772\u001b[0m                 )\n\u001b[1;32m    773\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mother_tz\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 774\u001b[0;31m             raise TypeError(\n\u001b[0m\u001b[1;32m    775\u001b[0m                 \u001b[0;34m\"Cannot compare tz-naive and tz-aware datetime-like objects\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    776\u001b[0m             )\n",
            "\u001b[0;31mTypeError\u001b[0m: Cannot compare tz-naive and tz-aware datetime-like objects"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import plotly.graph_objects as go\n",
        "##################################\n",
        "# Define the best configuration\n",
        "#################################\n",
        "# IMPORTANT\n",
        "#################################\n",
        "# Best Configuration - Years: 10, Batch Size: 32, Epochs: 60 - MAE: 3.2200385199652777\n",
        "\n",
        "\n",
        "best_years = 10\n",
        "best_batch_size = 32\n",
        "best_epochs = 80\n",
        "\n",
        "##########################################################################\n",
        "# Configuration for the LSTM model (Random Search - ******IMPORTANT******)\n",
        "##########################################################################\n",
        "config = {'units': 50, 'dropout': 0.1, 'learning_rate': 0.01}\n",
        "\n",
        "\n",
        "# Download the data\n",
        "start_date = '1974-01-01'\n",
        "end_date = '2024-08-10'\n",
        "all_stocks = ['BA', 'MSI', 'DE', 'SPGI']\n",
        "df = yf.download(all_stocks, start=start_date, end=end_date)['Adj Close']\n",
        "df = df.reset_index()\n",
        "\n",
        "test_start = '2024-01-01'\n",
        "test_end = '2024-08-10'\n",
        "\n",
        "# Prepare the BA stock data\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "df.set_index('Date', inplace=True)\n",
        "df = df['SPGI']\n",
        "\n",
        "# Preprocess Data\n",
        "def preprocess_data(df, window_size):\n",
        "    #scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    #scaled_data = scaler.fit_transform(df.values.reshape(-1, 1))\n",
        "\n",
        "    scaled_data = df.values.reshape(-1, 1)\n",
        "\n",
        "    X, y, dates = [], [], []\n",
        "    for i in range(window_size, len(df)):\n",
        "        X.append(scaled_data[i-window_size:i, 0])\n",
        "        y.append(scaled_data[i, 0])\n",
        "        dates.append(df.index[i])  # Collect dates for plotting\n",
        "\n",
        "    X, y = np.array(X), np.array(y)\n",
        "    X = np.reshape(X, (X.shape[0], X.shape[1], 1))  # Reshape for LSTM [samples, time steps, features]\n",
        "\n",
        "    return X, y, None, dates\n",
        "\n",
        "# Define LSTM Model\n",
        "def create_lstm_model(units, dropout, learning_rate):\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(units=units, return_sequences=False, input_shape=(90, 1)))\n",
        "    model.add(Dropout(dropout))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_absolute_error')\n",
        "    return model\n",
        "\n",
        "# Train and evaluate each model configuration\n",
        "def train_and_evaluate(X_train, y_train, X_test, y_test, hyperparams, epochs, batch_size):\n",
        "        model = create_lstm_model(hyperparams['units'], hyperparams['dropout'], hyperparams['learning_rate'])\n",
        "        model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=0)\n",
        "        predictions = model.predict(X_test)\n",
        "        mae = mean_absolute_error(y_test, predictions)\n",
        "        return mae, predictions\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Prepare the data using the best configuration\n",
        "train_end = pd.to_datetime('2023-12-31').tz_localize(None)\n",
        "print(train_end)\n",
        "train_start = train_end - pd.DateOffset(years=best_years)\n",
        "df_train = df[train_start:train_end]\n",
        "df_test = df[test_start:test_end]\n",
        "\n",
        "# Prepare data\n",
        "window_size = 90\n",
        "X_train, y_train, scaler, train_dates = preprocess_data(df_train, window_size)\n",
        "X_test, y_test, _, test_dates = preprocess_data(df_test, window_size)\n",
        "\n",
        "# Train the model using the best configuration\n",
        "model = create_lstm_model(units=150, dropout=0.2, learning_rate=0.001)\n",
        "model.fit(X_train, y_train, epochs=best_epochs, batch_size=best_batch_size, verbose=0)\n",
        "\n",
        "# Make predictions on the test data\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "# Inverse transform the predictions and true values using the scaler\n",
        "#predictions = scaler.inverse_transform(predictions)\n",
        "#y_test = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
        "\n",
        "# Calculate MAE\n",
        "mae = mean_absolute_error(y_test, predictions)\n",
        "print(f\"MAE on Test Data: {mae}\")\n",
        "\n",
        "# Create a Plotly plot to show the predictions and actual values\n",
        "fig = go.Figure()\n",
        "\n",
        "# Plot actual values\n",
        "fig.add_trace(go.Scatter(x=test_dates, y=y_test.flatten(), mode='lines', name='Actual Values', line=dict(color='black', dash='dash')))\n",
        "\n",
        "# Plot predictions\n",
        "fig.add_trace(go.Scatter(x=test_dates, y=predictions.flatten(), mode='lines', name=f'Predictions (MAE: {mae:.4f})', line=dict(color='blue')))\n",
        "\n",
        "# Add layout details\n",
        "fig.update_layout(\n",
        "    title=\"LSTM Model Predictions vs Actual Values\",\n",
        "    xaxis_title=\"Date\",\n",
        "    yaxis_title=\"Stock Price\",\n",
        "    legend_title=\"Legend\",\n",
        "    hovermode=\"x unified\"\n",
        ")\n",
        "\n",
        "# Show the plot\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1g1Yt7krioT"
      },
      "source": [
        "## Bayesian Optimization\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x1Rlr3QocjB7",
        "outputId": "4cd6d07f-b480-42c3-ea4a-ddaa82aa52b2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[*********************100%%**********************]  4 of 4 completed\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2/2 [==============================] - 0s 4ms/step\n",
            "Training with 50 years, batch_size 32, epochs 20 - MAE: 389.41772909013054\n",
            "Training Loss: 0.6419885754585266, Validation Loss: 164.61538696289062\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Training with 50 years, batch_size 32, epochs 40 - MAE: 395.74134947761655\n",
            "Training Loss: 0.6116853952407837, Validation Loss: 168.5093994140625\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Training with 50 years, batch_size 32, epochs 60 - MAE: 406.5645722888765\n",
            "Training Loss: 1.0126392841339111, Validation Loss: 181.79661560058594\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Training with 50 years, batch_size 32, epochs 80 - MAE: 403.0495383853004\n",
            "Training Loss: 0.8981713056564331, Validation Loss: 174.24710083007812\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Training with 50 years, batch_size 64, epochs 20 - MAE: 397.14037649972096\n",
            "Training Loss: 0.6059040427207947, Validation Loss: 168.76734924316406\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "Training with 50 years, batch_size 64, epochs 40 - MAE: 394.40579756479417\n",
            "Training Loss: 0.515987753868103, Validation Loss: 166.31881713867188\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Training with 50 years, batch_size 64, epochs 60 - MAE: 390.0322789994497\n",
            "Training Loss: 0.6210677027702332, Validation Loss: 161.98727416992188\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Training with 50 years, batch_size 64, epochs 80 - MAE: 396.21689751034694\n",
            "Training Loss: 0.8549878001213074, Validation Loss: 171.85374450683594\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Training with 40 years, batch_size 32, epochs 20 - MAE: 366.79484315902465\n",
            "Training Loss: 0.9517552852630615, Validation Loss: 175.5392303466797\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Training with 40 years, batch_size 32, epochs 40 - MAE: 359.30267224993025\n",
            "Training Loss: 1.1365121603012085, Validation Loss: 170.59521484375\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Training with 40 years, batch_size 32, epochs 60 - MAE: 363.4895794580853\n",
            "Training Loss: 1.420225977897644, Validation Loss: 174.3737030029297\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Training with 40 years, batch_size 32, epochs 80 - MAE: 367.9319603329613\n",
            "Training Loss: 1.051198959350586, Validation Loss: 177.29513549804688\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Training with 40 years, batch_size 64, epochs 20 - MAE: 362.1681937565879\n",
            "Training Loss: 1.035231113433838, Validation Loss: 171.88125610351562\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Training with 40 years, batch_size 64, epochs 40 - MAE: 352.68597690642827\n",
            "Training Loss: 0.9352309703826904, Validation Loss: 163.12644958496094\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Training with 40 years, batch_size 64, epochs 60 - MAE: 365.33522348555306\n",
            "Training Loss: 0.8181678056716919, Validation Loss: 176.27783203125\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Training with 40 years, batch_size 64, epochs 80 - MAE: 376.3848321097238\n",
            "Training Loss: 0.8944905996322632, Validation Loss: 182.59820556640625\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Training with 30 years, batch_size 32, epochs 20 - MAE: 311.00992693219865\n",
            "Training Loss: 1.8897087574005127, Validation Loss: 167.3485565185547\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Training with 30 years, batch_size 32, epochs 40 - MAE: 306.664065164233\n",
            "Training Loss: 2.169589042663574, Validation Loss: 168.95867919921875\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Training with 30 years, batch_size 32, epochs 60 - MAE: 322.676276312934\n",
            "Training Loss: 2.138143539428711, Validation Loss: 180.52133178710938\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Training with 30 years, batch_size 32, epochs 80 - MAE: 284.1851368253193\n",
            "Training Loss: 2.043954610824585, Validation Loss: 152.6965789794922\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Training with 30 years, batch_size 64, epochs 20 - MAE: 302.24193100702195\n",
            "Training Loss: 1.654534935951233, Validation Loss: 160.9810333251953\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Training with 30 years, batch_size 64, epochs 40 - MAE: 309.7962382483104\n",
            "Training Loss: 1.7768359184265137, Validation Loss: 168.1217803955078\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Training with 30 years, batch_size 64, epochs 60 - MAE: 301.4373040577722\n",
            "Training Loss: 1.6291736364364624, Validation Loss: 157.63571166992188\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Training with 30 years, batch_size 64, epochs 80 - MAE: 295.8374452136812\n",
            "Training Loss: 1.8401814699172974, Validation Loss: 159.52218627929688\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Training with 20 years, batch_size 32, epochs 20 - MAE: 214.3318902878534\n",
            "Training Loss: 3.7561614513397217, Validation Loss: 123.41624450683594\n",
            "2/2 [==============================] - 2s 6ms/step\n",
            "Training with 20 years, batch_size 32, epochs 40 - MAE: 197.1485799153646\n",
            "Training Loss: 3.840137004852295, Validation Loss: 111.51719665527344\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Training with 20 years, batch_size 32, epochs 60 - MAE: 201.8226318359375\n",
            "Training Loss: 4.14054012298584, Validation Loss: 118.03518676757812\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Training with 20 years, batch_size 32, epochs 80 - MAE: 209.95117817227802\n",
            "Training Loss: 4.171234607696533, Validation Loss: 130.89515686035156\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Training with 20 years, batch_size 64, epochs 20 - MAE: 219.42583986312624\n",
            "Training Loss: 3.827658176422119, Validation Loss: 134.36521911621094\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Training with 20 years, batch_size 64, epochs 40 - MAE: 203.53468225872706\n",
            "Training Loss: 3.6652417182922363, Validation Loss: 121.27116394042969\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Training with 20 years, batch_size 64, epochs 60 - MAE: 199.51271226671008\n",
            "Training Loss: 3.320300817489624, Validation Loss: 111.07398986816406\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Training with 20 years, batch_size 64, epochs 80 - MAE: 197.08218480670263\n",
            "Training Loss: 3.538709878921509, Validation Loss: 112.18077087402344\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Training with 10 years, batch_size 32, epochs 20 - MAE: 91.48159838479663\n",
            "Training Loss: 13.67929458618164, Validation Loss: 15.744487762451172\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Training with 10 years, batch_size 32, epochs 40 - MAE: 51.51551116458953\n",
            "Training Loss: 11.993063926696777, Validation Loss: 7.648780345916748\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Training with 10 years, batch_size 32, epochs 60 - MAE: 58.11868867420015\n",
            "Training Loss: 12.227560043334961, Validation Loss: 10.648014068603516\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Training with 10 years, batch_size 32, epochs 80 - MAE: 48.54752361963666\n",
            "Training Loss: 11.232260704040527, Validation Loss: 15.65673828125\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Training with 10 years, batch_size 64, epochs 20 - MAE: 184.5649167015439\n",
            "Training Loss: 28.15300941467285, Validation Loss: 88.39335632324219\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Training with 10 years, batch_size 64, epochs 40 - MAE: 87.27601308671255\n",
            "Training Loss: 12.474906921386719, Validation Loss: 21.548540115356445\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Training with 10 years, batch_size 64, epochs 60 - MAE: 66.38580709790426\n",
            "Training Loss: 12.000823020935059, Validation Loss: 8.315948486328125\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Training with 10 years, batch_size 64, epochs 80 - MAE: 58.1366940452939\n",
            "Training Loss: 11.210954666137695, Validation Loss: 21.185548782348633\n",
            "Best Configuration - Years: 10, Batch Size: 32, Epochs: 80 - MAE: 48.54752361963666\n"
          ]
        }
      ],
      "source": [
        "# Include overfitting parameter\n",
        "\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "##########################################################################\n",
        "# Configuration for the LSTM model (Bayesian  - ******IMPORTANT******)\n",
        "##########################################################################\n",
        "config = {'units': 100, 'dropout': 0.1, 'learning_rate': 0.01}\n",
        "\n",
        "\n",
        "# Download the data\n",
        "start_date = '1974-01-01'\n",
        "end_date = '2024-08-10'\n",
        "all_stocks = ['BA', 'MSI', 'DE', 'SPGI']\n",
        "df = yf.download(all_stocks, start=start_date, end=end_date)['Adj Close']\n",
        "df = df.reset_index()\n",
        "\n",
        "# Prepare the BA stock data\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "df.set_index('Date', inplace=True)\n",
        "df = df['SPGI']\n",
        "\n",
        "# Preprocess Data\n",
        "def preprocess_data(df, window_size):\n",
        "    scaled_data = df.values.reshape(-1, 1)\n",
        "\n",
        "    X, y, dates = [], [], []\n",
        "    for i in range(window_size, len(df)):\n",
        "        X.append(scaled_data[i-window_size:i, 0])\n",
        "        y.append(scaled_data[i, 0])\n",
        "        dates.append(df.index[i])  # Collect dates for plotting\n",
        "\n",
        "    X, y = np.array(X), np.array(y)\n",
        "    X = np.reshape(X, (X.shape[0], X.shape[1], 1))  # Reshape for LSTM [samples, time steps, features]\n",
        "\n",
        "    return X, y, None, dates\n",
        "\n",
        "# Define LSTM Model\n",
        "def create_lstm_model(units, dropout, learning_rate):\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(units=units, return_sequences=False, input_shape=(90, 1)))\n",
        "    model.add(Dropout(dropout))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_absolute_error')\n",
        "    return model\n",
        "\n",
        "# Train and evaluate each model configuration\n",
        "def train_and_evaluate(X_train, y_train, X_val, y_val, X_test, y_test, hyperparams, epochs, batch_size):\n",
        "        model = create_lstm_model(hyperparams['units'], hyperparams['dropout'], hyperparams['learning_rate'])\n",
        "\n",
        "        # Adding EarlyStopping callback to prevent overfitting\n",
        "        early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "        history = model.fit(\n",
        "            X_train, y_train,\n",
        "            validation_data=(X_val, y_val),\n",
        "            epochs=epochs,\n",
        "            batch_size=batch_size,\n",
        "            verbose=0\n",
        "        )\n",
        "\n",
        "        train_loss = history.history['loss']\n",
        "        val_loss = history.history['val_loss']\n",
        "        predictions = model.predict(X_test)\n",
        "\n",
        "        mae = mean_absolute_error(y_test, predictions)\n",
        "\n",
        "        return mae, predictions, train_loss, val_loss\n",
        "\n",
        "\n",
        "\n",
        "# Possible values for epochs and batch size\n",
        "batch_sizes = [32, 64]\n",
        "epochs_list = [20, 40, 60, 80]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Varying training periods\n",
        "# Ex- If we are using a 20-year training period, we will use data from 20 years before\n",
        "# the end date of your training set. The end date of the training data is fixed at 2023-12-31,\n",
        "# and the start date is calculated by subtracting 20 years from this end date.\n",
        "\n",
        "training_years = [50, 40, 30, 20, 10]\n",
        "results = {}\n",
        "\n",
        "# Test data (2024)\n",
        "test_start = '2024-01-01'\n",
        "test_end = '2024-08-10'\n",
        "df_test = df[test_start:test_end]\n",
        "\n",
        "# Grid Search for best batch size and epochs\n",
        "best_config = None\n",
        "best_mae = float('inf')\n",
        "best_predictions = None\n",
        "best_dates = None\n",
        "\n",
        "for years in training_years:\n",
        "    train_end = pd.to_datetime('2023-12-31')\n",
        "    train_start = train_end - pd.DateOffset(years=years)\n",
        "    df_train = df[train_start:train_end]\n",
        "\n",
        "    # Split into training and validation sets (80/20 split)\n",
        "    val_size = int(len(df_train) * 0.2)\n",
        "    df_val = df_train[-val_size:]\n",
        "    df_train = df_train[:-val_size]\n",
        "\n",
        "    window_size = 90\n",
        "    X_train, y_train, scaler, train_dates = preprocess_data(df_train, window_size)\n",
        "    X_val, y_val, _, val_dates = preprocess_data(df_val, window_size)\n",
        "    X_test, y_test, _, test_dates = preprocess_data(df_test, window_size)\n",
        "\n",
        "    for batch_size in batch_sizes:\n",
        "        for epochs in epochs_list:\n",
        "            mae, predictions, train_loss, val_loss = train_and_evaluate(\n",
        "                X_train, y_train, X_val, y_val, X_test, y_test, config, epochs, batch_size)\n",
        "\n",
        "            if mae < best_mae:\n",
        "                best_mae = mae\n",
        "                best_config = {'years': years, 'batch_size': batch_size, 'epochs': epochs}\n",
        "                best_predictions = predictions\n",
        "                best_dates = test_dates\n",
        "            print(f\"Training with {years} years, batch_size {batch_size}, epochs {epochs} - MAE: {mae}\")\n",
        "            print(f\"Training Loss: {train_loss[-1]}, Validation Loss: {val_loss[-1]}\")\n",
        "\n",
        "print(f\"Best Configuration - Years: {best_config['years']}, Batch Size: {best_config['batch_size']}, Epochs: {best_config['epochs']} - MAE: {best_mae}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 594
        },
        "id": "pSl2S_VtoPTP",
        "outputId": "c8a2069e-6a51-412d-ce37-a9263c5816fb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[*********************100%%**********************]  4 of 4 completed\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2/2 [==============================] - 0s 5ms/step\n",
            "MAE on Test Data: 32.88434467618428\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.24.1.min.js\"></script>                <div id=\"895dcc2e-fe54-41cd-b5e1-8e967ad7935f\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"895dcc2e-fe54-41cd-b5e1-8e967ad7935f\")) {                    Plotly.newPlot(                        \"895dcc2e-fe54-41cd-b5e1-8e967ad7935f\",                        [{\"line\":{\"color\":\"black\",\"dash\":\"dash\"},\"mode\":\"lines\",\"name\":\"Actual Values\",\"x\":[\"2024-05-10T00:00:00\",\"2024-05-13T00:00:00\",\"2024-05-14T00:00:00\",\"2024-05-15T00:00:00\",\"2024-05-16T00:00:00\",\"2024-05-17T00:00:00\",\"2024-05-20T00:00:00\",\"2024-05-21T00:00:00\",\"2024-05-22T00:00:00\",\"2024-05-23T00:00:00\",\"2024-05-24T00:00:00\",\"2024-05-28T00:00:00\",\"2024-05-29T00:00:00\",\"2024-05-30T00:00:00\",\"2024-05-31T00:00:00\",\"2024-06-03T00:00:00\",\"2024-06-04T00:00:00\",\"2024-06-05T00:00:00\",\"2024-06-06T00:00:00\",\"2024-06-07T00:00:00\",\"2024-06-10T00:00:00\",\"2024-06-11T00:00:00\",\"2024-06-12T00:00:00\",\"2024-06-13T00:00:00\",\"2024-06-14T00:00:00\",\"2024-06-17T00:00:00\",\"2024-06-18T00:00:00\",\"2024-06-20T00:00:00\",\"2024-06-21T00:00:00\",\"2024-06-24T00:00:00\",\"2024-06-25T00:00:00\",\"2024-06-26T00:00:00\",\"2024-06-27T00:00:00\",\"2024-06-28T00:00:00\",\"2024-07-01T00:00:00\",\"2024-07-02T00:00:00\",\"2024-07-03T00:00:00\",\"2024-07-05T00:00:00\",\"2024-07-08T00:00:00\",\"2024-07-09T00:00:00\",\"2024-07-10T00:00:00\",\"2024-07-11T00:00:00\",\"2024-07-12T00:00:00\",\"2024-07-15T00:00:00\",\"2024-07-16T00:00:00\",\"2024-07-17T00:00:00\",\"2024-07-18T00:00:00\",\"2024-07-19T00:00:00\",\"2024-07-22T00:00:00\",\"2024-07-23T00:00:00\",\"2024-07-24T00:00:00\",\"2024-07-25T00:00:00\",\"2024-07-26T00:00:00\",\"2024-07-29T00:00:00\",\"2024-07-30T00:00:00\",\"2024-07-31T00:00:00\",\"2024-08-01T00:00:00\",\"2024-08-02T00:00:00\",\"2024-08-05T00:00:00\",\"2024-08-06T00:00:00\",\"2024-08-07T00:00:00\",\"2024-08-08T00:00:00\",\"2024-08-09T00:00:00\"],\"y\":[430.6588134765625,427.1063232421875,427.12628173828125,434.78009033203125,437.09515380859375,440.947021484375,436.8157653808594,438.9412536621094,441.3661193847656,435.3588562011719,437.314697265625,430.0899963378906,424.9599914550781,421.5899963378906,427.510009765625,429.260009765625,431.8399963378906,436.2900085449219,434.6600036621094,428.739990234375,430.0,428.80999755859375,440.25,437.19000244140625,434.3999938964844,433.29998779296875,435.8500061035156,438.7099914550781,439.2200012207031,444.9700012207031,448.7799987792969,448.3599853515625,448.9100036621094,446.0,446.32000732421875,450.45001220703125,449.44000244140625,453.2699890136719,455.8900146484375,460.29998779296875,466.0,473.2200012207031,478.32000732421875,482.54998779296875,488.7900085449219,489.8699951171875,484.69000244140625,479.8399963378906,487.32000732421875,495.55999755859375,481.6600036621094,483.3299865722656,489.82000732421875,489.8299865722656,488.510009765625,484.7300109863281,490.5299987792969,486.0400085449219,471.989990234375,479.4800109863281,478.1199951171875,487.19000244140625,488.510009765625],\"type\":\"scatter\"},{\"line\":{\"color\":\"blue\"},\"mode\":\"lines\",\"name\":\"Predictions (MAE: 32.8843)\",\"x\":[\"2024-05-10T00:00:00\",\"2024-05-13T00:00:00\",\"2024-05-14T00:00:00\",\"2024-05-15T00:00:00\",\"2024-05-16T00:00:00\",\"2024-05-17T00:00:00\",\"2024-05-20T00:00:00\",\"2024-05-21T00:00:00\",\"2024-05-22T00:00:00\",\"2024-05-23T00:00:00\",\"2024-05-24T00:00:00\",\"2024-05-28T00:00:00\",\"2024-05-29T00:00:00\",\"2024-05-30T00:00:00\",\"2024-05-31T00:00:00\",\"2024-06-03T00:00:00\",\"2024-06-04T00:00:00\",\"2024-06-05T00:00:00\",\"2024-06-06T00:00:00\",\"2024-06-07T00:00:00\",\"2024-06-10T00:00:00\",\"2024-06-11T00:00:00\",\"2024-06-12T00:00:00\",\"2024-06-13T00:00:00\",\"2024-06-14T00:00:00\",\"2024-06-17T00:00:00\",\"2024-06-18T00:00:00\",\"2024-06-20T00:00:00\",\"2024-06-21T00:00:00\",\"2024-06-24T00:00:00\",\"2024-06-25T00:00:00\",\"2024-06-26T00:00:00\",\"2024-06-27T00:00:00\",\"2024-06-28T00:00:00\",\"2024-07-01T00:00:00\",\"2024-07-02T00:00:00\",\"2024-07-03T00:00:00\",\"2024-07-05T00:00:00\",\"2024-07-08T00:00:00\",\"2024-07-09T00:00:00\",\"2024-07-10T00:00:00\",\"2024-07-11T00:00:00\",\"2024-07-12T00:00:00\",\"2024-07-15T00:00:00\",\"2024-07-16T00:00:00\",\"2024-07-17T00:00:00\",\"2024-07-18T00:00:00\",\"2024-07-19T00:00:00\",\"2024-07-22T00:00:00\",\"2024-07-23T00:00:00\",\"2024-07-24T00:00:00\",\"2024-07-25T00:00:00\",\"2024-07-26T00:00:00\",\"2024-07-29T00:00:00\",\"2024-07-30T00:00:00\",\"2024-07-31T00:00:00\",\"2024-08-01T00:00:00\",\"2024-08-02T00:00:00\",\"2024-08-05T00:00:00\",\"2024-08-06T00:00:00\",\"2024-08-07T00:00:00\",\"2024-08-08T00:00:00\",\"2024-08-09T00:00:00\"],\"y\":[418.80303955078125,418.85833740234375,417.785888671875,417.4553527832031,419.579833984375,420.58404541015625,421.443603515625,420.83544921875,421.10919189453125,421.5806884765625,420.55950927734375,420.7445983886719,419.1322326660156,417.0912780761719,415.15753173828125,416.7972412109375,417.98089599609375,418.9964599609375,420.2808837890625,420.1791076660156,418.6163330078125,418.54498291015625,418.2239685058594,420.9041748046875,420.8467102050781,420.2051086425781,419.80682373046875,420.31011962890625,420.9952087402344,421.2059631347656,422.1264343261719,422.70587158203125,422.7187194824219,422.775390625,422.4502258300781,422.44891357421875,422.90631103515625,422.8523254394531,423.2039794921875,423.4430847167969,423.75244140625,424.06378173828125,424.3478088378906,424.4988098144531,424.5970153808594,424.70947265625,424.7278747558594,424.64324951171875,424.5423583984375,424.6848449707031,424.8018798828125,424.5854797363281,424.6143798828125,424.7252502441406,424.7275390625,424.7076110839844,424.64349365234375,424.736083984375,424.6673889160156,424.32830810546875,424.52532958984375,424.498779296875,424.6816711425781],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"title\":{\"text\":\"LSTM Model Predictions vs Actual Values\"},\"xaxis\":{\"title\":{\"text\":\"Date\"}},\"yaxis\":{\"title\":{\"text\":\"Stock Price\"}},\"legend\":{\"title\":{\"text\":\"Legend\"}},\"hovermode\":\"x unified\"},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('895dcc2e-fe54-41cd-b5e1-8e967ad7935f');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "\n",
        "# Best Configuration - Years: 10, Batch Size: 64, Epochs: 80 - MAE: 3.0124608599950395\n",
        "\n",
        "\n",
        "##################################\n",
        "# Define the best configuration\n",
        "#################################\n",
        "# IMPORTANT\n",
        "#################################\n",
        "best_years = 10\n",
        "best_batch_size = 32\n",
        "best_epochs = 80\n",
        "\n",
        "##########################################################################\n",
        "# Configuration for the LSTM model (Bayesian  - ******IMPORTANT******)\n",
        "##########################################################################\n",
        "config = {'units': 100, 'dropout': 0.1, 'learning_rate': 0.01}\n",
        "\n",
        "\n",
        "# Download the data\n",
        "start_date = '1974-01-01'\n",
        "end_date = '2024-08-10'\n",
        "all_stocks = ['BA', 'MSI', 'DE', 'SPGI']\n",
        "df = yf.download(all_stocks, start=start_date, end=end_date)['Adj Close']\n",
        "df = df.reset_index()\n",
        "\n",
        "test_start = '2024-01-01'\n",
        "test_end = '2024-08-10'\n",
        "\n",
        "# Prepare the BA stock data\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "df.set_index('Date', inplace=True)\n",
        "df = df['SPGI']\n",
        "\n",
        "# Preprocess Data\n",
        "def preprocess_data(df, window_size):\n",
        "    #scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    #scaled_data = scaler.fit_transform(df.values.reshape(-1, 1))\n",
        "\n",
        "    scaled_data = df.values.reshape(-1, 1)\n",
        "\n",
        "    X, y, dates = [], [], []\n",
        "    for i in range(window_size, len(df)):\n",
        "        X.append(scaled_data[i-window_size:i, 0])\n",
        "        y.append(scaled_data[i, 0])\n",
        "        dates.append(df.index[i])  # Collect dates for plotting\n",
        "\n",
        "    X, y = np.array(X), np.array(y)\n",
        "    X = np.reshape(X, (X.shape[0], X.shape[1], 1))  # Reshape for LSTM [samples, time steps, features]\n",
        "\n",
        "    return X, y, None, dates\n",
        "\n",
        "# Define LSTM Model\n",
        "def create_lstm_model(units, dropout, learning_rate):\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(units=units, return_sequences=False, input_shape=(90, 1)))\n",
        "    model.add(Dropout(dropout))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_absolute_error')\n",
        "    return model\n",
        "\n",
        "# Train and evaluate each model configuration\n",
        "def train_and_evaluate(X_train, y_train, X_test, y_test, hyperparams, epochs, batch_size):\n",
        "        model = create_lstm_model(hyperparams['units'], hyperparams['dropout'], hyperparams['learning_rate'])\n",
        "        model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=0)\n",
        "        predictions = model.predict(X_test)\n",
        "        mae = mean_absolute_error(y_test, predictions)\n",
        "        return mae, predictions\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Prepare the data using the best configuration\n",
        "train_end = pd.to_datetime('2023-12-31')\n",
        "train_start = train_end - pd.DateOffset(years=best_years)\n",
        "df_train = df[train_start:train_end]\n",
        "df_test = df[test_start:test_end]\n",
        "\n",
        "# Prepare data\n",
        "window_size = 90\n",
        "X_train, y_train, scaler, train_dates = preprocess_data(df_train, window_size)\n",
        "X_test, y_test, _, test_dates = preprocess_data(df_test, window_size)\n",
        "\n",
        "# Train the model using the best configuration\n",
        "model = create_lstm_model(units=150, dropout=0.2, learning_rate=0.001)\n",
        "model.fit(X_train, y_train, epochs=best_epochs, batch_size=best_batch_size, verbose=0)\n",
        "\n",
        "# Make predictions on the test data\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "# Inverse transform the predictions and true values using the scaler\n",
        "#predictions = scaler.inverse_transform(predictions)\n",
        "#y_test = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
        "\n",
        "# Calculate MAE\n",
        "mae = mean_absolute_error(y_test, predictions)\n",
        "print(f\"MAE on Test Data: {mae}\")\n",
        "\n",
        "# Create a Plotly plot to show the predictions and actual values\n",
        "fig = go.Figure()\n",
        "\n",
        "# Plot actual values\n",
        "fig.add_trace(go.Scatter(x=test_dates, y=y_test.flatten(), mode='lines', name='Actual Values', line=dict(color='black', dash='dash')))\n",
        "\n",
        "# Plot predictions\n",
        "fig.add_trace(go.Scatter(x=test_dates, y=predictions.flatten(), mode='lines', name=f'Predictions (MAE: {mae:.4f})', line=dict(color='blue')))\n",
        "\n",
        "# Add layout details\n",
        "fig.update_layout(\n",
        "    title=\"LSTM Model Predictions vs Actual Values\",\n",
        "    xaxis_title=\"Date\",\n",
        "    yaxis_title=\"Stock Price\",\n",
        "    legend_title=\"Legend\",\n",
        "    hovermode=\"x unified\"\n",
        ")\n",
        "\n",
        "# Show the plot\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEUnKw6EfGnB"
      },
      "source": [
        "## Hyperband"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_rBkM3DVeF07",
        "outputId": "7e81bd2a-bd9a-4cbb-8187-5e4e7cf42cd3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[*********************100%%**********************]  4 of 4 completed\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2/2 [==============================] - 0s 5ms/step\n",
            "Training with 50 years, batch_size 32, epochs 20 - MAE: 392.84876275441\n",
            "Training Loss: 0.7680743932723999, Validation Loss: 166.9007110595703\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Training with 50 years, batch_size 32, epochs 40 - MAE: 389.20641351124596\n",
            "Training Loss: 0.7507827877998352, Validation Loss: 163.732666015625\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Training with 50 years, batch_size 32, epochs 60 - MAE: 399.5402729700482\n",
            "Training Loss: 1.1837022304534912, Validation Loss: 173.43560791015625\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Training with 50 years, batch_size 32, epochs 80 - MAE: 390.98035903204055\n",
            "Training Loss: 1.10163414478302, Validation Loss: 167.59112548828125\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Training with 50 years, batch_size 64, epochs 20 - MAE: 387.4120001414465\n",
            "Training Loss: 0.6315317153930664, Validation Loss: 161.4898681640625\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Training with 50 years, batch_size 64, epochs 40 - MAE: 387.7411136929951\n",
            "Training Loss: 0.5661131739616394, Validation Loss: 160.0078582763672\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Training with 50 years, batch_size 64, epochs 60 - MAE: 392.534431820824\n",
            "Training Loss: 0.6021161079406738, Validation Loss: 164.54354858398438\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "Training with 50 years, batch_size 64, epochs 80 - MAE: 384.1463267008464\n",
            "Training Loss: 0.6477645039558411, Validation Loss: 156.91566467285156\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Training with 40 years, batch_size 32, epochs 20 - MAE: 361.9089110843719\n",
            "Training Loss: 1.2023857831954956, Validation Loss: 173.21539306640625\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Training with 40 years, batch_size 32, epochs 40 - MAE: 390.0276425074017\n",
            "Training Loss: 2.756786584854126, Validation Loss: 201.14845275878906\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Training with 40 years, batch_size 32, epochs 60 - MAE: 371.4957287500775\n",
            "Training Loss: 1.8794543743133545, Validation Loss: 182.75210571289062\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Training with 40 years, batch_size 32, epochs 80 - MAE: 361.7600435529436\n",
            "Training Loss: 2.4182240962982178, Validation Loss: 172.9906768798828\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Training with 40 years, batch_size 64, epochs 20 - MAE: 361.22493828667535\n",
            "Training Loss: 0.9776032567024231, Validation Loss: 170.52110290527344\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Training with 40 years, batch_size 64, epochs 40 - MAE: 374.88100179036456\n",
            "Training Loss: 1.345958948135376, Validation Loss: 181.93226623535156\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Training with 40 years, batch_size 64, epochs 60 - MAE: 368.3509343465169\n",
            "Training Loss: 0.9487212896347046, Validation Loss: 176.6956024169922\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Training with 40 years, batch_size 64, epochs 80 - MAE: 363.6458981226361\n",
            "Training Loss: 1.212062954902649, Validation Loss: 173.9887237548828\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Training with 30 years, batch_size 32, epochs 20 - MAE: 302.4272807287791\n",
            "Training Loss: 2.003115653991699, Validation Loss: 163.90936279296875\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Training with 30 years, batch_size 32, epochs 40 - MAE: 314.6430523584759\n",
            "Training Loss: 2.065333604812622, Validation Loss: 172.0915069580078\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Training with 30 years, batch_size 32, epochs 60 - MAE: 331.7335185701885\n",
            "Training Loss: 4.877195358276367, Validation Loss: 190.80584716796875\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Training with 30 years, batch_size 32, epochs 80 - MAE: 297.00987728058345\n",
            "Training Loss: 1.8467471599578857, Validation Loss: 156.92417907714844\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Training with 30 years, batch_size 64, epochs 20 - MAE: 322.9186067127046\n",
            "Training Loss: 2.1796531677246094, Validation Loss: 180.65769958496094\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Training with 30 years, batch_size 64, epochs 40 - MAE: 327.1578686426556\n",
            "Training Loss: 2.5781614780426025, Validation Loss: 185.4969024658203\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Training with 30 years, batch_size 64, epochs 60 - MAE: 309.3952394515749\n",
            "Training Loss: 2.2598636150360107, Validation Loss: 173.9650115966797\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Training with 30 years, batch_size 64, epochs 80 - MAE: 312.0688672746931\n",
            "Training Loss: 1.7424460649490356, Validation Loss: 161.21652221679688\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Training with 20 years, batch_size 32, epochs 20 - MAE: 220.2115439763145\n",
            "Training Loss: 4.683337688446045, Validation Loss: 130.51022338867188\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Training with 20 years, batch_size 32, epochs 40 - MAE: 225.89144558376736\n",
            "Training Loss: 4.623924732208252, Validation Loss: 137.9661102294922\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Training with 20 years, batch_size 32, epochs 60 - MAE: 215.09515187096974\n",
            "Training Loss: 4.697768688201904, Validation Loss: 127.7392349243164\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Training with 20 years, batch_size 32, epochs 80 - MAE: 228.54569426037017\n",
            "Training Loss: 4.665466785430908, Validation Loss: 144.7901153564453\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Training with 20 years, batch_size 64, epochs 20 - MAE: 253.92867218501985\n",
            "Training Loss: 5.2577996253967285, Validation Loss: 164.9385528564453\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "Training with 20 years, batch_size 64, epochs 40 - MAE: 211.36694311717199\n",
            "Training Loss: 3.888662338256836, Validation Loss: 122.36540985107422\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Training with 20 years, batch_size 64, epochs 60 - MAE: 238.96793377588665\n",
            "Training Loss: 4.720940113067627, Validation Loss: 149.97926330566406\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Training with 20 years, batch_size 64, epochs 80 - MAE: 212.76032269190227\n",
            "Training Loss: 4.114588737487793, Validation Loss: 123.77487182617188\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Training with 10 years, batch_size 32, epochs 20 - MAE: 209.38641478523377\n",
            "Training Loss: 36.177467346191406, Validation Loss: 113.16722106933594\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "Training with 10 years, batch_size 32, epochs 40 - MAE: 109.33788578093998\n",
            "Training Loss: 18.226696014404297, Validation Loss: 24.46425437927246\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Training with 10 years, batch_size 32, epochs 60 - MAE: 83.49131460038443\n",
            "Training Loss: 15.451827049255371, Validation Loss: 23.04418182373047\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Training with 10 years, batch_size 32, epochs 80 - MAE: 52.421758258153524\n",
            "Training Loss: 13.706413269042969, Validation Loss: 6.078925609588623\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Training with 10 years, batch_size 64, epochs 20 - MAE: 300.29062010749936\n",
            "Training Loss: 68.61165618896484, Validation Loss: 204.070068359375\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Training with 10 years, batch_size 64, epochs 40 - MAE: 208.72098141624815\n",
            "Training Loss: 35.65459060668945, Validation Loss: 112.50606536865234\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Training with 10 years, batch_size 64, epochs 60 - MAE: 113.06552995954242\n",
            "Training Loss: 17.807594299316406, Validation Loss: 27.18146514892578\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "Training with 10 years, batch_size 64, epochs 80 - MAE: 108.81454855298239\n",
            "Training Loss: 16.62674331665039, Validation Loss: 46.09524917602539\n",
            "Best Configuration - Years: 10, Batch Size: 32, Epochs: 80 - MAE: 52.421758258153524\n"
          ]
        }
      ],
      "source": [
        "# Include overfitting parameter\n",
        "# No EARLY STOP\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "##########################################################################\n",
        "# Configuration for the LSTM model (Hyperband  - ******IMPORTANT******)\n",
        "##########################################################################\n",
        "config = {'units': 50, 'dropout': 0.1, 'learning_rate': 0.01}\n",
        "\n",
        "\n",
        "# Download the data\n",
        "start_date = '1974-01-01'\n",
        "end_date = '2024-08-10'\n",
        "all_stocks = ['BA', 'MSI', 'DE', 'SPGI']\n",
        "df = yf.download(all_stocks, start=start_date, end=end_date)['Adj Close']\n",
        "df = df.reset_index()\n",
        "\n",
        "# Prepare the BA stock data\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "df.set_index('Date', inplace=True)\n",
        "df = df['SPGI']\n",
        "\n",
        "# Preprocess Data\n",
        "def preprocess_data(df, window_size):\n",
        "    scaled_data = df.values.reshape(-1, 1)\n",
        "\n",
        "    X, y, dates = [], [], []\n",
        "    for i in range(window_size, len(df)):\n",
        "        X.append(scaled_data[i-window_size:i, 0])\n",
        "        y.append(scaled_data[i, 0])\n",
        "        dates.append(df.index[i])  # Collect dates for plotting\n",
        "\n",
        "    X, y = np.array(X), np.array(y)\n",
        "    X = np.reshape(X, (X.shape[0], X.shape[1], 1))  # Reshape for LSTM [samples, time steps, features]\n",
        "\n",
        "    return X, y, None, dates\n",
        "\n",
        "# Define LSTM Model\n",
        "def create_lstm_model(units, dropout, learning_rate):\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(units=units, return_sequences=False, input_shape=(90, 1)))\n",
        "    model.add(Dropout(dropout))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_absolute_error')\n",
        "    return model\n",
        "\n",
        "# Train and evaluate each model configuration\n",
        "def train_and_evaluate(X_train, y_train, X_val, y_val, X_test, y_test, hyperparams, epochs, batch_size):\n",
        "        model = create_lstm_model(hyperparams['units'], hyperparams['dropout'], hyperparams['learning_rate'])\n",
        "\n",
        "        # Adding EarlyStopping callback to prevent overfitting\n",
        "        early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "        history = model.fit(\n",
        "            X_train, y_train,\n",
        "            validation_data=(X_val, y_val),\n",
        "            epochs=epochs,\n",
        "            batch_size=batch_size,\n",
        "            verbose=0\n",
        "        )\n",
        "\n",
        "        train_loss = history.history['loss']\n",
        "        val_loss = history.history['val_loss']\n",
        "        predictions = model.predict(X_test)\n",
        "\n",
        "        mae = mean_absolute_error(y_test, predictions)\n",
        "\n",
        "        return mae, predictions, train_loss, val_loss\n",
        "\n",
        "\n",
        "\n",
        "# Possible values for epochs and batch size\n",
        "batch_sizes = [32, 64]\n",
        "epochs_list = [20, 40, 60, 80]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Varying training periods\n",
        "# Ex- If we are using a 20-year training period, we will use data from 20 years before\n",
        "# the end date of your training set. The end date of the training data is fixed at 2023-12-31,\n",
        "# and the start date is calculated by subtracting 20 years from this end date.\n",
        "\n",
        "training_years = [50, 40, 30, 20, 10]\n",
        "results = {}\n",
        "\n",
        "# Test data (2024)\n",
        "test_start = '2024-01-01'\n",
        "test_end = '2024-08-10'\n",
        "df_test = df[test_start:test_end]\n",
        "\n",
        "# Grid Search for best batch size and epochs\n",
        "best_config = None\n",
        "best_mae = float('inf')\n",
        "best_predictions = None\n",
        "best_dates = None\n",
        "\n",
        "for years in training_years:\n",
        "    train_end = pd.to_datetime('2023-12-31')\n",
        "    train_start = train_end - pd.DateOffset(years=years)\n",
        "    df_train = df[train_start:train_end]\n",
        "\n",
        "    # Split into training and validation sets (80/20 split)\n",
        "    val_size = int(len(df_train) * 0.2)\n",
        "    df_val = df_train[-val_size:]\n",
        "    df_train = df_train[:-val_size]\n",
        "\n",
        "    window_size = 90\n",
        "    X_train, y_train, scaler, train_dates = preprocess_data(df_train, window_size)\n",
        "    X_val, y_val, _, val_dates = preprocess_data(df_val, window_size)\n",
        "    X_test, y_test, _, test_dates = preprocess_data(df_test, window_size)\n",
        "\n",
        "    for batch_size in batch_sizes:\n",
        "        for epochs in epochs_list:\n",
        "            mae, predictions, train_loss, val_loss = train_and_evaluate(\n",
        "                X_train, y_train, X_val, y_val, X_test, y_test, config, epochs, batch_size)\n",
        "\n",
        "            if mae < best_mae:\n",
        "                best_mae = mae\n",
        "                best_config = {'years': years, 'batch_size': batch_size, 'epochs': epochs}\n",
        "                best_predictions = predictions\n",
        "                best_dates = test_dates\n",
        "            print(f\"Training with {years} years, batch_size {batch_size}, epochs {epochs} - MAE: {mae}\")\n",
        "            print(f\"Training Loss: {train_loss[-1]}, Validation Loss: {val_loss[-1]}\")\n",
        "\n",
        "print(f\"Best Configuration - Years: {best_config['years']}, Batch Size: {best_config['batch_size']}, Epochs: {best_config['epochs']} - MAE: {best_mae}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 594
        },
        "id": "uzUgdZ4Ag40n",
        "outputId": "cdf3454e-4959-4f5c-eccd-22103895f719"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[*********************100%%**********************]  4 of 4 completed\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2/2 [==============================] - 0s 5ms/step\n",
            "MAE on Test Data: 30.58630855499752\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.24.1.min.js\"></script>                <div id=\"f2a5d59f-1ed1-47a9-8e6b-16b3e7c95443\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"f2a5d59f-1ed1-47a9-8e6b-16b3e7c95443\")) {                    Plotly.newPlot(                        \"f2a5d59f-1ed1-47a9-8e6b-16b3e7c95443\",                        [{\"line\":{\"color\":\"black\",\"dash\":\"dash\"},\"mode\":\"lines\",\"name\":\"Actual Values\",\"x\":[\"2024-05-10T00:00:00\",\"2024-05-13T00:00:00\",\"2024-05-14T00:00:00\",\"2024-05-15T00:00:00\",\"2024-05-16T00:00:00\",\"2024-05-17T00:00:00\",\"2024-05-20T00:00:00\",\"2024-05-21T00:00:00\",\"2024-05-22T00:00:00\",\"2024-05-23T00:00:00\",\"2024-05-24T00:00:00\",\"2024-05-28T00:00:00\",\"2024-05-29T00:00:00\",\"2024-05-30T00:00:00\",\"2024-05-31T00:00:00\",\"2024-06-03T00:00:00\",\"2024-06-04T00:00:00\",\"2024-06-05T00:00:00\",\"2024-06-06T00:00:00\",\"2024-06-07T00:00:00\",\"2024-06-10T00:00:00\",\"2024-06-11T00:00:00\",\"2024-06-12T00:00:00\",\"2024-06-13T00:00:00\",\"2024-06-14T00:00:00\",\"2024-06-17T00:00:00\",\"2024-06-18T00:00:00\",\"2024-06-20T00:00:00\",\"2024-06-21T00:00:00\",\"2024-06-24T00:00:00\",\"2024-06-25T00:00:00\",\"2024-06-26T00:00:00\",\"2024-06-27T00:00:00\",\"2024-06-28T00:00:00\",\"2024-07-01T00:00:00\",\"2024-07-02T00:00:00\",\"2024-07-03T00:00:00\",\"2024-07-05T00:00:00\",\"2024-07-08T00:00:00\",\"2024-07-09T00:00:00\",\"2024-07-10T00:00:00\",\"2024-07-11T00:00:00\",\"2024-07-12T00:00:00\",\"2024-07-15T00:00:00\",\"2024-07-16T00:00:00\",\"2024-07-17T00:00:00\",\"2024-07-18T00:00:00\",\"2024-07-19T00:00:00\",\"2024-07-22T00:00:00\",\"2024-07-23T00:00:00\",\"2024-07-24T00:00:00\",\"2024-07-25T00:00:00\",\"2024-07-26T00:00:00\",\"2024-07-29T00:00:00\",\"2024-07-30T00:00:00\",\"2024-07-31T00:00:00\",\"2024-08-01T00:00:00\",\"2024-08-02T00:00:00\",\"2024-08-05T00:00:00\",\"2024-08-06T00:00:00\",\"2024-08-07T00:00:00\",\"2024-08-08T00:00:00\",\"2024-08-09T00:00:00\"],\"y\":[430.6588134765625,427.1063232421875,427.12628173828125,434.78009033203125,437.09515380859375,440.947021484375,436.8157653808594,438.9412536621094,441.3661193847656,435.3588562011719,437.314697265625,430.0899963378906,424.9599914550781,421.5899963378906,427.510009765625,429.260009765625,431.8399963378906,436.2900085449219,434.6600036621094,428.739990234375,430.0,428.80999755859375,440.25,437.19000244140625,434.3999938964844,433.29998779296875,435.8500061035156,438.7099914550781,439.2200012207031,444.9700012207031,448.7799987792969,448.3599853515625,448.9100036621094,446.0,446.32000732421875,450.45001220703125,449.44000244140625,453.2699890136719,455.8900146484375,460.29998779296875,466.0,473.2200012207031,478.32000732421875,482.54998779296875,488.7900085449219,489.8699951171875,484.69000244140625,479.8399963378906,487.32000732421875,495.55999755859375,481.6600036621094,483.3299865722656,489.82000732421875,489.8299865722656,488.510009765625,484.7300109863281,490.5299987792969,486.0400085449219,471.989990234375,479.4800109863281,478.1199951171875,487.19000244140625,488.510009765625],\"type\":\"scatter\"},{\"line\":{\"color\":\"blue\"},\"mode\":\"lines\",\"name\":\"Predictions (MAE: 30.5863)\",\"x\":[\"2024-05-10T00:00:00\",\"2024-05-13T00:00:00\",\"2024-05-14T00:00:00\",\"2024-05-15T00:00:00\",\"2024-05-16T00:00:00\",\"2024-05-17T00:00:00\",\"2024-05-20T00:00:00\",\"2024-05-21T00:00:00\",\"2024-05-22T00:00:00\",\"2024-05-23T00:00:00\",\"2024-05-24T00:00:00\",\"2024-05-28T00:00:00\",\"2024-05-29T00:00:00\",\"2024-05-30T00:00:00\",\"2024-05-31T00:00:00\",\"2024-06-03T00:00:00\",\"2024-06-04T00:00:00\",\"2024-06-05T00:00:00\",\"2024-06-06T00:00:00\",\"2024-06-07T00:00:00\",\"2024-06-10T00:00:00\",\"2024-06-11T00:00:00\",\"2024-06-12T00:00:00\",\"2024-06-13T00:00:00\",\"2024-06-14T00:00:00\",\"2024-06-17T00:00:00\",\"2024-06-18T00:00:00\",\"2024-06-20T00:00:00\",\"2024-06-21T00:00:00\",\"2024-06-24T00:00:00\",\"2024-06-25T00:00:00\",\"2024-06-26T00:00:00\",\"2024-06-27T00:00:00\",\"2024-06-28T00:00:00\",\"2024-07-01T00:00:00\",\"2024-07-02T00:00:00\",\"2024-07-03T00:00:00\",\"2024-07-05T00:00:00\",\"2024-07-08T00:00:00\",\"2024-07-09T00:00:00\",\"2024-07-10T00:00:00\",\"2024-07-11T00:00:00\",\"2024-07-12T00:00:00\",\"2024-07-15T00:00:00\",\"2024-07-16T00:00:00\",\"2024-07-17T00:00:00\",\"2024-07-18T00:00:00\",\"2024-07-19T00:00:00\",\"2024-07-22T00:00:00\",\"2024-07-23T00:00:00\",\"2024-07-24T00:00:00\",\"2024-07-25T00:00:00\",\"2024-07-26T00:00:00\",\"2024-07-29T00:00:00\",\"2024-07-30T00:00:00\",\"2024-07-31T00:00:00\",\"2024-08-01T00:00:00\",\"2024-08-02T00:00:00\",\"2024-08-05T00:00:00\",\"2024-08-06T00:00:00\",\"2024-08-07T00:00:00\",\"2024-08-08T00:00:00\",\"2024-08-09T00:00:00\"],\"y\":[422.2235412597656,422.19854736328125,421.4374694824219,421.2843322753906,422.75775146484375,423.33160400390625,423.85919189453125,423.4039611816406,423.61737060546875,423.93023681640625,423.203369140625,423.379638671875,422.2481689453125,420.9016418457031,419.6849670410156,421.0219421386719,421.74713134765625,422.348388671875,423.15130615234375,423.01495361328125,421.92010498046875,421.97802734375,421.7734375,423.5873718261719,423.4443359375,422.9950256347656,422.7550964355469,423.1246337890625,423.5680236816406,423.6808776855469,424.29541015625,424.66510009765625,424.6586608886719,424.697021484375,424.4697570800781,424.4779052734375,424.7938537597656,424.7464599609375,424.99169921875,425.1534729003906,425.3694152832031,425.5918273925781,425.8025817871094,425.9189147949219,425.9974060058594,426.0900573730469,426.105712890625,426.0350646972656,425.9530334472656,426.0694580078125,426.16876220703125,425.9877624511719,426.0113220214844,426.1033020019531,426.10540771484375,426.0885925292969,426.0351867675781,426.1124572753906,426.0549621582031,425.7846984863281,425.93994140625,425.91851806640625,426.0667724609375],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"title\":{\"text\":\"LSTM Model Predictions vs Actual Values\"},\"xaxis\":{\"title\":{\"text\":\"Date\"}},\"yaxis\":{\"title\":{\"text\":\"Stock Price\"}},\"legend\":{\"title\":{\"text\":\"Legend\"}},\"hovermode\":\"x unified\"},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('f2a5d59f-1ed1-47a9-8e6b-16b3e7c95443');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# Best Configuration - Years: 30, Batch Size: 32, Epochs: 60 - MAE: 3.071416219075521\n",
        "\n",
        "\n",
        "##################################\n",
        "# Define the best configuration\n",
        "#################################\n",
        "# IMPORTANT\n",
        "#################################\n",
        "best_years = 10\n",
        "best_batch_size = 32\n",
        "best_epochs = 80\n",
        "\n",
        "##########################################################################\n",
        "# Configuration for the LSTM model (Hyperband  - ******IMPORTANT******)\n",
        "##########################################################################\n",
        "config = {'units': 50, 'dropout': 0.1, 'learning_rate': 0.01}\n",
        "\n",
        "\n",
        "# Download the data\n",
        "start_date = '1974-01-01'\n",
        "end_date = '2024-08-10'\n",
        "all_stocks = ['BA', 'MSI', 'DE', 'SPGI']\n",
        "df = yf.download(all_stocks, start=start_date, end=end_date)['Adj Close']\n",
        "df = df.reset_index()\n",
        "\n",
        "test_start = '2024-01-01'\n",
        "test_end = '2024-08-10'\n",
        "\n",
        "# Prepare the BA stock data\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "df.set_index('Date', inplace=True)\n",
        "df = df['SPGI']\n",
        "\n",
        "# Preprocess Data\n",
        "def preprocess_data(df, window_size):\n",
        "    #scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    #scaled_data = scaler.fit_transform(df.values.reshape(-1, 1))\n",
        "\n",
        "    scaled_data = df.values.reshape(-1, 1)\n",
        "\n",
        "    X, y, dates = [], [], []\n",
        "    for i in range(window_size, len(df)):\n",
        "        X.append(scaled_data[i-window_size:i, 0])\n",
        "        y.append(scaled_data[i, 0])\n",
        "        dates.append(df.index[i])  # Collect dates for plotting\n",
        "\n",
        "    X, y = np.array(X), np.array(y)\n",
        "    X = np.reshape(X, (X.shape[0], X.shape[1], 1))  # Reshape for LSTM [samples, time steps, features]\n",
        "\n",
        "    return X, y, None, dates\n",
        "\n",
        "# Define LSTM Model\n",
        "def create_lstm_model(units, dropout, learning_rate):\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(units=units, return_sequences=False, input_shape=(90, 1)))\n",
        "    model.add(Dropout(dropout))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_absolute_error')\n",
        "    return model\n",
        "\n",
        "# Train and evaluate each model configuration\n",
        "def train_and_evaluate(X_train, y_train, X_test, y_test, hyperparams, epochs, batch_size):\n",
        "        model = create_lstm_model(hyperparams['units'], hyperparams['dropout'], hyperparams['learning_rate'])\n",
        "        model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=0)\n",
        "        predictions = model.predict(X_test)\n",
        "        mae = mean_absolute_error(y_test, predictions)\n",
        "        return mae, predictions\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Prepare the data using the best configuration\n",
        "train_end = pd.to_datetime('2023-12-31')\n",
        "train_start = train_end - pd.DateOffset(years=best_years)\n",
        "df_train = df[train_start:train_end]\n",
        "df_test = df[test_start:test_end]\n",
        "\n",
        "# Prepare data\n",
        "window_size = 90\n",
        "X_train, y_train, scaler, train_dates = preprocess_data(df_train, window_size)\n",
        "X_test, y_test, _, test_dates = preprocess_data(df_test, window_size)\n",
        "\n",
        "# Train the model using the best configuration\n",
        "model = create_lstm_model(units=150, dropout=0.2, learning_rate=0.001)\n",
        "model.fit(X_train, y_train, epochs=best_epochs, batch_size=best_batch_size, verbose=0)\n",
        "\n",
        "# Make predictions on the test data\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "# Inverse transform the predictions and true values using the scaler\n",
        "#predictions = scaler.inverse_transform(predictions)\n",
        "#y_test = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
        "\n",
        "# Calculate MAE\n",
        "mae = mean_absolute_error(y_test, predictions)\n",
        "print(f\"MAE on Test Data: {mae}\")\n",
        "\n",
        "# Create a Plotly plot to show the predictions and actual values\n",
        "fig = go.Figure()\n",
        "\n",
        "# Plot actual values\n",
        "fig.add_trace(go.Scatter(x=test_dates, y=y_test.flatten(), mode='lines', name='Actual Values', line=dict(color='black', dash='dash')))\n",
        "\n",
        "# Plot predictions\n",
        "fig.add_trace(go.Scatter(x=test_dates, y=predictions.flatten(), mode='lines', name=f'Predictions (MAE: {mae:.4f})', line=dict(color='blue')))\n",
        "\n",
        "# Add layout details\n",
        "fig.update_layout(\n",
        "    title=\"LSTM Model Predictions vs Actual Values\",\n",
        "    xaxis_title=\"Date\",\n",
        "    yaxis_title=\"Stock Price\",\n",
        "    legend_title=\"Legend\",\n",
        "    hovermode=\"x unified\"\n",
        ")\n",
        "\n",
        "# Show the plot\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChUfptYrgEls"
      },
      "source": [
        "## Grid Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FrfmPimggHyW",
        "outputId": "a3b69730-25a6-47a4-b62f-9a99304a19ed"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[*********************100%%**********************]  4 of 4 completed\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2/2 [==============================] - 0s 5ms/step\n",
            "Training with 50 years, batch_size 32, epochs 20 - MAE: 392.4785319131518\n",
            "Training Loss: 0.6561456322669983, Validation Loss: 165.26991271972656\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Training with 50 years, batch_size 32, epochs 40 - MAE: 404.26564013768757\n",
            "Training Loss: 0.7176506519317627, Validation Loss: 177.24729919433594\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Training with 50 years, batch_size 32, epochs 60 - MAE: 407.340687464154\n",
            "Training Loss: 2.5182924270629883, Validation Loss: 181.76708984375\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Training with 50 years, batch_size 32, epochs 80 - MAE: 393.2445261516268\n",
            "Training Loss: 0.6779260039329529, Validation Loss: 165.013427734375\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Training with 50 years, batch_size 64, epochs 20 - MAE: 396.0409002758208\n",
            "Training Loss: 0.5992680191993713, Validation Loss: 167.1132354736328\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Training with 50 years, batch_size 64, epochs 40 - MAE: 401.9784774174766\n",
            "Training Loss: 0.8935298919677734, Validation Loss: 176.21066284179688\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Training with 50 years, batch_size 64, epochs 60 - MAE: 395.49779413616847\n",
            "Training Loss: 0.8606196045875549, Validation Loss: 169.15814208984375\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Training with 50 years, batch_size 64, epochs 80 - MAE: 411.2396668933687\n",
            "Training Loss: 1.4950724840164185, Validation Loss: 184.9013671875\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Training with 40 years, batch_size 32, epochs 20 - MAE: 363.7852831643725\n",
            "Training Loss: 1.252760410308838, Validation Loss: 178.40264892578125\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Training with 40 years, batch_size 32, epochs 40 - MAE: 364.4935462588356\n",
            "Training Loss: 1.0316988229751587, Validation Loss: 173.06585693359375\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Training with 40 years, batch_size 32, epochs 60 - MAE: 392.27924468025327\n",
            "Training Loss: 3.2246406078338623, Validation Loss: 204.36204528808594\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Training with 40 years, batch_size 32, epochs 80 - MAE: 376.7832491435702\n",
            "Training Loss: 2.028473138809204, Validation Loss: 187.27557373046875\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Training with 40 years, batch_size 64, epochs 20 - MAE: 358.2389418586852\n",
            "Training Loss: 0.9520658850669861, Validation Loss: 168.33522033691406\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Training with 40 years, batch_size 64, epochs 40 - MAE: 361.3656754266648\n",
            "Training Loss: 1.0072581768035889, Validation Loss: 170.1141815185547\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Training with 40 years, batch_size 64, epochs 60 - MAE: 360.37713647267174\n",
            "Training Loss: 1.049085259437561, Validation Loss: 171.0816192626953\n",
            "2/2 [==============================] - 3s 5ms/step\n",
            "Training with 40 years, batch_size 64, epochs 80 - MAE: 367.26964387439546\n",
            "Training Loss: 1.1274371147155762, Validation Loss: 173.0867919921875\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "Training with 30 years, batch_size 32, epochs 20 - MAE: 312.1078655908978\n",
            "Training Loss: 2.025564432144165, Validation Loss: 170.43972778320312\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Training with 30 years, batch_size 32, epochs 40 - MAE: 301.96710205078125\n",
            "Training Loss: 1.9511479139328003, Validation Loss: 161.64332580566406\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Training with 30 years, batch_size 32, epochs 60 - MAE: 316.3545956081814\n",
            "Training Loss: 2.4374260902404785, Validation Loss: 173.9158172607422\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Training with 30 years, batch_size 32, epochs 80 - MAE: 305.1712644062345\n",
            "Training Loss: 2.017577886581421, Validation Loss: 163.51605224609375\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Training with 30 years, batch_size 64, epochs 20 - MAE: 326.3074243939112\n",
            "Training Loss: 2.2391979694366455, Validation Loss: 184.3731689453125\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Training with 30 years, batch_size 64, epochs 40 - MAE: 314.07613845098587\n",
            "Training Loss: 1.9602566957473755, Validation Loss: 172.41656494140625\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Training with 30 years, batch_size 64, epochs 60 - MAE: 303.132815164233\n",
            "Training Loss: 1.9117696285247803, Validation Loss: 163.14779663085938\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Training with 30 years, batch_size 64, epochs 80 - MAE: 304.6571800595238\n",
            "Training Loss: 2.2648844718933105, Validation Loss: 170.04847717285156\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Training with 20 years, batch_size 32, epochs 20 - MAE: 231.1522451733786\n",
            "Training Loss: 5.008114814758301, Validation Loss: 142.96002197265625\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Training with 20 years, batch_size 32, epochs 40 - MAE: 216.99403284466456\n",
            "Training Loss: 4.308731555938721, Validation Loss: 128.5133056640625\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Training with 20 years, batch_size 32, epochs 60 - MAE: 231.64891996837798\n",
            "Training Loss: 5.495095729827881, Validation Loss: 152.76577758789062\n",
            "2/2 [==============================] - 0s 4ms/step\n",
            "Training with 20 years, batch_size 32, epochs 80 - MAE: 243.99588618202816\n",
            "Training Loss: 5.473488807678223, Validation Loss: 154.67996215820312\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Training with 20 years, batch_size 64, epochs 20 - MAE: 253.4654749310206\n",
            "Training Loss: 5.214163780212402, Validation Loss: 164.4739227294922\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Training with 20 years, batch_size 64, epochs 40 - MAE: 224.88163418240018\n",
            "Training Loss: 4.231680393218994, Validation Loss: 135.8946075439453\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Training with 20 years, batch_size 64, epochs 60 - MAE: 265.2816530257937\n",
            "Training Loss: 6.888446807861328, Validation Loss: 176.29104614257812\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Training with 20 years, batch_size 64, epochs 80 - MAE: 239.47889346168154\n",
            "Training Loss: 5.130362510681152, Validation Loss: 150.49026489257812\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Training with 10 years, batch_size 32, epochs 20 - MAE: 208.34156920417908\n",
            "Training Loss: 39.206214904785156, Validation Loss: 112.12293243408203\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Training with 10 years, batch_size 32, epochs 40 - MAE: 107.31777711898562\n",
            "Training Loss: 18.605337142944336, Validation Loss: 22.083847045898438\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Training with 10 years, batch_size 32, epochs 60 - MAE: 60.429354713076634\n",
            "Training Loss: 13.415587425231934, Validation Loss: 12.640022277832031\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Training with 10 years, batch_size 32, epochs 80 - MAE: 53.455862378317214\n",
            "Training Loss: 14.37450122833252, Validation Loss: 11.12560749053955\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Training with 10 years, batch_size 64, epochs 20 - MAE: 275.7183850000775\n",
            "Training Loss: 57.50921630859375, Validation Loss: 179.4978485107422\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Training with 10 years, batch_size 64, epochs 40 - MAE: 229.00740172371033\n",
            "Training Loss: 41.28823471069336, Validation Loss: 132.78697204589844\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Training with 10 years, batch_size 64, epochs 60 - MAE: 126.41920350089906\n",
            "Training Loss: 19.940988540649414, Validation Loss: 35.072879791259766\n",
            "2/2 [==============================] - 0s 5ms/step\n",
            "Training with 10 years, batch_size 64, epochs 80 - MAE: 91.69921439034599\n",
            "Training Loss: 15.679911613464355, Validation Loss: 14.071264266967773\n",
            "Best Configuration - Years: 10, Batch Size: 32, Epochs: 80 - MAE: 53.455862378317214\n"
          ]
        }
      ],
      "source": [
        "# Include overfitting parameter\n",
        "# No EARLY STOP\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "##########################################################################\n",
        "# Configuration for the LSTM model (GRID Search  - ******IMPORTANT******)\n",
        "##########################################################################\n",
        "config = {'units': 50, 'dropout': 0.1, 'learning_rate': 0.01}\n",
        "\n",
        "\n",
        "# Download the data\n",
        "start_date = '1974-01-01'\n",
        "end_date = '2024-08-10'\n",
        "all_stocks = ['BA', 'MSI', 'DE', 'SPGI']\n",
        "df = yf.download(all_stocks, start=start_date, end=end_date)['Adj Close']\n",
        "df = df.reset_index()\n",
        "\n",
        "# Prepare the BA stock data\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "df.set_index('Date', inplace=True)\n",
        "df = df['SPGI']\n",
        "\n",
        "# Preprocess Data\n",
        "def preprocess_data(df, window_size):\n",
        "    scaled_data = df.values.reshape(-1, 1)\n",
        "\n",
        "    X, y, dates = [], [], []\n",
        "    for i in range(window_size, len(df)):\n",
        "        X.append(scaled_data[i-window_size:i, 0])\n",
        "        y.append(scaled_data[i, 0])\n",
        "        dates.append(df.index[i])  # Collect dates for plotting\n",
        "\n",
        "    X, y = np.array(X), np.array(y)\n",
        "    X = np.reshape(X, (X.shape[0], X.shape[1], 1))  # Reshape for LSTM [samples, time steps, features]\n",
        "\n",
        "    return X, y, None, dates\n",
        "\n",
        "# Define LSTM Model\n",
        "def create_lstm_model(units, dropout, learning_rate):\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(units=units, return_sequences=False, input_shape=(90, 1)))\n",
        "    model.add(Dropout(dropout))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_absolute_error')\n",
        "    return model\n",
        "\n",
        "# Train and evaluate each model configuration\n",
        "def train_and_evaluate(X_train, y_train, X_val, y_val, X_test, y_test, hyperparams, epochs, batch_size):\n",
        "        model = create_lstm_model(hyperparams['units'], hyperparams['dropout'], hyperparams['learning_rate'])\n",
        "\n",
        "        # Adding EarlyStopping callback to prevent overfitting\n",
        "        early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "        history = model.fit(\n",
        "            X_train, y_train,\n",
        "            validation_data=(X_val, y_val),\n",
        "            epochs=epochs,\n",
        "            batch_size=batch_size,\n",
        "            verbose=0\n",
        "        )\n",
        "\n",
        "        train_loss = history.history['loss']\n",
        "        val_loss = history.history['val_loss']\n",
        "        predictions = model.predict(X_test)\n",
        "\n",
        "        mae = mean_absolute_error(y_test, predictions)\n",
        "\n",
        "        return mae, predictions, train_loss, val_loss\n",
        "\n",
        "\n",
        "\n",
        "# Possible values for epochs and batch size\n",
        "batch_sizes = [32, 64]\n",
        "epochs_list = [20, 40, 60, 80]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Varying training periods\n",
        "# Ex- If we are using a 20-year training period, we will use data from 20 years before\n",
        "# the end date of your training set. The end date of the training data is fixed at 2023-12-31,\n",
        "# and the start date is calculated by subtracting 20 years from this end date.\n",
        "\n",
        "training_years = [50, 40, 30, 20, 10]\n",
        "results = {}\n",
        "\n",
        "# Test data (2024)\n",
        "test_start = '2024-01-01'\n",
        "test_end = '2024-08-10'\n",
        "df_test = df[test_start:test_end]\n",
        "\n",
        "# Grid Search for best batch size and epochs\n",
        "best_config = None\n",
        "best_mae = float('inf')\n",
        "best_predictions = None\n",
        "best_dates = None\n",
        "\n",
        "for years in training_years:\n",
        "    train_end = pd.to_datetime('2023-12-31')\n",
        "    train_start = train_end - pd.DateOffset(years=years)\n",
        "    df_train = df[train_start:train_end]\n",
        "\n",
        "    # Split into training and validation sets (80/20 split)\n",
        "    val_size = int(len(df_train) * 0.2)\n",
        "    df_val = df_train[-val_size:]\n",
        "    df_train = df_train[:-val_size]\n",
        "\n",
        "    window_size = 90\n",
        "    X_train, y_train, scaler, train_dates = preprocess_data(df_train, window_size)\n",
        "    X_val, y_val, _, val_dates = preprocess_data(df_val, window_size)\n",
        "    X_test, y_test, _, test_dates = preprocess_data(df_test, window_size)\n",
        "\n",
        "    for batch_size in batch_sizes:\n",
        "        for epochs in epochs_list:\n",
        "            mae, predictions, train_loss, val_loss = train_and_evaluate(\n",
        "                X_train, y_train, X_val, y_val, X_test, y_test, config, epochs, batch_size)\n",
        "\n",
        "            if mae < best_mae:\n",
        "                best_mae = mae\n",
        "                best_config = {'years': years, 'batch_size': batch_size, 'epochs': epochs}\n",
        "                best_predictions = predictions\n",
        "                best_dates = test_dates\n",
        "            print(f\"Training with {years} years, batch_size {batch_size}, epochs {epochs} - MAE: {mae}\")\n",
        "            print(f\"Training Loss: {train_loss[-1]}, Validation Loss: {val_loss[-1]}\")\n",
        "\n",
        "print(f\"Best Configuration - Years: {best_config['years']}, Batch Size: {best_config['batch_size']}, Epochs: {best_config['epochs']} - MAE: {best_mae}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 594
        },
        "id": "pITh3gwughr0",
        "outputId": "86dc631a-9cab-48f4-abab-4023504c80b6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[*********************100%%**********************]  4 of 4 completed\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2/2 [==============================] - 0s 5ms/step\n",
            "MAE on Test Data: 280.65760003952755\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.24.1.min.js\"></script>                <div id=\"ca5abeb8-cef7-4818-bba2-51bbc1e65a19\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"ca5abeb8-cef7-4818-bba2-51bbc1e65a19\")) {                    Plotly.newPlot(                        \"ca5abeb8-cef7-4818-bba2-51bbc1e65a19\",                        [{\"line\":{\"color\":\"black\",\"dash\":\"dash\"},\"mode\":\"lines\",\"name\":\"Actual Values\",\"x\":[\"2024-05-10T00:00:00\",\"2024-05-13T00:00:00\",\"2024-05-14T00:00:00\",\"2024-05-15T00:00:00\",\"2024-05-16T00:00:00\",\"2024-05-17T00:00:00\",\"2024-05-20T00:00:00\",\"2024-05-21T00:00:00\",\"2024-05-22T00:00:00\",\"2024-05-23T00:00:00\",\"2024-05-24T00:00:00\",\"2024-05-28T00:00:00\",\"2024-05-29T00:00:00\",\"2024-05-30T00:00:00\",\"2024-05-31T00:00:00\",\"2024-06-03T00:00:00\",\"2024-06-04T00:00:00\",\"2024-06-05T00:00:00\",\"2024-06-06T00:00:00\",\"2024-06-07T00:00:00\",\"2024-06-10T00:00:00\",\"2024-06-11T00:00:00\",\"2024-06-12T00:00:00\",\"2024-06-13T00:00:00\",\"2024-06-14T00:00:00\",\"2024-06-17T00:00:00\",\"2024-06-18T00:00:00\",\"2024-06-20T00:00:00\",\"2024-06-21T00:00:00\",\"2024-06-24T00:00:00\",\"2024-06-25T00:00:00\",\"2024-06-26T00:00:00\",\"2024-06-27T00:00:00\",\"2024-06-28T00:00:00\",\"2024-07-01T00:00:00\",\"2024-07-02T00:00:00\",\"2024-07-03T00:00:00\",\"2024-07-05T00:00:00\",\"2024-07-08T00:00:00\",\"2024-07-09T00:00:00\",\"2024-07-10T00:00:00\",\"2024-07-11T00:00:00\",\"2024-07-12T00:00:00\",\"2024-07-15T00:00:00\",\"2024-07-16T00:00:00\",\"2024-07-17T00:00:00\",\"2024-07-18T00:00:00\",\"2024-07-19T00:00:00\",\"2024-07-22T00:00:00\",\"2024-07-23T00:00:00\",\"2024-07-24T00:00:00\",\"2024-07-25T00:00:00\",\"2024-07-26T00:00:00\",\"2024-07-29T00:00:00\",\"2024-07-30T00:00:00\",\"2024-07-31T00:00:00\",\"2024-08-01T00:00:00\",\"2024-08-02T00:00:00\",\"2024-08-05T00:00:00\",\"2024-08-06T00:00:00\",\"2024-08-07T00:00:00\",\"2024-08-08T00:00:00\",\"2024-08-09T00:00:00\"],\"y\":[430.6588134765625,427.1063232421875,427.12628173828125,434.78009033203125,437.09515380859375,440.947021484375,436.8157653808594,438.9412536621094,441.3661193847656,435.3588562011719,437.314697265625,430.0899963378906,424.9599914550781,421.5899963378906,427.510009765625,429.260009765625,431.8399963378906,436.2900085449219,434.6600036621094,428.739990234375,430.0,428.80999755859375,440.25,437.19000244140625,434.3999938964844,433.29998779296875,435.8500061035156,438.7099914550781,439.2200012207031,444.9700012207031,448.7799987792969,448.3599853515625,448.9100036621094,446.0,446.32000732421875,450.45001220703125,449.44000244140625,453.2699890136719,455.8900146484375,460.29998779296875,466.0,473.2200012207031,478.32000732421875,482.54998779296875,488.7900085449219,489.8699951171875,484.69000244140625,479.8399963378906,487.32000732421875,495.55999755859375,481.6600036621094,483.3299865722656,489.82000732421875,489.8299865722656,488.510009765625,484.7300109863281,490.5299987792969,486.0400085449219,471.989990234375,479.4800109863281,478.1199951171875,487.19000244140625,488.510009765625],\"type\":\"scatter\"},{\"line\":{\"color\":\"blue\"},\"mode\":\"lines\",\"name\":\"Predictions (MAE: 280.6576)\",\"x\":[\"2024-05-10T00:00:00\",\"2024-05-13T00:00:00\",\"2024-05-14T00:00:00\",\"2024-05-15T00:00:00\",\"2024-05-16T00:00:00\",\"2024-05-17T00:00:00\",\"2024-05-20T00:00:00\",\"2024-05-21T00:00:00\",\"2024-05-22T00:00:00\",\"2024-05-23T00:00:00\",\"2024-05-24T00:00:00\",\"2024-05-28T00:00:00\",\"2024-05-29T00:00:00\",\"2024-05-30T00:00:00\",\"2024-05-31T00:00:00\",\"2024-06-03T00:00:00\",\"2024-06-04T00:00:00\",\"2024-06-05T00:00:00\",\"2024-06-06T00:00:00\",\"2024-06-07T00:00:00\",\"2024-06-10T00:00:00\",\"2024-06-11T00:00:00\",\"2024-06-12T00:00:00\",\"2024-06-13T00:00:00\",\"2024-06-14T00:00:00\",\"2024-06-17T00:00:00\",\"2024-06-18T00:00:00\",\"2024-06-20T00:00:00\",\"2024-06-21T00:00:00\",\"2024-06-24T00:00:00\",\"2024-06-25T00:00:00\",\"2024-06-26T00:00:00\",\"2024-06-27T00:00:00\",\"2024-06-28T00:00:00\",\"2024-07-01T00:00:00\",\"2024-07-02T00:00:00\",\"2024-07-03T00:00:00\",\"2024-07-05T00:00:00\",\"2024-07-08T00:00:00\",\"2024-07-09T00:00:00\",\"2024-07-10T00:00:00\",\"2024-07-11T00:00:00\",\"2024-07-12T00:00:00\",\"2024-07-15T00:00:00\",\"2024-07-16T00:00:00\",\"2024-07-17T00:00:00\",\"2024-07-18T00:00:00\",\"2024-07-19T00:00:00\",\"2024-07-22T00:00:00\",\"2024-07-23T00:00:00\",\"2024-07-24T00:00:00\",\"2024-07-25T00:00:00\",\"2024-07-26T00:00:00\",\"2024-07-29T00:00:00\",\"2024-07-30T00:00:00\",\"2024-07-31T00:00:00\",\"2024-08-01T00:00:00\",\"2024-08-02T00:00:00\",\"2024-08-05T00:00:00\",\"2024-08-06T00:00:00\",\"2024-08-07T00:00:00\",\"2024-08-08T00:00:00\",\"2024-08-09T00:00:00\"],\"y\":[174.08453369140625,174.08453369140625,174.08445739746094,174.08445739746094,174.08457946777344,174.0845947265625,174.08465576171875,174.0845947265625,174.0846405029297,174.08465576171875,174.08457946777344,174.0845947265625,174.0845184326172,174.08444213867188,174.08436584472656,174.08447265625,174.08450317382812,174.08453369140625,174.0845947265625,174.08457946777344,174.08448791503906,174.08450317382812,174.08448791503906,174.0846405029297,174.0845947265625,174.08457946777344,174.0845489501953,174.0845947265625,174.08460998535156,174.0846405029297,174.084716796875,174.08477783203125,174.08474731445312,174.08477783203125,174.08473205566406,174.08473205566406,174.08477783203125,174.08477783203125,174.08480834960938,174.08482360839844,174.0848846435547,174.0849609375,174.08502197265625,174.08505249023438,174.08509826660156,174.0851593017578,174.08517456054688,174.08514404296875,174.08506774902344,174.08514404296875,174.085205078125,174.08509826660156,174.08509826660156,174.0851593017578,174.0851593017578,174.0851593017578,174.08514404296875,174.08517456054688,174.08514404296875,174.08502197265625,174.08506774902344,174.08505249023438,174.08514404296875],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"title\":{\"text\":\"LSTM Model Predictions vs Actual Values\"},\"xaxis\":{\"title\":{\"text\":\"Date\"}},\"yaxis\":{\"title\":{\"text\":\"Stock Price\"}},\"legend\":{\"title\":{\"text\":\"Legend\"}},\"hovermode\":\"x unified\"},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('ca5abeb8-cef7-4818-bba2-51bbc1e65a19');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# Best Configuration - Years: 20, Batch Size: 64, Epochs: 40 - MAE: 3.7216394818018355\n",
        "\n",
        "\n",
        "##################################\n",
        "# Define the best configuration\n",
        "#################################\n",
        "# IMPORTANT\n",
        "#################################\n",
        "\n",
        "best_years = 10\n",
        "best_batch_size = 32\n",
        "best_epochs = 20\n",
        "\n",
        "##########################################################################\n",
        "# Configuration for the LSTM model (GRID Search  - ******IMPORTANT******)\n",
        "##########################################################################\n",
        "config = {'units': 50, 'dropout': 0.1, 'learning_rate': 0.01}\n",
        "\n",
        "# Download the data\n",
        "start_date = '1974-01-01'\n",
        "end_date = '2024-08-10'\n",
        "all_stocks = ['BA', 'MSI', 'DE', 'SPGI']\n",
        "df = yf.download(all_stocks, start=start_date, end=end_date)['Adj Close']\n",
        "df = df.reset_index()\n",
        "\n",
        "test_start = '2024-01-01'\n",
        "test_end = '2024-08-10'\n",
        "\n",
        "# Prepare the BA stock data\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "df.set_index('Date', inplace=True)\n",
        "df = df['SPGI']\n",
        "\n",
        "# Preprocess Data\n",
        "def preprocess_data(df, window_size):\n",
        "    #scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    #scaled_data = scaler.fit_transform(df.values.reshape(-1, 1))\n",
        "\n",
        "    scaled_data = df.values.reshape(-1, 1)\n",
        "\n",
        "    X, y, dates = [], [], []\n",
        "    for i in range(window_size, len(df)):\n",
        "        X.append(scaled_data[i-window_size:i, 0])\n",
        "        y.append(scaled_data[i, 0])\n",
        "        dates.append(df.index[i])  # Collect dates for plotting\n",
        "\n",
        "    X, y = np.array(X), np.array(y)\n",
        "    X = np.reshape(X, (X.shape[0], X.shape[1], 1))  # Reshape for LSTM [samples, time steps, features]\n",
        "\n",
        "    return X, y, None, dates\n",
        "\n",
        "# Define LSTM Model\n",
        "def create_lstm_model(units, dropout, learning_rate):\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(units=units, return_sequences=False, input_shape=(90, 1)))\n",
        "    model.add(Dropout(dropout))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_absolute_error')\n",
        "    return model\n",
        "\n",
        "# Train and evaluate each model configuration\n",
        "def train_and_evaluate(X_train, y_train, X_test, y_test, hyperparams, epochs, batch_size):\n",
        "        model = create_lstm_model(hyperparams['units'], hyperparams['dropout'], hyperparams['learning_rate'])\n",
        "        model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=0)\n",
        "        predictions = model.predict(X_test)\n",
        "        mae = mean_absolute_error(y_test, predictions)\n",
        "        return mae, predictions\n",
        "\n",
        "\n",
        "\n",
        "# Prepare the data using the best configuration\n",
        "train_end = pd.to_datetime('2023-12-31')\n",
        "train_start = train_end - pd.DateOffset(years=best_years)\n",
        "df_train = df[train_start:train_end]\n",
        "df_test = df[test_start:test_end]\n",
        "\n",
        "# Prepare data\n",
        "window_size = 90\n",
        "X_train, y_train, scaler, train_dates = preprocess_data(df_train, window_size)\n",
        "X_test, y_test, _, test_dates = preprocess_data(df_test, window_size)\n",
        "\n",
        "# Train the model using the best configuration\n",
        "model = create_lstm_model(units=150, dropout=0.2, learning_rate=0.001)\n",
        "model.fit(X_train, y_train, epochs=best_epochs, batch_size=best_batch_size, verbose=0)\n",
        "\n",
        "# Make predictions on the test data\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "# Inverse transform the predictions and true values using the scaler\n",
        "#predictions = scaler.inverse_transform(predictions)\n",
        "#y_test = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
        "\n",
        "# Calculate MAE\n",
        "mae = mean_absolute_error(y_test, predictions)\n",
        "print(f\"MAE on Test Data: {mae}\")\n",
        "\n",
        "# Create a Plotly plot to show the predictions and actual values\n",
        "fig = go.Figure()\n",
        "\n",
        "# Plot actual values\n",
        "fig.add_trace(go.Scatter(x=test_dates, y=y_test.flatten(), mode='lines', name='Actual Values', line=dict(color='black', dash='dash')))\n",
        "\n",
        "# Plot predictions\n",
        "fig.add_trace(go.Scatter(x=test_dates, y=predictions.flatten(), mode='lines', name=f'Predictions (MAE: {mae:.4f})', line=dict(color='blue')))\n",
        "\n",
        "# Add layout details\n",
        "fig.update_layout(\n",
        "    title=\"LSTM Model Predictions vs Actual Values\",\n",
        "    xaxis_title=\"Date\",\n",
        "    yaxis_title=\"Stock Price\",\n",
        "    legend_title=\"Legend\",\n",
        "    hovermode=\"x unified\"\n",
        ")\n",
        "\n",
        "# Show the plot\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NkTo26Ev6kjN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "ikaJR0iEha9c",
        "outputId": "85215369-ec6f-45d9-e99f-819695b6a544"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n\\nimport numpy as np\\nimport pandas as pd\\nimport yfinance as yf\\nimport tensorflow as tf\\nfrom tensorflow.keras.models import Sequential\\nfrom tensorflow.keras.layers import LSTM, Dense, Dropout\\nfrom tensorflow.keras.optimizers import Adam\\nfrom tensorflow.keras.callbacks import EarlyStopping\\nfrom sklearn.metrics import mean_absolute_error\\nfrom sklearn.preprocessing import MinMaxScaler\\nimport plotly.graph_objects as go\\n\\n!pip install tensorflow-gpu\\nprint(\"Num GPUs Available: \", len(tf.config.list_physical_devices(\\'GPU\\')))\\n\\n'"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "!pip install tensorflow-gpu\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
        "\n",
        "\"\"\"\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
