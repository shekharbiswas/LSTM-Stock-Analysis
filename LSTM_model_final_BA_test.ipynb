{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z1h2cxODjgJy"
      },
      "source": [
        "# **Detailed Analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eu79nL0XsKPv"
      },
      "source": [
        "( The description of figure will change- it is just an sample to mention the concepts)\n",
        "\n",
        "- Best Configuration\n",
        "- Best Configuration: 30 years, batch size 32, epochs 80\n",
        "- MAE: 3.2587\n",
        "\n",
        "\n",
        "- Training Loss: 2.3098\n",
        "- Validation Loss: 34.2010\n",
        "\n",
        "**Observations and Implications**\n",
        "\n",
        "- **Training Loss:**\n",
        "\n",
        "Lowest Training Loss: Generally observed with configurations involving 30 years of data and higher epochs (e.g., epochs 60 or 80).\n",
        "Trends: Training loss decreases as the number of epochs increases, which indicates that the model is fitting the training data better with more training time. However, very high training loss values in configurations with fewer epochs or different batch sizes suggest that the model may not have enough time or capacity to learn effectively.\n",
        "Validation Loss:\n",
        "\n",
        "- **Lowest Validation Loss:**\n",
        "\n",
        "Achieved with the configuration of 30 years, batch size 32, and 80 epochs, which is the same as the configuration with the best MAE.\n",
        "\n",
        "- **Trends:** Validation loss also tends to decrease with more epochs up to a point, showing that the model generalizes better with more training. However, the validation loss does not decrease as much in some cases where the training loss is very low. This could suggest overfitting, where the model performs well on training data but less well on unseen data.\n",
        "\n",
        "- **Comparison of Training Loss and Validation Loss:**\n",
        "\n",
        "- **Overfitting:** When training loss is significantly lower than validation loss, it often indicates overfitting. The model learns to perform well on the training data but struggles with new, unseen data. For example, in the configuration with 30 years of data and a batch size of 64, the training loss is quite low, but the validation loss is relatively higher, suggesting overfitting.\n",
        "\n",
        "- **Generalization:** The best configuration (30 years, batch size 32, epochs 80) shows relatively balanced training and validation losses, indicating good generalization. The validation loss is higher than the training loss but not excessively so, which is a positive sign.\n",
        "\n",
        "**Summary**\n",
        "\n",
        "\n",
        "- **Best Overall Performance:**\n",
        "\n",
        "- The configuration with 30 years of data, batch size 32, and 80 epochs is the best based on MAE, training loss, and validation loss. This setup provides a good balance between fitting the training data and generalizing to unseen data.\n",
        "Training vs. Validation Loss:\n",
        "\n",
        "- A lower training loss compared to validation loss suggests that while the model fits the training data well, there might be some overfitting. The goal is to minimize both losses and ensure that they are as close as possible, which indicates the model's ability to generalize.\n",
        "Practical Recommendations:\n",
        "\n",
        "- Monitor Overfitting: Keep an eye on the gap between training and validation loss. Consider regularization techniques or early stopping to mitigate overfitting if necessary.\n",
        "\n",
        "- Continue Experimenting: Further experiment with different configurations or techniques (e.g., more data, different architectures) if we aim to improve performance even more.\n",
        "\n",
        "\n",
        "- By considering both training and validation losses along with MAE, we can get a more comprehensive understanding of our modelâ€™s performance and make more informed decisions about adjustments and improvements."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-OplbrRcapY"
      },
      "source": [
        "![image.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2aRtOlQfstm"
      },
      "source": [
        "We included a parameter to check for overfitting in the code by monitoring the training and validation losses during the model training process. Overfitting occurs when the model performs well on the training data but poorly on the validation data, indicating that it has learned the training data too well (including noise) and is not generalizing to unseen data.\n",
        "\n",
        "Steps :\n",
        "\n",
        "- Split the Training Data: Split the training data into training and validation sets.\n",
        "- Track Training and Validation Losses: Use these sets to track the training and validation losses during the model fitting.\n",
        "- Early Stopping (Optional): Not implemented as it is not required and can alter the actual results by stopping early."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltwjP-jDc7kJ"
      },
      "source": [
        "## Random Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tUdAslA2jSRS",
        "outputId": "5576cf9e-5c32-4214-d3c0-95bda5a46366"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[*********************100%%**********************]  4 of 4 completed\n",
            "ERROR:yfinance:\n",
            "1 Failed download:\n",
            "ERROR:yfinance:['MSI']: OperationalError('database is locked')\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2/2 [==============================] - 0s 13ms/step\n",
            "Training with 50 years, batch_size 32, epochs 20 - MAE: 79.89817773728143\n",
            "Training Loss: 1.0715302228927612, Validation Loss: 109.54474639892578\n",
            "2/2 [==============================] - 0s 18ms/step\n",
            "Training with 50 years, batch_size 32, epochs 40 - MAE: 66.98471396309989\n",
            "Training Loss: 1.0015274286270142, Validation Loss: 96.8540267944336\n",
            "2/2 [==============================] - 0s 18ms/step\n",
            "Training with 50 years, batch_size 32, epochs 60 - MAE: 62.208534240722656\n",
            "Training Loss: 0.9687308669090271, Validation Loss: 93.07957458496094\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "Training with 50 years, batch_size 32, epochs 80 - MAE: 63.894925193181116\n",
            "Training Loss: 0.9574600458145142, Validation Loss: 94.71245574951172\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:5 out of the last 9 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f4b79558f70> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2/2 [==============================] - 0s 16ms/step\n",
            "Training with 50 years, batch_size 64, epochs 20 - MAE: 94.3357635982453\n",
            "Training Loss: 1.1722228527069092, Validation Loss: 123.95526885986328\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:6 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f435c7081f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2/2 [==============================] - 0s 16ms/step\n",
            "Training with 50 years, batch_size 64, epochs 40 - MAE: 76.03362031966921\n",
            "Training Loss: 1.028404951095581, Validation Loss: 107.89740753173828\n",
            "2/2 [==============================] - 0s 14ms/step\n",
            "Training with 50 years, batch_size 64, epochs 60 - MAE: 71.22289796859499\n",
            "Training Loss: 0.9808511734008789, Validation Loss: 103.65023040771484\n",
            "2/2 [==============================] - 1s 17ms/step\n",
            "Training with 50 years, batch_size 64, epochs 80 - MAE: 66.66553824288505\n",
            "Training Loss: 0.9628663063049316, Validation Loss: 99.81465911865234\n",
            "2/2 [==============================] - 0s 15ms/step\n",
            "Training with 40 years, batch_size 32, epochs 20 - MAE: 50.535488552517364\n",
            "Training Loss: 1.6967655420303345, Validation Loss: 104.52111053466797\n",
            "2/2 [==============================] - 0s 15ms/step\n",
            "Training with 40 years, batch_size 32, epochs 40 - MAE: 46.883788093688\n",
            "Training Loss: 1.6228028535842896, Validation Loss: 100.27164459228516\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "Training with 40 years, batch_size 32, epochs 60 - MAE: 41.227422805059526\n",
            "Training Loss: 1.5266956090927124, Validation Loss: 95.66853332519531\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "Training with 40 years, batch_size 32, epochs 80 - MAE: 44.56820024762835\n",
            "Training Loss: 1.4970617294311523, Validation Loss: 98.98796081542969\n",
            "2/2 [==============================] - 0s 16ms/step\n",
            "Training with 40 years, batch_size 64, epochs 20 - MAE: 66.51022484188988\n",
            "Training Loss: 1.9638434648513794, Validation Loss: 119.91205596923828\n",
            "2/2 [==============================] - 0s 18ms/step\n",
            "Training with 40 years, batch_size 64, epochs 40 - MAE: 48.84509592207651\n",
            "Training Loss: 1.5880719423294067, Validation Loss: 103.37897491455078\n",
            "2/2 [==============================] - 0s 16ms/step\n",
            "Training with 40 years, batch_size 64, epochs 60 - MAE: 43.59355744861421\n",
            "Training Loss: 1.5515663623809814, Validation Loss: 101.12154388427734\n",
            "2/2 [==============================] - 0s 16ms/step\n",
            "Training with 40 years, batch_size 64, epochs 80 - MAE: 46.39123196072049\n",
            "Training Loss: 1.4843086004257202, Validation Loss: 102.63066864013672\n",
            "2/2 [==============================] - 0s 14ms/step\n",
            "Training with 30 years, batch_size 32, epochs 20 - MAE: 25.657742454892112\n",
            "Training Loss: 4.413127899169922, Validation Loss: 93.045166015625\n",
            "2/2 [==============================] - 0s 21ms/step\n",
            "Training with 30 years, batch_size 32, epochs 40 - MAE: 4.324630010695684\n",
            "Training Loss: 3.20747447013855, Validation Loss: 55.948097229003906\n",
            "2/2 [==============================] - 0s 18ms/step\n",
            "Training with 30 years, batch_size 32, epochs 60 - MAE: 3.1616923014322915\n",
            "Training Loss: 2.655163288116455, Validation Loss: 38.21672058105469\n",
            "2/2 [==============================] - 0s 18ms/step\n",
            "Training with 30 years, batch_size 32, epochs 80 - MAE: 3.1704602922712053\n",
            "Training Loss: 2.5455758571624756, Validation Loss: 33.446598052978516\n",
            "2/2 [==============================] - 0s 16ms/step\n",
            "Training with 30 years, batch_size 64, epochs 20 - MAE: 64.74405742826916\n",
            "Training Loss: 6.586986541748047, Validation Loss: 128.90921020507812\n",
            "2/2 [==============================] - 0s 16ms/step\n",
            "Training with 30 years, batch_size 64, epochs 40 - MAE: 25.952932690817214\n",
            "Training Loss: 4.413556098937988, Validation Loss: 91.9782485961914\n",
            "2/2 [==============================] - 0s 18ms/step\n",
            "Training with 30 years, batch_size 64, epochs 60 - MAE: 4.763652498759921\n",
            "Training Loss: 3.7942311763763428, Validation Loss: 70.26441192626953\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "Training with 30 years, batch_size 64, epochs 80 - MAE: 5.970984080481151\n",
            "Training Loss: 3.1051418781280518, Validation Loss: 52.18048095703125\n",
            "2/2 [==============================] - 0s 15ms/step\n",
            "Training with 20 years, batch_size 32, epochs 20 - MAE: 21.61228458465092\n",
            "Training Loss: 29.923917770385742, Validation Loss: 40.9592170715332\n",
            "2/2 [==============================] - 0s 14ms/step\n",
            "Training with 20 years, batch_size 32, epochs 40 - MAE: 3.473507593548487\n",
            "Training Loss: 12.98932933807373, Validation Loss: 4.49205207824707\n",
            "2/2 [==============================] - 0s 18ms/step\n",
            "Training with 20 years, batch_size 32, epochs 60 - MAE: 5.340832180447048\n",
            "Training Loss: 6.128810882568359, Validation Loss: 5.312347412109375\n",
            "2/2 [==============================] - 0s 18ms/step\n",
            "Training with 20 years, batch_size 32, epochs 80 - MAE: 4.3018961104135665\n",
            "Training Loss: 5.795559406280518, Validation Loss: 5.035451889038086\n",
            "2/2 [==============================] - 0s 18ms/step\n",
            "Training with 20 years, batch_size 64, epochs 20 - MAE: 75.76727210150824\n",
            "Training Loss: 42.56554412841797, Validation Loss: 92.09220123291016\n",
            "2/2 [==============================] - 0s 18ms/step\n",
            "Training with 20 years, batch_size 64, epochs 40 - MAE: 22.76539999341208\n",
            "Training Loss: 29.7252254486084, Validation Loss: 42.41872787475586\n",
            "2/2 [==============================] - 0s 18ms/step\n",
            "Training with 20 years, batch_size 64, epochs 60 - MAE: 3.7151690286303323\n",
            "Training Loss: 18.969295501708984, Validation Loss: 7.325584411621094\n",
            "2/2 [==============================] - 0s 19ms/step\n",
            "Training with 20 years, batch_size 64, epochs 80 - MAE: 3.9239378429594494\n",
            "Training Loss: 9.070098876953125, Validation Loss: 4.889264106750488\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "Training with 10 years, batch_size 32, epochs 20 - MAE: 77.4356220790318\n",
            "Training Loss: 113.08805084228516, Validation Loss: 85.37523651123047\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "Training with 10 years, batch_size 32, epochs 40 - MAE: 28.408668154761905\n",
            "Training Loss: 76.8730697631836, Validation Loss: 40.57707595825195\n",
            "2/2 [==============================] - 0s 15ms/step\n",
            "Training with 10 years, batch_size 32, epochs 60 - MAE: 4.237376137385293\n",
            "Training Loss: 39.71371078491211, Validation Loss: 6.375057697296143\n",
            "2/2 [==============================] - 0s 14ms/step\n",
            "Training with 10 years, batch_size 32, epochs 80 - MAE: 5.515811011904762\n",
            "Training Loss: 17.552234649658203, Validation Loss: 5.272933483123779\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "Training with 10 years, batch_size 64, epochs 20 - MAE: 124.55531377640982\n",
            "Training Loss: 158.7591094970703, Validation Loss: 132.49496459960938\n",
            "2/2 [==============================] - 0s 17ms/step\n",
            "Training with 10 years, batch_size 64, epochs 40 - MAE: 79.46285853310236\n",
            "Training Loss: 113.915771484375, Validation Loss: 87.40247344970703\n",
            "2/2 [==============================] - 0s 15ms/step\n",
            "Training with 10 years, batch_size 64, epochs 60 - MAE: 43.67219834100632\n",
            "Training Loss: 85.7115478515625, Validation Loss: 53.239501953125\n",
            "2/2 [==============================] - 0s 18ms/step\n",
            "Training with 10 years, batch_size 64, epochs 80 - MAE: 18.957598973834326\n",
            "Training Loss: 70.8563232421875, Validation Loss: 33.12498474121094\n",
            "Best Configuration - Years: 30, Batch Size: 32, Epochs: 60 - MAE: 3.1616923014322915\n"
          ]
        }
      ],
      "source": [
        "# Include overfitting parameter\n",
        "# No EARLY STOP\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "##########################################################################\n",
        "# Configuration for the LSTM model (Random Search - ******IMPORTANT******)\n",
        "##########################################################################\n",
        "config = {'units': 100, 'dropout': 0.2, 'learning_rate': 0.001}\n",
        "\n",
        "\n",
        "\n",
        "# Download the data\n",
        "start_date = '1974-01-01'\n",
        "end_date = '2024-08-10'\n",
        "all_stocks = ['BA', 'MSI', 'DE', 'SPGI']\n",
        "df = yf.download(all_stocks, start=start_date, end=end_date)['Adj Close']\n",
        "df = df.reset_index()\n",
        "\n",
        "# Prepare the BA stock data\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "df.set_index('Date', inplace=True)\n",
        "df = df['BA']\n",
        "\n",
        "# Preprocess Data\n",
        "def preprocess_data(df, window_size):\n",
        "    scaled_data = df.values.reshape(-1, 1)\n",
        "\n",
        "    X, y, dates = [], [], []\n",
        "    for i in range(window_size, len(df)):\n",
        "        X.append(scaled_data[i-window_size:i, 0])\n",
        "        y.append(scaled_data[i, 0])\n",
        "        dates.append(df.index[i])  # Collect dates for plotting\n",
        "\n",
        "    X, y = np.array(X), np.array(y)\n",
        "    X = np.reshape(X, (X.shape[0], X.shape[1], 1))  # Reshape for LSTM [samples, time steps, features]\n",
        "\n",
        "    return X, y, None, dates\n",
        "\n",
        "# Define LSTM Model\n",
        "def create_lstm_model(units, dropout, learning_rate):\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(units=units, return_sequences=False, input_shape=(90, 1)))\n",
        "    model.add(Dropout(dropout))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_absolute_error')\n",
        "    return model\n",
        "\n",
        "# Train and evaluate each model configuration\n",
        "def train_and_evaluate(X_train, y_train, X_val, y_val, X_test, y_test, hyperparams, epochs, batch_size):\n",
        "        model = create_lstm_model(hyperparams['units'], hyperparams['dropout'], hyperparams['learning_rate'])\n",
        "\n",
        "        # Adding EarlyStopping callback to prevent overfitting\n",
        "        early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "        history = model.fit(\n",
        "            X_train, y_train,\n",
        "            validation_data=(X_val, y_val),\n",
        "            epochs=epochs,\n",
        "            batch_size=batch_size,\n",
        "            verbose=0\n",
        "        )\n",
        "\n",
        "        train_loss = history.history['loss']\n",
        "        val_loss = history.history['val_loss']\n",
        "        predictions = model.predict(X_test)\n",
        "\n",
        "        mae = mean_absolute_error(y_test, predictions)\n",
        "\n",
        "        return mae, predictions, train_loss, val_loss\n",
        "\n",
        "# Possible values for epochs and batch size\n",
        "batch_sizes = [32, 64]\n",
        "# batch_sizes = [32, 64, 128, 256]\n",
        "# epochs_list = [20, 40, 50, 75]\n",
        "epochs_list = [20, 40, 60, 80]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Varying training periods\n",
        "training_years = [50, 40, 30, 20, 10]\n",
        "results = {}\n",
        "\n",
        "# Test data (2024)\n",
        "test_start = '2024-01-01'\n",
        "test_end = '2024-08-10'\n",
        "df_test = df[test_start:test_end]\n",
        "\n",
        "# Grid Search for best batch size and epochs\n",
        "best_config = None\n",
        "best_mae = float('inf')\n",
        "best_predictions = None\n",
        "best_dates = None\n",
        "\n",
        "for years in training_years:\n",
        "    train_end = pd.to_datetime('2023-12-31')\n",
        "    train_start = train_end - pd.DateOffset(years=years)\n",
        "    df_train = df[train_start:train_end]\n",
        "\n",
        "    # Split into training and validation sets (80/20 split)\n",
        "    val_size = int(len(df_train) * 0.2)\n",
        "    df_val = df_train[-val_size:]\n",
        "    df_train = df_train[:-val_size]\n",
        "\n",
        "    window_size = 90\n",
        "    X_train, y_train, scaler, train_dates = preprocess_data(df_train, window_size)\n",
        "    X_val, y_val, _, val_dates = preprocess_data(df_val, window_size)\n",
        "    X_test, y_test, _, test_dates = preprocess_data(df_test, window_size)\n",
        "\n",
        "    for batch_size in batch_sizes:\n",
        "        for epochs in epochs_list:\n",
        "            mae, predictions, train_loss, val_loss = train_and_evaluate(\n",
        "                X_train, y_train, X_val, y_val, X_test, y_test, config, epochs, batch_size)\n",
        "\n",
        "            if mae < best_mae:\n",
        "                best_mae = mae\n",
        "                best_config = {'years': years, 'batch_size': batch_size, 'epochs': epochs}\n",
        "                best_predictions = predictions\n",
        "                best_dates = test_dates\n",
        "            print(f\"Training with {years} years, batch_size {batch_size}, epochs {epochs} - MAE: {mae}\")\n",
        "            print(f\"Training Loss: {train_loss[-1]}, Validation Loss: {val_loss[-1]}\")\n",
        "\n",
        "print(f\"Best Configuration - Years: {best_config['years']}, Batch Size: {best_config['batch_size']}, Epochs: {best_config['epochs']} - MAE: {best_mae}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 596
        },
        "id": "YoLTpJn1gzzU",
        "outputId": "5dd246cf-cb12-451c-f1a1-1deb761a1ce9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import plotly.graph_objects as go\n",
        "##################################\n",
        "# Define the best configuration\n",
        "#################################\n",
        "# IMPORTANT\n",
        "#################################\n",
        "# Best Configuration - Years: 10, Batch Size: 32, Epochs: 60 - MAE: 3.2200385199652777\n",
        "\n",
        "\n",
        "best_years = 10\n",
        "best_batch_size = 32\n",
        "best_epochs = 60\n",
        "\n",
        "##########################################################################\n",
        "# Configuration for the LSTM model (Random Search - ******IMPORTANT******)\n",
        "##########################################################################\n",
        "config = {'units': 100, 'dropout': 0.2, 'learning_rate': 0.001}\n",
        "\n",
        "\n",
        "# Download the data\n",
        "start_date = '1974-01-01'\n",
        "end_date = '2024-08-10'\n",
        "all_stocks = ['BA', 'MSI', 'DE', 'SPGI']\n",
        "df = yf.download(all_stocks, start=start_date, end=end_date)['Adj Close']\n",
        "df = df.reset_index()\n",
        "\n",
        "test_start = '2024-01-01'\n",
        "test_end = '2024-08-10'\n",
        "\n",
        "# Prepare the BA stock data\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "df.set_index('Date', inplace=True)\n",
        "df = df['BA']\n",
        "\n",
        "# Preprocess Data\n",
        "def preprocess_data(df, window_size):\n",
        "    #scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    #scaled_data = scaler.fit_transform(df.values.reshape(-1, 1))\n",
        "\n",
        "    scaled_data = df.values.reshape(-1, 1)\n",
        "\n",
        "    X, y, dates = [], [], []\n",
        "    for i in range(window_size, len(df)):\n",
        "        X.append(scaled_data[i-window_size:i, 0])\n",
        "        y.append(scaled_data[i, 0])\n",
        "        dates.append(df.index[i])  # Collect dates for plotting\n",
        "\n",
        "    X, y = np.array(X), np.array(y)\n",
        "    X = np.reshape(X, (X.shape[0], X.shape[1], 1))  # Reshape for LSTM [samples, time steps, features]\n",
        "\n",
        "    return X, y, None, dates\n",
        "\n",
        "# Define LSTM Model\n",
        "def create_lstm_model(units, dropout, learning_rate):\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(units=units, return_sequences=False, input_shape=(90, 1)))\n",
        "    model.add(Dropout(dropout))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_absolute_error')\n",
        "    return model\n",
        "\n",
        "# Train and evaluate each model configuration\n",
        "def train_and_evaluate(X_train, y_train, X_test, y_test, hyperparams, epochs, batch_size):\n",
        "        model = create_lstm_model(hyperparams['units'], hyperparams['dropout'], hyperparams['learning_rate'])\n",
        "        model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=0)\n",
        "        predictions = model.predict(X_test)\n",
        "        mae = mean_absolute_error(y_test, predictions)\n",
        "        return mae, predictions\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Prepare the data using the best configuration\n",
        "train_end = pd.to_datetime('2023-12-31')\n",
        "train_start = train_end - pd.DateOffset(years=best_years)\n",
        "df_train = df[train_start:train_end]\n",
        "df_test = df[test_start:test_end]\n",
        "\n",
        "# Prepare data\n",
        "window_size = 90\n",
        "X_train, y_train, scaler, train_dates = preprocess_data(df_train, window_size)\n",
        "X_test, y_test, _, test_dates = preprocess_data(df_test, window_size)\n",
        "\n",
        "# Train the model using the best configuration\n",
        "model = create_lstm_model(units=150, dropout=0.2, learning_rate=0.001)\n",
        "model.fit(X_train, y_train, epochs=best_epochs, batch_size=best_batch_size, verbose=0)\n",
        "\n",
        "# Make predictions on the test data\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "# Inverse transform the predictions and true values using the scaler\n",
        "#predictions = scaler.inverse_transform(predictions)\n",
        "#y_test = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
        "\n",
        "# Calculate MAE\n",
        "mae = mean_absolute_error(y_test, predictions)\n",
        "print(f\"MAE on Test Data: {mae}\")\n",
        "\n",
        "# Create a Plotly plot to show the predictions and actual values\n",
        "fig = go.Figure()\n",
        "\n",
        "# Plot actual values\n",
        "fig.add_trace(go.Scatter(x=test_dates, y=y_test.flatten(), mode='lines', name='Actual Values', line=dict(color='black', dash='dash')))\n",
        "\n",
        "# Plot predictions\n",
        "fig.add_trace(go.Scatter(x=test_dates, y=predictions.flatten(), mode='lines', name=f'Predictions (MAE: {mae:.4f})', line=dict(color='blue')))\n",
        "\n",
        "# Add layout details\n",
        "fig.update_layout(\n",
        "    title=\"LSTM Model Predictions vs Actual Values\",\n",
        "    xaxis_title=\"Date\",\n",
        "    yaxis_title=\"Stock Price\",\n",
        "    legend_title=\"Legend\",\n",
        "    hovermode=\"x unified\"\n",
        ")\n",
        "\n",
        "# Show the plot\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k1g1Yt7krioT"
      },
      "source": [
        "## Bayesian Optimization\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "x1Rlr3QocjB7",
        "outputId": "35c38576-ff4e-4cab-9e3c-a05eeae06b89"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[*********************100%%**********************]  4 of 4 completed\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2/2 [==============================] - 0s 30ms/step\n",
            "Training with 50 years, batch_size 32, epochs 20 - MAE: 72.69085087851873\n",
            "Training Loss: 0.7123160362243652, Validation Loss: 102.19564056396484\n",
            "2/2 [==============================] - 0s 29ms/step\n",
            "Training with 50 years, batch_size 32, epochs 40 - MAE: 63.52500249469091\n",
            "Training Loss: 0.6632856130599976, Validation Loss: 94.6717300415039\n",
            "2/2 [==============================] - 0s 33ms/step\n",
            "Training with 50 years, batch_size 32, epochs 60 - MAE: 63.87140982491629\n",
            "Training Loss: 0.6446781158447266, Validation Loss: 97.92396545410156\n",
            "2/2 [==============================] - 0s 35ms/step\n",
            "Training with 50 years, batch_size 32, epochs 80 - MAE: 62.3197274586511\n",
            "Training Loss: 0.608905553817749, Validation Loss: 93.3561019897461\n",
            "2/2 [==============================] - 0s 34ms/step\n",
            "Training with 50 years, batch_size 64, epochs 20 - MAE: 89.2814683460054\n",
            "Training Loss: 0.7429695129394531, Validation Loss: 119.33419799804688\n",
            "2/2 [==============================] - 0s 31ms/step\n",
            "Training with 50 years, batch_size 64, epochs 40 - MAE: 67.41183399018787\n",
            "Training Loss: 0.6391971707344055, Validation Loss: 101.84239959716797\n",
            "2/2 [==============================] - 0s 31ms/step\n",
            "Training with 50 years, batch_size 64, epochs 60 - MAE: 65.95961240738157\n",
            "Training Loss: 0.6187973618507385, Validation Loss: 101.4039306640625\n",
            "2/2 [==============================] - 0s 34ms/step\n",
            "Training with 50 years, batch_size 64, epochs 80 - MAE: 65.22190202985492\n",
            "Training Loss: 0.6165168285369873, Validation Loss: 98.35997009277344\n",
            "2/2 [==============================] - 0s 27ms/step\n",
            "Training with 40 years, batch_size 32, epochs 20 - MAE: 47.81797960069444\n",
            "Training Loss: 1.0629997253417969, Validation Loss: 101.9623031616211\n",
            "2/2 [==============================] - 0s 35ms/step\n",
            "Training with 40 years, batch_size 32, epochs 40 - MAE: 42.549525911845855\n",
            "Training Loss: 1.012258768081665, Validation Loss: 97.54186248779297\n",
            "2/2 [==============================] - 0s 36ms/step\n",
            "Training with 40 years, batch_size 32, epochs 60 - MAE: 40.36228264702691\n",
            "Training Loss: 0.9767297506332397, Validation Loss: 108.36595153808594\n",
            "2/2 [==============================] - 0s 31ms/step\n",
            "Training with 40 years, batch_size 32, epochs 80 - MAE: 41.27398730081225\n",
            "Training Loss: 0.9591765999794006, Validation Loss: 96.09539031982422\n",
            "2/2 [==============================] - 0s 35ms/step\n",
            "Training with 40 years, batch_size 64, epochs 20 - MAE: 54.65837194049169\n",
            "Training Loss: 1.0761919021606445, Validation Loss: 109.08551025390625\n",
            "2/2 [==============================] - 0s 30ms/step\n",
            "Training with 40 years, batch_size 64, epochs 40 - MAE: 46.430604722764755\n",
            "Training Loss: 1.009844183921814, Validation Loss: 106.58932495117188\n",
            "2/2 [==============================] - 0s 33ms/step\n",
            "Training with 40 years, batch_size 64, epochs 60 - MAE: 44.56228686135913\n",
            "Training Loss: 0.9857701659202576, Validation Loss: 102.25090026855469\n",
            "2/2 [==============================] - 0s 36ms/step\n",
            "Training with 40 years, batch_size 64, epochs 80 - MAE: 40.833361186678445\n",
            "Training Loss: 0.9716151356697083, Validation Loss: 95.29682159423828\n",
            "2/2 [==============================] - 0s 31ms/step\n",
            "Training with 30 years, batch_size 32, epochs 20 - MAE: 9.454319302997892\n",
            "Training Loss: 2.980630874633789, Validation Loss: 76.66569519042969\n",
            "2/2 [==============================] - 0s 29ms/step\n",
            "Training with 30 years, batch_size 32, epochs 40 - MAE: 3.901917957124256\n",
            "Training Loss: 2.0908448696136475, Validation Loss: 61.65432357788086\n",
            "2/2 [==============================] - 0s 30ms/step\n",
            "Training with 30 years, batch_size 32, epochs 60 - MAE: 3.2845105367993552\n",
            "Training Loss: 1.8682843446731567, Validation Loss: 52.62753677368164\n",
            "2/2 [==============================] - 0s 30ms/step\n",
            "Training with 30 years, batch_size 32, epochs 80 - MAE: 3.856624542720734\n",
            "Training Loss: 1.7065891027450562, Validation Loss: 36.79867172241211\n",
            "2/2 [==============================] - 0s 31ms/step\n",
            "Training with 30 years, batch_size 64, epochs 20 - MAE: 46.525575183686755\n",
            "Training Loss: 4.599348545074463, Validation Loss: 112.59324645996094\n",
            "2/2 [==============================] - 0s 29ms/step\n",
            "Training with 30 years, batch_size 64, epochs 40 - MAE: 4.961942884657118\n",
            "Training Loss: 2.823233127593994, Validation Loss: 76.69799041748047\n",
            "2/2 [==============================] - 0s 30ms/step\n",
            "Training with 30 years, batch_size 64, epochs 60 - MAE: 3.9353208996000744\n",
            "Training Loss: 2.092038154602051, Validation Loss: 48.31304931640625\n",
            "2/2 [==============================] - 0s 32ms/step\n",
            "Training with 30 years, batch_size 64, epochs 80 - MAE: 5.194036574590774\n",
            "Training Loss: 1.9868698120117188, Validation Loss: 42.78776931762695\n",
            "2/2 [==============================] - 0s 23ms/step\n",
            "Training with 20 years, batch_size 32, epochs 20 - MAE: 3.254725864955357\n",
            "Training Loss: 21.731821060180664, Validation Loss: 13.171493530273438\n",
            "2/2 [==============================] - 0s 30ms/step\n",
            "Training with 20 years, batch_size 32, epochs 40 - MAE: 3.5987720792255704\n",
            "Training Loss: 4.0108418464660645, Validation Loss: 4.541766166687012\n",
            "2/2 [==============================] - 0s 33ms/step\n",
            "Training with 20 years, batch_size 32, epochs 60 - MAE: 4.355837625170511\n",
            "Training Loss: 3.927899122238159, Validation Loss: 5.215782165527344\n",
            "2/2 [==============================] - 0s 27ms/step\n",
            "Training with 20 years, batch_size 32, epochs 80 - MAE: 4.85143800765749\n",
            "Training Loss: 3.5876948833465576, Validation Loss: 6.058656692504883\n",
            "2/2 [==============================] - 0s 31ms/step\n",
            "Training with 20 years, batch_size 64, epochs 20 - MAE: 42.52576071118552\n",
            "Training Loss: 32.847957611083984, Validation Loss: 59.763065338134766\n",
            "2/2 [==============================] - 0s 31ms/step\n",
            "Training with 20 years, batch_size 64, epochs 40 - MAE: 3.19075932578435\n",
            "Training Loss: 19.038875579833984, Validation Loss: 7.5932297706604\n",
            "2/2 [==============================] - 0s 31ms/step\n",
            "Training with 20 years, batch_size 64, epochs 60 - MAE: 4.38083249046689\n",
            "Training Loss: 6.027464389801025, Validation Loss: 5.207588195800781\n",
            "2/2 [==============================] - 0s 34ms/step\n",
            "Training with 20 years, batch_size 64, epochs 80 - MAE: 3.885313730391245\n",
            "Training Loss: 4.204771518707275, Validation Loss: 4.543896198272705\n",
            "2/2 [==============================] - 0s 25ms/step\n",
            "Training with 10 years, batch_size 32, epochs 20 - MAE: 47.904740590897816\n",
            "Training Loss: 89.5686264038086, Validation Loss: 56.78556823730469\n",
            "2/2 [==============================] - 0s 25ms/step\n",
            "Training with 10 years, batch_size 32, epochs 40 - MAE: 4.564462570917039\n",
            "Training Loss: 33.88250732421875, Validation Loss: 4.8772454261779785\n",
            "2/2 [==============================] - 0s 31ms/step\n",
            "Training with 10 years, batch_size 32, epochs 60 - MAE: 3.4194737994481645\n",
            "Training Loss: 11.478399276733398, Validation Loss: 3.602391004562378\n",
            "2/2 [==============================] - 0s 32ms/step\n",
            "Training with 10 years, batch_size 32, epochs 80 - MAE: 4.730795239645337\n",
            "Training Loss: 8.597220420837402, Validation Loss: 3.859130859375\n",
            "2/2 [==============================] - 0s 37ms/step\n",
            "Training with 10 years, batch_size 64, epochs 20 - MAE: 101.00233035617404\n",
            "Training Loss: 135.9586944580078, Validation Loss: 108.94195556640625\n",
            "2/2 [==============================] - 0s 35ms/step\n",
            "Training with 10 years, batch_size 64, epochs 40 - MAE: 38.483921111576144\n",
            "Training Loss: 82.06399536132812, Validation Loss: 48.61463928222656\n",
            "2/2 [==============================] - 0s 36ms/step\n",
            "Training with 10 years, batch_size 64, epochs 60 - MAE: 3.1677490718781\n",
            "Training Loss: 55.3930778503418, Validation Loss: 15.897231101989746\n",
            "2/2 [==============================] - 0s 31ms/step\n",
            "Training with 10 years, batch_size 64, epochs 80 - MAE: 3.169802711123512\n",
            "Training Loss: 37.956363677978516, Validation Loss: 4.66777229309082\n",
            "Best Configuration - Years: 10, Batch Size: 64, Epochs: 60 - MAE: 3.1677490718781\n"
          ]
        }
      ],
      "source": [
        "# Include overfitting parameter\n",
        "\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "##########################################################################\n",
        "# Configuration for the LSTM model (Bayesian  - ******IMPORTANT******)\n",
        "##########################################################################\n",
        "config = {'units': 150, 'dropout': 0.1, 'learning_rate': 0.001}\n",
        "\n",
        "\n",
        "# Download the data\n",
        "start_date = '1974-01-01'\n",
        "end_date = '2024-08-10'\n",
        "all_stocks = ['BA', 'MSI', 'DE', 'SPGI']\n",
        "df = yf.download(all_stocks, start=start_date, end=end_date)['Adj Close']\n",
        "df = df.reset_index()\n",
        "\n",
        "# Prepare the BA stock data\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "df.set_index('Date', inplace=True)\n",
        "df = df['BA']\n",
        "\n",
        "# Preprocess Data\n",
        "def preprocess_data(df, window_size):\n",
        "    scaled_data = df.values.reshape(-1, 1)\n",
        "\n",
        "    X, y, dates = [], [], []\n",
        "    for i in range(window_size, len(df)):\n",
        "        X.append(scaled_data[i-window_size:i, 0])\n",
        "        y.append(scaled_data[i, 0])\n",
        "        dates.append(df.index[i])  # Collect dates for plotting\n",
        "\n",
        "    X, y = np.array(X), np.array(y)\n",
        "    X = np.reshape(X, (X.shape[0], X.shape[1], 1))  # Reshape for LSTM [samples, time steps, features]\n",
        "\n",
        "    return X, y, None, dates\n",
        "\n",
        "# Define LSTM Model\n",
        "def create_lstm_model(units, dropout, learning_rate):\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(units=units, return_sequences=False, input_shape=(90, 1)))\n",
        "    model.add(Dropout(dropout))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_absolute_error')\n",
        "    return model\n",
        "\n",
        "# Train and evaluate each model configuration\n",
        "def train_and_evaluate(X_train, y_train, X_val, y_val, X_test, y_test, hyperparams, epochs, batch_size):\n",
        "        model = create_lstm_model(hyperparams['units'], hyperparams['dropout'], hyperparams['learning_rate'])\n",
        "\n",
        "        # Adding EarlyStopping callback to prevent overfitting\n",
        "        early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "        history = model.fit(\n",
        "            X_train, y_train,\n",
        "            validation_data=(X_val, y_val),\n",
        "            epochs=epochs,\n",
        "            batch_size=batch_size,\n",
        "            verbose=0\n",
        "        )\n",
        "\n",
        "        train_loss = history.history['loss']\n",
        "        val_loss = history.history['val_loss']\n",
        "        predictions = model.predict(X_test)\n",
        "\n",
        "        mae = mean_absolute_error(y_test, predictions)\n",
        "\n",
        "        return mae, predictions, train_loss, val_loss\n",
        "\n",
        "\n",
        "\n",
        "# Possible values for epochs and batch size\n",
        "batch_sizes = [32, 64]\n",
        "epochs_list = [20, 40, 60, 80]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Varying training periods\n",
        "# Ex- If we are using a 20-year training period, we will use data from 20 years before\n",
        "# the end date of your training set. The end date of the training data is fixed at 2023-12-31,\n",
        "# and the start date is calculated by subtracting 20 years from this end date.\n",
        "\n",
        "training_years = [50, 40, 30, 20, 10]\n",
        "results = {}\n",
        "\n",
        "# Test data (2024)\n",
        "test_start = '2024-01-01'\n",
        "test_end = '2024-08-10'\n",
        "df_test = df[test_start:test_end]\n",
        "\n",
        "# Grid Search for best batch size and epochs\n",
        "best_config = None\n",
        "best_mae = float('inf')\n",
        "best_predictions = None\n",
        "best_dates = None\n",
        "\n",
        "for years in training_years:\n",
        "    train_end = pd.to_datetime('2023-12-31')\n",
        "    train_start = train_end - pd.DateOffset(years=years)\n",
        "    df_train = df[train_start:train_end]\n",
        "\n",
        "    # Split into training and validation sets (80/20 split)\n",
        "    val_size = int(len(df_train) * 0.2)\n",
        "    df_val = df_train[-val_size:]\n",
        "    df_train = df_train[:-val_size]\n",
        "\n",
        "    window_size = 90\n",
        "    X_train, y_train, scaler, train_dates = preprocess_data(df_train, window_size)\n",
        "    X_val, y_val, _, val_dates = preprocess_data(df_val, window_size)\n",
        "    X_test, y_test, _, test_dates = preprocess_data(df_test, window_size)\n",
        "\n",
        "    for batch_size in batch_sizes:\n",
        "        for epochs in epochs_list:\n",
        "            mae, predictions, train_loss, val_loss = train_and_evaluate(\n",
        "                X_train, y_train, X_val, y_val, X_test, y_test, config, epochs, batch_size)\n",
        "\n",
        "            if mae < best_mae:\n",
        "                best_mae = mae\n",
        "                best_config = {'years': years, 'batch_size': batch_size, 'epochs': epochs}\n",
        "                best_predictions = predictions\n",
        "                best_dates = test_dates\n",
        "            print(f\"Training with {years} years, batch_size {batch_size}, epochs {epochs} - MAE: {mae}\")\n",
        "            print(f\"Training Loss: {train_loss[-1]}, Validation Loss: {val_loss[-1]}\")\n",
        "\n",
        "print(f\"Best Configuration - Years: {best_config['years']}, Batch Size: {best_config['batch_size']}, Epochs: {best_config['epochs']} - MAE: {best_mae}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "gHRth4FYg2mX",
        "outputId": "64c59635-138d-445d-b998-ad348827ffec"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[*********************100%%**********************]  4 of 4 completed\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2/2 [==============================] - 0s 33ms/step\n",
            "MAE on Test Data: 3.115085177951389\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.24.1.min.js\"></script>                <div id=\"b90af6c9-5e7a-4e31-8fab-c05dd0a36658\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"b90af6c9-5e7a-4e31-8fab-c05dd0a36658\")) {                    Plotly.newPlot(                        \"b90af6c9-5e7a-4e31-8fab-c05dd0a36658\",                        [{\"line\":{\"color\":\"black\",\"dash\":\"dash\"},\"mode\":\"lines\",\"name\":\"Actual Values\",\"x\":[\"2024-05-10T00:00:00\",\"2024-05-13T00:00:00\",\"2024-05-14T00:00:00\",\"2024-05-15T00:00:00\",\"2024-05-16T00:00:00\",\"2024-05-17T00:00:00\",\"2024-05-20T00:00:00\",\"2024-05-21T00:00:00\",\"2024-05-22T00:00:00\",\"2024-05-23T00:00:00\",\"2024-05-24T00:00:00\",\"2024-05-28T00:00:00\",\"2024-05-29T00:00:00\",\"2024-05-30T00:00:00\",\"2024-05-31T00:00:00\",\"2024-06-03T00:00:00\",\"2024-06-04T00:00:00\",\"2024-06-05T00:00:00\",\"2024-06-06T00:00:00\",\"2024-06-07T00:00:00\",\"2024-06-10T00:00:00\",\"2024-06-11T00:00:00\",\"2024-06-12T00:00:00\",\"2024-06-13T00:00:00\",\"2024-06-14T00:00:00\",\"2024-06-17T00:00:00\",\"2024-06-18T00:00:00\",\"2024-06-20T00:00:00\",\"2024-06-21T00:00:00\",\"2024-06-24T00:00:00\",\"2024-06-25T00:00:00\",\"2024-06-26T00:00:00\",\"2024-06-27T00:00:00\",\"2024-06-28T00:00:00\",\"2024-07-01T00:00:00\",\"2024-07-02T00:00:00\",\"2024-07-03T00:00:00\",\"2024-07-05T00:00:00\",\"2024-07-08T00:00:00\",\"2024-07-09T00:00:00\",\"2024-07-10T00:00:00\",\"2024-07-11T00:00:00\",\"2024-07-12T00:00:00\",\"2024-07-15T00:00:00\",\"2024-07-16T00:00:00\",\"2024-07-17T00:00:00\",\"2024-07-18T00:00:00\",\"2024-07-19T00:00:00\",\"2024-07-22T00:00:00\",\"2024-07-23T00:00:00\",\"2024-07-24T00:00:00\",\"2024-07-25T00:00:00\",\"2024-07-26T00:00:00\",\"2024-07-29T00:00:00\",\"2024-07-30T00:00:00\",\"2024-07-31T00:00:00\",\"2024-08-01T00:00:00\",\"2024-08-02T00:00:00\",\"2024-08-05T00:00:00\",\"2024-08-06T00:00:00\",\"2024-08-07T00:00:00\",\"2024-08-08T00:00:00\",\"2024-08-09T00:00:00\"],\"y\":[178.50999450683594,178.44000244140625,180.75999450683594,176.99000549316406,182.9600067138672,184.9499969482422,186.61000061035156,184.77999877929688,186.27999877929688,172.2100067138672,174.52000427246094,175.0800018310547,171.64999389648438,172.75,177.61000061035156,184.57000732421875,188.6199951171875,189.85000610351562,191.4199981689453,190.24000549316406,190.1199951171875,185.5,182.6699981689453,180.6999969482422,177.27000427246094,178.38999938964844,174.99000549316406,176.3000030517578,176.55999755859375,179.10000610351562,175.10000610351562,178.5,182.50999450683594,182.00999450683594,186.6999969482422,185.4199981689453,184.30999755859375,184.8300018310547,185.83999633789062,183.24000549316406,183.72999572753906,183.91000366210938,182.30999755859375,179.11000061035156,186.0500030517578,184.83999633789062,180.22999572753906,179.6699981689453,178.89999389648438,186.47999572753906,180.07000732421875,184.35000610351562,186.88999938964844,185.42999267578125,186.86000061035156,190.60000610351562,178.30999755859375,169.9499969482422,167.02999877929688,165.0399932861328,163.24000549316406,168.8699951171875,167.91000366210938],\"type\":\"scatter\"},{\"line\":{\"color\":\"blue\"},\"mode\":\"lines\",\"name\":\"Predictions (MAE: 3.1151)\",\"x\":[\"2024-05-10T00:00:00\",\"2024-05-13T00:00:00\",\"2024-05-14T00:00:00\",\"2024-05-15T00:00:00\",\"2024-05-16T00:00:00\",\"2024-05-17T00:00:00\",\"2024-05-20T00:00:00\",\"2024-05-21T00:00:00\",\"2024-05-22T00:00:00\",\"2024-05-23T00:00:00\",\"2024-05-24T00:00:00\",\"2024-05-28T00:00:00\",\"2024-05-29T00:00:00\",\"2024-05-30T00:00:00\",\"2024-05-31T00:00:00\",\"2024-06-03T00:00:00\",\"2024-06-04T00:00:00\",\"2024-06-05T00:00:00\",\"2024-06-06T00:00:00\",\"2024-06-07T00:00:00\",\"2024-06-10T00:00:00\",\"2024-06-11T00:00:00\",\"2024-06-12T00:00:00\",\"2024-06-13T00:00:00\",\"2024-06-14T00:00:00\",\"2024-06-17T00:00:00\",\"2024-06-18T00:00:00\",\"2024-06-20T00:00:00\",\"2024-06-21T00:00:00\",\"2024-06-24T00:00:00\",\"2024-06-25T00:00:00\",\"2024-06-26T00:00:00\",\"2024-06-27T00:00:00\",\"2024-06-28T00:00:00\",\"2024-07-01T00:00:00\",\"2024-07-02T00:00:00\",\"2024-07-03T00:00:00\",\"2024-07-05T00:00:00\",\"2024-07-08T00:00:00\",\"2024-07-09T00:00:00\",\"2024-07-10T00:00:00\",\"2024-07-11T00:00:00\",\"2024-07-12T00:00:00\",\"2024-07-15T00:00:00\",\"2024-07-16T00:00:00\",\"2024-07-17T00:00:00\",\"2024-07-18T00:00:00\",\"2024-07-19T00:00:00\",\"2024-07-22T00:00:00\",\"2024-07-23T00:00:00\",\"2024-07-24T00:00:00\",\"2024-07-25T00:00:00\",\"2024-07-26T00:00:00\",\"2024-07-29T00:00:00\",\"2024-07-30T00:00:00\",\"2024-07-31T00:00:00\",\"2024-08-01T00:00:00\",\"2024-08-02T00:00:00\",\"2024-08-05T00:00:00\",\"2024-08-06T00:00:00\",\"2024-08-07T00:00:00\",\"2024-08-08T00:00:00\",\"2024-08-09T00:00:00\"],\"y\":[180.9899444580078,177.38406372070312,177.51988220214844,180.45289611816406,175.3012237548828,184.3146514892578,184.94972229003906,186.86785888671875,183.5293426513672,185.80271911621094,167.3153076171875,174.73138427734375,173.43206787109375,170.46995544433594,172.3379669189453,178.47215270996094,186.5667266845703,189.80172729492188,189.82254028320312,190.92324829101562,189.03981018066406,189.1483612060547,183.34861755371094,180.8360137939453,178.79335021972656,175.25503540039062,177.8695526123047,173.07125854492188,176.16224670410156,175.67819213867188,179.4720458984375,173.21258544921875,179.00559997558594,182.7965087890625,181.70465087890625,187.43043518066406,184.4327392578125,183.34310913085938,183.80970764160156,185.25729370117188,181.84307861328125,183.02304077148438,182.9336700439453,181.14434814453125,177.26370239257812,187.1926727294922,184.1915740966797,178.5625762939453,178.11239624023438,177.37689208984375,188.1995086669922,178.36529541015625,185.22122192382812,186.26568603515625,184.68099975585938,186.3850860595703,190.64781188964844,174.68460083007812,166.36068725585938,163.3700408935547,163.211669921875,162.1732177734375,170.74559020996094],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"title\":{\"text\":\"LSTM Model Predictions vs Actual Values\"},\"xaxis\":{\"title\":{\"text\":\"Date\"}},\"yaxis\":{\"title\":{\"text\":\"Stock Price\"}},\"legend\":{\"title\":{\"text\":\"Legend\"}},\"hovermode\":\"x unified\"},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('b90af6c9-5e7a-4e31-8fab-c05dd0a36658');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "\n",
        "# Best Configuration - Years: 10, Batch Size: 64, Epochs: 80 - MAE: 3.0124608599950395\n",
        "\n",
        "\n",
        "##################################\n",
        "# Define the best configuration\n",
        "#################################\n",
        "# IMPORTANT\n",
        "#################################\n",
        "best_years = 10\n",
        "best_batch_size = 64\n",
        "best_epochs = 80\n",
        "\n",
        "##########################################################################\n",
        "# Configuration for the LSTM model (Bayesian  - ******IMPORTANT******)\n",
        "##########################################################################\n",
        "config = {'units': 150, 'dropout': 0.1, 'learning_rate': 0.001}\n",
        "\n",
        "\n",
        "# Download the data\n",
        "start_date = '1974-01-01'\n",
        "end_date = '2024-08-10'\n",
        "all_stocks = ['BA', 'MSI', 'DE', 'SPGI']\n",
        "df = yf.download(all_stocks, start=start_date, end=end_date)['Adj Close']\n",
        "df = df.reset_index()\n",
        "\n",
        "test_start = '2024-01-01'\n",
        "test_end = '2024-08-10'\n",
        "\n",
        "# Prepare the BA stock data\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "df.set_index('Date', inplace=True)\n",
        "df = df['BA']\n",
        "\n",
        "# Preprocess Data\n",
        "def preprocess_data(df, window_size):\n",
        "    #scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    #scaled_data = scaler.fit_transform(df.values.reshape(-1, 1))\n",
        "\n",
        "    scaled_data = df.values.reshape(-1, 1)\n",
        "\n",
        "    X, y, dates = [], [], []\n",
        "    for i in range(window_size, len(df)):\n",
        "        X.append(scaled_data[i-window_size:i, 0])\n",
        "        y.append(scaled_data[i, 0])\n",
        "        dates.append(df.index[i])  # Collect dates for plotting\n",
        "\n",
        "    X, y = np.array(X), np.array(y)\n",
        "    X = np.reshape(X, (X.shape[0], X.shape[1], 1))  # Reshape for LSTM [samples, time steps, features]\n",
        "\n",
        "    return X, y, None, dates\n",
        "\n",
        "# Define LSTM Model\n",
        "def create_lstm_model(units, dropout, learning_rate):\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(units=units, return_sequences=False, input_shape=(90, 1)))\n",
        "    model.add(Dropout(dropout))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_absolute_error')\n",
        "    return model\n",
        "\n",
        "# Train and evaluate each model configuration\n",
        "def train_and_evaluate(X_train, y_train, X_test, y_test, hyperparams, epochs, batch_size):\n",
        "        model = create_lstm_model(hyperparams['units'], hyperparams['dropout'], hyperparams['learning_rate'])\n",
        "        model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=0)\n",
        "        predictions = model.predict(X_test)\n",
        "        mae = mean_absolute_error(y_test, predictions)\n",
        "        return mae, predictions\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Prepare the data using the best configuration\n",
        "train_end = pd.to_datetime('2023-12-31')\n",
        "train_start = train_end - pd.DateOffset(years=best_years)\n",
        "df_train = df[train_start:train_end]\n",
        "df_test = df[test_start:test_end]\n",
        "\n",
        "# Prepare data\n",
        "window_size = 90\n",
        "X_train, y_train, scaler, train_dates = preprocess_data(df_train, window_size)\n",
        "X_test, y_test, _, test_dates = preprocess_data(df_test, window_size)\n",
        "\n",
        "# Train the model using the best configuration\n",
        "model = create_lstm_model(units=150, dropout=0.2, learning_rate=0.001)\n",
        "model.fit(X_train, y_train, epochs=best_epochs, batch_size=best_batch_size, verbose=0)\n",
        "\n",
        "# Make predictions on the test data\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "# Inverse transform the predictions and true values using the scaler\n",
        "#predictions = scaler.inverse_transform(predictions)\n",
        "#y_test = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
        "\n",
        "# Calculate MAE\n",
        "mae = mean_absolute_error(y_test, predictions)\n",
        "print(f\"MAE on Test Data: {mae}\")\n",
        "\n",
        "# Create a Plotly plot to show the predictions and actual values\n",
        "fig = go.Figure()\n",
        "\n",
        "# Plot actual values\n",
        "fig.add_trace(go.Scatter(x=test_dates, y=y_test.flatten(), mode='lines', name='Actual Values', line=dict(color='black', dash='dash')))\n",
        "\n",
        "# Plot predictions\n",
        "fig.add_trace(go.Scatter(x=test_dates, y=predictions.flatten(), mode='lines', name=f'Predictions (MAE: {mae:.4f})', line=dict(color='blue')))\n",
        "\n",
        "# Add layout details\n",
        "fig.update_layout(\n",
        "    title=\"LSTM Model Predictions vs Actual Values\",\n",
        "    xaxis_title=\"Date\",\n",
        "    yaxis_title=\"Stock Price\",\n",
        "    legend_title=\"Legend\",\n",
        "    hovermode=\"x unified\"\n",
        ")\n",
        "\n",
        "# Show the plot\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEUnKw6EfGnB"
      },
      "source": [
        "## Hyperband"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "_rBkM3DVeF07",
        "outputId": "bdedbb63-f757-4b25-e750-c4416f731658"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[*********************100%%**********************]  4 of 4 completed\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2/2 [==============================] - 0s 31ms/step\n",
            "Training with 50 years, batch_size 32, epochs 20 - MAE: 73.42035566057477\n",
            "Training Loss: 0.9401059150695801, Validation Loss: 105.07255554199219\n",
            "2/2 [==============================] - 0s 28ms/step\n",
            "Training with 50 years, batch_size 32, epochs 40 - MAE: 63.62293291848803\n",
            "Training Loss: 0.8982372879981995, Validation Loss: 93.39637756347656\n",
            "2/2 [==============================] - 0s 36ms/step\n",
            "Training with 50 years, batch_size 32, epochs 60 - MAE: 62.58793058849516\n",
            "Training Loss: 0.8497372269630432, Validation Loss: 94.72047424316406\n",
            "2/2 [==============================] - 0s 28ms/step\n",
            "Training with 50 years, batch_size 32, epochs 80 - MAE: 62.56660861060733\n",
            "Training Loss: 0.8335740566253662, Validation Loss: 93.5205078125\n",
            "2/2 [==============================] - 0s 33ms/step\n",
            "Training with 50 years, batch_size 64, epochs 20 - MAE: 85.26160527789403\n",
            "Training Loss: 0.9466325044631958, Validation Loss: 117.42871856689453\n",
            "2/2 [==============================] - 0s 32ms/step\n",
            "Training with 50 years, batch_size 64, epochs 40 - MAE: 69.74527910020616\n",
            "Training Loss: 0.8839511275291443, Validation Loss: 101.76484680175781\n",
            "2/2 [==============================] - 0s 36ms/step\n",
            "Training with 50 years, batch_size 64, epochs 60 - MAE: 65.87042841835627\n",
            "Training Loss: 0.8465246558189392, Validation Loss: 98.00811767578125\n",
            "2/2 [==============================] - 0s 35ms/step\n",
            "Training with 50 years, batch_size 64, epochs 80 - MAE: 66.02250598725819\n",
            "Training Loss: 0.838367760181427, Validation Loss: 97.3733139038086\n",
            "2/2 [==============================] - 0s 32ms/step\n",
            "Training with 40 years, batch_size 32, epochs 20 - MAE: 46.79264686221168\n",
            "Training Loss: 1.4406846761703491, Validation Loss: 100.86119079589844\n",
            "2/2 [==============================] - 0s 35ms/step\n",
            "Training with 40 years, batch_size 32, epochs 40 - MAE: 44.70985606360057\n",
            "Training Loss: 1.399768590927124, Validation Loss: 98.01789855957031\n",
            "2/2 [==============================] - 0s 31ms/step\n",
            "Training with 40 years, batch_size 32, epochs 60 - MAE: 43.53444514198909\n",
            "Training Loss: 1.3230342864990234, Validation Loss: 99.39871978759766\n",
            "2/2 [==============================] - 0s 32ms/step\n",
            "Training with 40 years, batch_size 32, epochs 80 - MAE: 41.851317874969\n",
            "Training Loss: 1.283467411994934, Validation Loss: 97.052978515625\n",
            "2/2 [==============================] - 0s 32ms/step\n",
            "Training with 40 years, batch_size 64, epochs 20 - MAE: 54.44820706806485\n",
            "Training Loss: 1.4766712188720703, Validation Loss: 110.68224334716797\n",
            "2/2 [==============================] - 0s 34ms/step\n",
            "Training with 40 years, batch_size 64, epochs 40 - MAE: 45.800258333720855\n",
            "Training Loss: 1.343507170677185, Validation Loss: 101.37642669677734\n",
            "2/2 [==============================] - 0s 31ms/step\n",
            "Training with 40 years, batch_size 64, epochs 60 - MAE: 43.28248305547805\n",
            "Training Loss: 1.3213118314743042, Validation Loss: 98.76383972167969\n",
            "2/2 [==============================] - 0s 34ms/step\n",
            "Training with 40 years, batch_size 64, epochs 80 - MAE: 44.776746477399556\n",
            "Training Loss: 1.3486124277114868, Validation Loss: 100.51976013183594\n",
            "2/2 [==============================] - 0s 33ms/step\n",
            "Training with 30 years, batch_size 32, epochs 20 - MAE: 11.141840132455977\n",
            "Training Loss: 3.643392562866211, Validation Loss: 79.29424285888672\n",
            "2/2 [==============================] - 0s 30ms/step\n",
            "Training with 30 years, batch_size 32, epochs 40 - MAE: 3.8513561430431547\n",
            "Training Loss: 2.4244751930236816, Validation Loss: 41.007362365722656\n",
            "2/2 [==============================] - 0s 23ms/step\n",
            "Training with 30 years, batch_size 32, epochs 60 - MAE: 4.201588706364707\n",
            "Training Loss: 2.3366472721099854, Validation Loss: 38.372291564941406\n",
            "2/2 [==============================] - 0s 32ms/step\n",
            "Training with 30 years, batch_size 32, epochs 80 - MAE: 7.83308362203931\n",
            "Training Loss: 2.3646535873413086, Validation Loss: 39.58161163330078\n",
            "2/2 [==============================] - 0s 35ms/step\n",
            "Training with 30 years, batch_size 64, epochs 20 - MAE: 44.495101202101935\n",
            "Training Loss: 4.874980926513672, Validation Loss: 109.10238647460938\n",
            "2/2 [==============================] - 0s 32ms/step\n",
            "Training with 30 years, batch_size 64, epochs 40 - MAE: 4.866798037574405\n",
            "Training Loss: 3.322061777114868, Validation Loss: 72.67733001708984\n",
            "2/2 [==============================] - 0s 32ms/step\n",
            "Training with 30 years, batch_size 64, epochs 60 - MAE: 3.746956234886533\n",
            "Training Loss: 2.8519299030303955, Validation Loss: 51.36357116699219\n",
            "2/2 [==============================] - 0s 30ms/step\n",
            "Training with 30 years, batch_size 64, epochs 80 - MAE: 5.4396597241598466\n",
            "Training Loss: 2.4428157806396484, Validation Loss: 48.25761032104492\n",
            "2/2 [==============================] - 0s 26ms/step\n",
            "Training with 20 years, batch_size 32, epochs 20 - MAE: 3.018435765826513\n",
            "Training Loss: 21.679018020629883, Validation Loss: 11.016389846801758\n",
            "2/2 [==============================] - 0s 33ms/step\n",
            "Training with 20 years, batch_size 32, epochs 40 - MAE: 6.673847501240079\n",
            "Training Loss: 5.75242280960083, Validation Loss: 8.260794639587402\n",
            "2/2 [==============================] - 0s 27ms/step\n",
            "Training with 20 years, batch_size 32, epochs 60 - MAE: 4.587972974020337\n",
            "Training Loss: 5.068207263946533, Validation Loss: 5.617791652679443\n",
            "2/2 [==============================] - 0s 29ms/step\n",
            "Training with 20 years, batch_size 32, epochs 80 - MAE: 3.8073926653180803\n",
            "Training Loss: 5.307889461517334, Validation Loss: 5.058821201324463\n",
            "2/2 [==============================] - 0s 31ms/step\n",
            "Training with 20 years, batch_size 64, epochs 20 - MAE: 48.85668630448599\n",
            "Training Loss: 34.87812423706055, Validation Loss: 65.58578491210938\n",
            "2/2 [==============================] - 0s 32ms/step\n",
            "Training with 20 years, batch_size 64, epochs 40 - MAE: 4.348435901460194\n",
            "Training Loss: 19.672760009765625, Validation Loss: 7.7950215339660645\n",
            "2/2 [==============================] - 0s 32ms/step\n",
            "Training with 20 years, batch_size 64, epochs 60 - MAE: 6.7552042158823165\n",
            "Training Loss: 7.464594841003418, Validation Loss: 7.387722969055176\n",
            "2/2 [==============================] - 0s 35ms/step\n",
            "Training with 20 years, batch_size 64, epochs 80 - MAE: 4.5307546948629716\n",
            "Training Loss: 5.042168617248535, Validation Loss: 5.201541900634766\n",
            "2/2 [==============================] - 0s 24ms/step\n",
            "Training with 10 years, batch_size 32, epochs 20 - MAE: 34.87494284009177\n",
            "Training Loss: 80.53829956054688, Validation Loss: 46.08977127075195\n",
            "2/2 [==============================] - 0s 23ms/step\n",
            "Training with 10 years, batch_size 32, epochs 40 - MAE: 3.0563773503379217\n",
            "Training Loss: 42.00088882446289, Validation Loss: 5.074608325958252\n",
            "2/2 [==============================] - 0s 22ms/step\n",
            "Training with 10 years, batch_size 32, epochs 60 - MAE: 3.5747937399243552\n",
            "Training Loss: 20.13931655883789, Validation Loss: 3.55505108833313\n",
            "2/2 [==============================] - 0s 24ms/step\n",
            "Training with 10 years, batch_size 32, epochs 80 - MAE: 3.8472086588541665\n",
            "Training Loss: 10.654060363769531, Validation Loss: 4.178384780883789\n",
            "2/2 [==============================] - 0s 35ms/step\n",
            "Training with 10 years, batch_size 64, epochs 20 - MAE: 96.25625852554563\n",
            "Training Loss: 131.365478515625, Validation Loss: 104.19589233398438\n",
            "2/2 [==============================] - 0s 33ms/step\n",
            "Training with 10 years, batch_size 64, epochs 40 - MAE: 35.59012373666915\n",
            "Training Loss: 80.60466003417969, Validation Loss: 46.30231857299805\n",
            "2/2 [==============================] - 0s 31ms/step\n",
            "Training with 10 years, batch_size 64, epochs 60 - MAE: 3.2573050847129217\n",
            "Training Loss: 47.40129089355469, Validation Loss: 7.6347880363464355\n",
            "2/2 [==============================] - 0s 32ms/step\n",
            "Training with 10 years, batch_size 64, epochs 80 - MAE: 4.149151272243923\n",
            "Training Loss: 42.384620666503906, Validation Loss: 6.540554523468018\n",
            "Best Configuration - Years: 20, Batch Size: 32, Epochs: 20 - MAE: 3.018435765826513\n"
          ]
        }
      ],
      "source": [
        "# Include overfitting parameter\n",
        "# No EARLY STOP\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "##########################################################################\n",
        "# Configuration for the LSTM model (Hyperband  - ******IMPORTANT******)\n",
        "##########################################################################\n",
        "config = {'units': 150, 'dropout': 0.2, 'learning_rate': 0.001}\n",
        "\n",
        "\n",
        "# Download the data\n",
        "start_date = '1974-01-01'\n",
        "end_date = '2024-08-10'\n",
        "all_stocks = ['BA', 'MSI', 'DE', 'SPGI']\n",
        "df = yf.download(all_stocks, start=start_date, end=end_date)['Adj Close']\n",
        "df = df.reset_index()\n",
        "\n",
        "# Prepare the BA stock data\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "df.set_index('Date', inplace=True)\n",
        "df = df['BA']\n",
        "\n",
        "# Preprocess Data\n",
        "def preprocess_data(df, window_size):\n",
        "    scaled_data = df.values.reshape(-1, 1)\n",
        "\n",
        "    X, y, dates = [], [], []\n",
        "    for i in range(window_size, len(df)):\n",
        "        X.append(scaled_data[i-window_size:i, 0])\n",
        "        y.append(scaled_data[i, 0])\n",
        "        dates.append(df.index[i])  # Collect dates for plotting\n",
        "\n",
        "    X, y = np.array(X), np.array(y)\n",
        "    X = np.reshape(X, (X.shape[0], X.shape[1], 1))  # Reshape for LSTM [samples, time steps, features]\n",
        "\n",
        "    return X, y, None, dates\n",
        "\n",
        "# Define LSTM Model\n",
        "def create_lstm_model(units, dropout, learning_rate):\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(units=units, return_sequences=False, input_shape=(90, 1)))\n",
        "    model.add(Dropout(dropout))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_absolute_error')\n",
        "    return model\n",
        "\n",
        "# Train and evaluate each model configuration\n",
        "def train_and_evaluate(X_train, y_train, X_val, y_val, X_test, y_test, hyperparams, epochs, batch_size):\n",
        "        model = create_lstm_model(hyperparams['units'], hyperparams['dropout'], hyperparams['learning_rate'])\n",
        "\n",
        "        # Adding EarlyStopping callback to prevent overfitting\n",
        "        early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "        history = model.fit(\n",
        "            X_train, y_train,\n",
        "            validation_data=(X_val, y_val),\n",
        "            epochs=epochs,\n",
        "            batch_size=batch_size,\n",
        "            verbose=0\n",
        "        )\n",
        "\n",
        "        train_loss = history.history['loss']\n",
        "        val_loss = history.history['val_loss']\n",
        "        predictions = model.predict(X_test)\n",
        "\n",
        "        mae = mean_absolute_error(y_test, predictions)\n",
        "\n",
        "        return mae, predictions, train_loss, val_loss\n",
        "\n",
        "\n",
        "\n",
        "# Possible values for epochs and batch size\n",
        "batch_sizes = [32, 64]\n",
        "epochs_list = [20, 40, 60, 80]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Varying training periods\n",
        "# Ex- If we are using a 20-year training period, we will use data from 20 years before\n",
        "# the end date of your training set. The end date of the training data is fixed at 2023-12-31,\n",
        "# and the start date is calculated by subtracting 20 years from this end date.\n",
        "\n",
        "training_years = [50, 40, 30, 20, 10]\n",
        "results = {}\n",
        "\n",
        "# Test data (2024)\n",
        "test_start = '2024-01-01'\n",
        "test_end = '2024-08-10'\n",
        "df_test = df[test_start:test_end]\n",
        "\n",
        "# Grid Search for best batch size and epochs\n",
        "best_config = None\n",
        "best_mae = float('inf')\n",
        "best_predictions = None\n",
        "best_dates = None\n",
        "\n",
        "for years in training_years:\n",
        "    train_end = pd.to_datetime('2023-12-31')\n",
        "    train_start = train_end - pd.DateOffset(years=years)\n",
        "    df_train = df[train_start:train_end]\n",
        "\n",
        "    # Split into training and validation sets (80/20 split)\n",
        "    val_size = int(len(df_train) * 0.2)\n",
        "    df_val = df_train[-val_size:]\n",
        "    df_train = df_train[:-val_size]\n",
        "\n",
        "    window_size = 90\n",
        "    X_train, y_train, scaler, train_dates = preprocess_data(df_train, window_size)\n",
        "    X_val, y_val, _, val_dates = preprocess_data(df_val, window_size)\n",
        "    X_test, y_test, _, test_dates = preprocess_data(df_test, window_size)\n",
        "\n",
        "    for batch_size in batch_sizes:\n",
        "        for epochs in epochs_list:\n",
        "            mae, predictions, train_loss, val_loss = train_and_evaluate(\n",
        "                X_train, y_train, X_val, y_val, X_test, y_test, config, epochs, batch_size)\n",
        "\n",
        "            if mae < best_mae:\n",
        "                best_mae = mae\n",
        "                best_config = {'years': years, 'batch_size': batch_size, 'epochs': epochs}\n",
        "                best_predictions = predictions\n",
        "                best_dates = test_dates\n",
        "            print(f\"Training with {years} years, batch_size {batch_size}, epochs {epochs} - MAE: {mae}\")\n",
        "            print(f\"Training Loss: {train_loss[-1]}, Validation Loss: {val_loss[-1]}\")\n",
        "\n",
        "print(f\"Best Configuration - Years: {best_config['years']}, Batch Size: {best_config['batch_size']}, Epochs: {best_config['epochs']} - MAE: {best_mae}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "uzUgdZ4Ag40n",
        "outputId": "22f5ef8e-b340-4d2b-b9e8-af7879569bc2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[*********************100%%**********************]  4 of 4 completed\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2/2 [==============================] - 0s 28ms/step\n",
            "MAE on Test Data: 4.011740306067089\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.24.1.min.js\"></script>                <div id=\"2146321f-b803-4ab3-b7f5-39c23c3b13d8\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"2146321f-b803-4ab3-b7f5-39c23c3b13d8\")) {                    Plotly.newPlot(                        \"2146321f-b803-4ab3-b7f5-39c23c3b13d8\",                        [{\"line\":{\"color\":\"black\",\"dash\":\"dash\"},\"mode\":\"lines\",\"name\":\"Actual Values\",\"x\":[\"2024-05-10T00:00:00\",\"2024-05-13T00:00:00\",\"2024-05-14T00:00:00\",\"2024-05-15T00:00:00\",\"2024-05-16T00:00:00\",\"2024-05-17T00:00:00\",\"2024-05-20T00:00:00\",\"2024-05-21T00:00:00\",\"2024-05-22T00:00:00\",\"2024-05-23T00:00:00\",\"2024-05-24T00:00:00\",\"2024-05-28T00:00:00\",\"2024-05-29T00:00:00\",\"2024-05-30T00:00:00\",\"2024-05-31T00:00:00\",\"2024-06-03T00:00:00\",\"2024-06-04T00:00:00\",\"2024-06-05T00:00:00\",\"2024-06-06T00:00:00\",\"2024-06-07T00:00:00\",\"2024-06-10T00:00:00\",\"2024-06-11T00:00:00\",\"2024-06-12T00:00:00\",\"2024-06-13T00:00:00\",\"2024-06-14T00:00:00\",\"2024-06-17T00:00:00\",\"2024-06-18T00:00:00\",\"2024-06-20T00:00:00\",\"2024-06-21T00:00:00\",\"2024-06-24T00:00:00\",\"2024-06-25T00:00:00\",\"2024-06-26T00:00:00\",\"2024-06-27T00:00:00\",\"2024-06-28T00:00:00\",\"2024-07-01T00:00:00\",\"2024-07-02T00:00:00\",\"2024-07-03T00:00:00\",\"2024-07-05T00:00:00\",\"2024-07-08T00:00:00\",\"2024-07-09T00:00:00\",\"2024-07-10T00:00:00\",\"2024-07-11T00:00:00\",\"2024-07-12T00:00:00\",\"2024-07-15T00:00:00\",\"2024-07-16T00:00:00\",\"2024-07-17T00:00:00\",\"2024-07-18T00:00:00\",\"2024-07-19T00:00:00\",\"2024-07-22T00:00:00\",\"2024-07-23T00:00:00\",\"2024-07-24T00:00:00\",\"2024-07-25T00:00:00\",\"2024-07-26T00:00:00\",\"2024-07-29T00:00:00\",\"2024-07-30T00:00:00\",\"2024-07-31T00:00:00\",\"2024-08-01T00:00:00\",\"2024-08-02T00:00:00\",\"2024-08-05T00:00:00\",\"2024-08-06T00:00:00\",\"2024-08-07T00:00:00\",\"2024-08-08T00:00:00\",\"2024-08-09T00:00:00\"],\"y\":[178.50999450683594,178.44000244140625,180.75999450683594,176.99000549316406,182.9600067138672,184.9499969482422,186.61000061035156,184.77999877929688,186.27999877929688,172.2100067138672,174.52000427246094,175.0800018310547,171.64999389648438,172.75,177.61000061035156,184.57000732421875,188.6199951171875,189.85000610351562,191.4199981689453,190.24000549316406,190.1199951171875,185.5,182.6699981689453,180.6999969482422,177.27000427246094,178.38999938964844,174.99000549316406,176.3000030517578,176.55999755859375,179.10000610351562,175.10000610351562,178.5,182.50999450683594,182.00999450683594,186.6999969482422,185.4199981689453,184.30999755859375,184.8300018310547,185.83999633789062,183.24000549316406,183.72999572753906,183.91000366210938,182.30999755859375,179.11000061035156,186.0500030517578,184.83999633789062,180.22999572753906,179.6699981689453,178.89999389648438,186.47999572753906,180.07000732421875,184.35000610351562,186.88999938964844,185.42999267578125,186.86000061035156,190.60000610351562,178.30999755859375,169.9499969482422,167.02999877929688,165.0399932861328,163.24000549316406,168.8699951171875,167.91000366210938],\"type\":\"scatter\"},{\"line\":{\"color\":\"blue\"},\"mode\":\"lines\",\"name\":\"Predictions (MAE: 4.0117)\",\"x\":[\"2024-05-10T00:00:00\",\"2024-05-13T00:00:00\",\"2024-05-14T00:00:00\",\"2024-05-15T00:00:00\",\"2024-05-16T00:00:00\",\"2024-05-17T00:00:00\",\"2024-05-20T00:00:00\",\"2024-05-21T00:00:00\",\"2024-05-22T00:00:00\",\"2024-05-23T00:00:00\",\"2024-05-24T00:00:00\",\"2024-05-28T00:00:00\",\"2024-05-29T00:00:00\",\"2024-05-30T00:00:00\",\"2024-05-31T00:00:00\",\"2024-06-03T00:00:00\",\"2024-06-04T00:00:00\",\"2024-06-05T00:00:00\",\"2024-06-06T00:00:00\",\"2024-06-07T00:00:00\",\"2024-06-10T00:00:00\",\"2024-06-11T00:00:00\",\"2024-06-12T00:00:00\",\"2024-06-13T00:00:00\",\"2024-06-14T00:00:00\",\"2024-06-17T00:00:00\",\"2024-06-18T00:00:00\",\"2024-06-20T00:00:00\",\"2024-06-21T00:00:00\",\"2024-06-24T00:00:00\",\"2024-06-25T00:00:00\",\"2024-06-26T00:00:00\",\"2024-06-27T00:00:00\",\"2024-06-28T00:00:00\",\"2024-07-01T00:00:00\",\"2024-07-02T00:00:00\",\"2024-07-03T00:00:00\",\"2024-07-05T00:00:00\",\"2024-07-08T00:00:00\",\"2024-07-09T00:00:00\",\"2024-07-10T00:00:00\",\"2024-07-11T00:00:00\",\"2024-07-12T00:00:00\",\"2024-07-15T00:00:00\",\"2024-07-16T00:00:00\",\"2024-07-17T00:00:00\",\"2024-07-18T00:00:00\",\"2024-07-19T00:00:00\",\"2024-07-22T00:00:00\",\"2024-07-23T00:00:00\",\"2024-07-24T00:00:00\",\"2024-07-25T00:00:00\",\"2024-07-26T00:00:00\",\"2024-07-29T00:00:00\",\"2024-07-30T00:00:00\",\"2024-07-31T00:00:00\",\"2024-08-01T00:00:00\",\"2024-08-02T00:00:00\",\"2024-08-05T00:00:00\",\"2024-08-06T00:00:00\",\"2024-08-07T00:00:00\",\"2024-08-08T00:00:00\",\"2024-08-09T00:00:00\"],\"y\":[179.5854949951172,174.07318115234375,176.18621826171875,180.1834716796875,172.4980926513672,183.5304412841797,184.8967742919922,183.4625701904297,181.6558380126953,184.30215454101562,163.07989501953125,173.49989318847656,176.2268524169922,165.93446350097656,172.19866943359375,179.48269653320312,185.34689331054688,188.10935974121094,187.387451171875,189.16917419433594,187.11282348632812,187.0813446044922,181.08157348632812,178.6284942626953,178.158203125,173.5322723388672,177.18922424316406,171.99673461914062,174.63314819335938,175.82791137695312,178.07469177246094,171.28855895996094,178.0978240966797,183.6841278076172,178.86463928222656,186.4970245361328,183.30674743652344,180.26901245117188,183.38046264648438,184.19410705566406,179.46328735351562,181.6999969482422,182.3697052001953,179.01731872558594,175.42947387695312,187.90243530273438,183.07293701171875,174.24514770507812,178.1398468017578,177.12059020996094,187.87464904785156,175.85414123535156,182.84898376464844,187.826904296875,181.3385467529297,185.08018493652344,190.74464416503906,169.9834747314453,162.72579956054688,166.13522338867188,162.53472900390625,160.88360595703125,171.8864288330078],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"title\":{\"text\":\"LSTM Model Predictions vs Actual Values\"},\"xaxis\":{\"title\":{\"text\":\"Date\"}},\"yaxis\":{\"title\":{\"text\":\"Stock Price\"}},\"legend\":{\"title\":{\"text\":\"Legend\"}},\"hovermode\":\"x unified\"},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('2146321f-b803-4ab3-b7f5-39c23c3b13d8');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# Best Configuration - Years: 30, Batch Size: 32, Epochs: 60 - MAE: 3.071416219075521\n",
        "\n",
        "\n",
        "##################################\n",
        "# Define the best configuration\n",
        "#################################\n",
        "# IMPORTANT\n",
        "#################################\n",
        "best_years = 30\n",
        "best_batch_size = 32\n",
        "best_epochs = 60\n",
        "\n",
        "##########################################################################\n",
        "# Configuration for the LSTM model (Hyperband  - ******IMPORTANT******)\n",
        "##########################################################################\n",
        "config = {'units': 150, 'dropout': 0.2, 'learning_rate': 0.001}\n",
        "\n",
        "\n",
        "# Download the data\n",
        "start_date = '1974-01-01'\n",
        "end_date = '2024-08-10'\n",
        "all_stocks = ['BA', 'MSI', 'DE', 'SPGI']\n",
        "df = yf.download(all_stocks, start=start_date, end=end_date)['Adj Close']\n",
        "df = df.reset_index()\n",
        "\n",
        "test_start = '2024-01-01'\n",
        "test_end = '2024-08-10'\n",
        "\n",
        "# Prepare the BA stock data\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "df.set_index('Date', inplace=True)\n",
        "df = df['BA']\n",
        "\n",
        "# Preprocess Data\n",
        "def preprocess_data(df, window_size):\n",
        "    #scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    #scaled_data = scaler.fit_transform(df.values.reshape(-1, 1))\n",
        "\n",
        "    scaled_data = df.values.reshape(-1, 1)\n",
        "\n",
        "    X, y, dates = [], [], []\n",
        "    for i in range(window_size, len(df)):\n",
        "        X.append(scaled_data[i-window_size:i, 0])\n",
        "        y.append(scaled_data[i, 0])\n",
        "        dates.append(df.index[i])  # Collect dates for plotting\n",
        "\n",
        "    X, y = np.array(X), np.array(y)\n",
        "    X = np.reshape(X, (X.shape[0], X.shape[1], 1))  # Reshape for LSTM [samples, time steps, features]\n",
        "\n",
        "    return X, y, None, dates\n",
        "\n",
        "# Define LSTM Model\n",
        "def create_lstm_model(units, dropout, learning_rate):\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(units=units, return_sequences=False, input_shape=(90, 1)))\n",
        "    model.add(Dropout(dropout))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_absolute_error')\n",
        "    return model\n",
        "\n",
        "# Train and evaluate each model configuration\n",
        "def train_and_evaluate(X_train, y_train, X_test, y_test, hyperparams, epochs, batch_size):\n",
        "        model = create_lstm_model(hyperparams['units'], hyperparams['dropout'], hyperparams['learning_rate'])\n",
        "        model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=0)\n",
        "        predictions = model.predict(X_test)\n",
        "        mae = mean_absolute_error(y_test, predictions)\n",
        "        return mae, predictions\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Prepare the data using the best configuration\n",
        "train_end = pd.to_datetime('2023-12-31')\n",
        "train_start = train_end - pd.DateOffset(years=best_years)\n",
        "df_train = df[train_start:train_end]\n",
        "df_test = df[test_start:test_end]\n",
        "\n",
        "# Prepare data\n",
        "window_size = 90\n",
        "X_train, y_train, scaler, train_dates = preprocess_data(df_train, window_size)\n",
        "X_test, y_test, _, test_dates = preprocess_data(df_test, window_size)\n",
        "\n",
        "# Train the model using the best configuration\n",
        "model = create_lstm_model(units=150, dropout=0.2, learning_rate=0.001)\n",
        "model.fit(X_train, y_train, epochs=best_epochs, batch_size=best_batch_size, verbose=0)\n",
        "\n",
        "# Make predictions on the test data\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "# Inverse transform the predictions and true values using the scaler\n",
        "#predictions = scaler.inverse_transform(predictions)\n",
        "#y_test = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
        "\n",
        "# Calculate MAE\n",
        "mae = mean_absolute_error(y_test, predictions)\n",
        "print(f\"MAE on Test Data: {mae}\")\n",
        "\n",
        "# Create a Plotly plot to show the predictions and actual values\n",
        "fig = go.Figure()\n",
        "\n",
        "# Plot actual values\n",
        "fig.add_trace(go.Scatter(x=test_dates, y=y_test.flatten(), mode='lines', name='Actual Values', line=dict(color='black', dash='dash')))\n",
        "\n",
        "# Plot predictions\n",
        "fig.add_trace(go.Scatter(x=test_dates, y=predictions.flatten(), mode='lines', name=f'Predictions (MAE: {mae:.4f})', line=dict(color='blue')))\n",
        "\n",
        "# Add layout details\n",
        "fig.update_layout(\n",
        "    title=\"LSTM Model Predictions vs Actual Values\",\n",
        "    xaxis_title=\"Date\",\n",
        "    yaxis_title=\"Stock Price\",\n",
        "    legend_title=\"Legend\",\n",
        "    hovermode=\"x unified\"\n",
        ")\n",
        "\n",
        "# Show the plot\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChUfptYrgEls"
      },
      "source": [
        "## Grid Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "FrfmPimggHyW",
        "outputId": "ac27ef7b-d302-4dbd-de29-ab311dd9d5fd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[*********************100%%**********************]  4 of 4 completed\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2/2 [==============================] - 0s 12ms/step\n",
            "Training with 50 years, batch_size 32, epochs 20 - MAE: 94.42692311604817\n",
            "Training Loss: 1.3999145030975342, Validation Loss: 122.9535903930664\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Training with 50 years, batch_size 32, epochs 40 - MAE: 81.76168375166635\n",
            "Training Loss: 1.455316185951233, Validation Loss: 109.84291076660156\n",
            "2/2 [==============================] - 0s 15ms/step\n",
            "Training with 50 years, batch_size 32, epochs 60 - MAE: 100.26780603802393\n",
            "Training Loss: 1.9175506830215454, Validation Loss: 128.67108154296875\n",
            "2/2 [==============================] - 0s 14ms/step\n",
            "Training with 50 years, batch_size 32, epochs 80 - MAE: 92.1963869125124\n",
            "Training Loss: 1.7465860843658447, Validation Loss: 120.56332397460938\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Training with 50 years, batch_size 64, epochs 20 - MAE: 78.41708107600137\n",
            "Training Loss: 1.09401273727417, Validation Loss: 106.55985260009766\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Training with 50 years, batch_size 64, epochs 40 - MAE: 77.7159927610367\n",
            "Training Loss: 1.2946217060089111, Validation Loss: 106.04508972167969\n",
            "2/2 [==============================] - 4s 14ms/step\n",
            "Training with 50 years, batch_size 64, epochs 60 - MAE: 81.08878495958116\n",
            "Training Loss: 1.463300347328186, Validation Loss: 109.37539672851562\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Training with 50 years, batch_size 64, epochs 80 - MAE: 70.61004117935423\n",
            "Training Loss: 1.2545396089553833, Validation Loss: 100.76057434082031\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Training with 40 years, batch_size 32, epochs 20 - MAE: 48.01431129092262\n",
            "Training Loss: 1.8561360836029053, Validation Loss: 100.84556579589844\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Training with 40 years, batch_size 32, epochs 40 - MAE: 49.8836413186694\n",
            "Training Loss: 1.7292386293411255, Validation Loss: 103.34072875976562\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Training with 40 years, batch_size 32, epochs 60 - MAE: 59.3490246969556\n",
            "Training Loss: 2.5801212787628174, Validation Loss: 109.1121826171875\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Training with 40 years, batch_size 32, epochs 80 - MAE: 42.567240397135414\n",
            "Training Loss: 2.3790979385375977, Validation Loss: 97.87438201904297\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Training with 40 years, batch_size 64, epochs 20 - MAE: 50.632222493489586\n",
            "Training Loss: 1.6917545795440674, Validation Loss: 103.49129486083984\n",
            "2/2 [==============================] - 0s 10ms/step\n",
            "Training with 40 years, batch_size 64, epochs 40 - MAE: 56.659346807570685\n",
            "Training Loss: 1.9712663888931274, Validation Loss: 108.61917877197266\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Training with 40 years, batch_size 64, epochs 60 - MAE: 48.70772395058284\n",
            "Training Loss: 1.6249560117721558, Validation Loss: 100.60262298583984\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Training with 40 years, batch_size 64, epochs 80 - MAE: 57.61171201675657\n",
            "Training Loss: 2.8905460834503174, Validation Loss: 109.31663513183594\n",
            "2/2 [==============================] - 0s 14ms/step\n",
            "Training with 30 years, batch_size 32, epochs 20 - MAE: 3.4892185756138394\n",
            "Training Loss: 3.8137998580932617, Validation Loss: 53.87411880493164\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Training with 30 years, batch_size 32, epochs 40 - MAE: 6.3619241865854415\n",
            "Training Loss: 3.4441652297973633, Validation Loss: 39.880096435546875\n",
            "2/2 [==============================] - 0s 14ms/step\n",
            "Training with 30 years, batch_size 32, epochs 60 - MAE: 9.366374666728671\n",
            "Training Loss: 4.998745918273926, Validation Loss: 75.83248138427734\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Training with 30 years, batch_size 32, epochs 80 - MAE: 8.762558952210442\n",
            "Training Loss: 4.366335391998291, Validation Loss: 63.33245849609375\n",
            "2/2 [==============================] - 0s 15ms/step\n",
            "Training with 30 years, batch_size 64, epochs 20 - MAE: 19.739744398328995\n",
            "Training Loss: 4.90300989151001, Validation Loss: 86.19187927246094\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Training with 30 years, batch_size 64, epochs 40 - MAE: 45.35640171595982\n",
            "Training Loss: 6.684443473815918, Validation Loss: 109.4881820678711\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Training with 30 years, batch_size 64, epochs 60 - MAE: 12.053467039078\n",
            "Training Loss: 4.138800621032715, Validation Loss: 69.30290985107422\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Training with 30 years, batch_size 64, epochs 80 - MAE: 8.724156939794147\n",
            "Training Loss: 3.449784278869629, Validation Loss: 44.61720657348633\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Training with 20 years, batch_size 32, epochs 20 - MAE: 6.691994318886409\n",
            "Training Loss: 7.798537254333496, Validation Loss: 9.904999732971191\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Training with 20 years, batch_size 32, epochs 40 - MAE: 7.229549831814236\n",
            "Training Loss: 9.13820743560791, Validation Loss: 7.845654010772705\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Training with 20 years, batch_size 32, epochs 60 - MAE: 9.798077537899925\n",
            "Training Loss: 9.211313247680664, Validation Loss: 10.6941499710083\n",
            "2/2 [==============================] - 4s 12ms/step\n",
            "Training with 20 years, batch_size 32, epochs 80 - MAE: 7.610337756928944\n",
            "Training Loss: 22.049358367919922, Validation Loss: 9.194171905517578\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Training with 20 years, batch_size 64, epochs 20 - MAE: 6.181001693483383\n",
            "Training Loss: 16.021757125854492, Validation Loss: 5.559223651885986\n",
            "2/2 [==============================] - 0s 14ms/step\n",
            "Training with 20 years, batch_size 64, epochs 40 - MAE: 6.36421155172681\n",
            "Training Loss: 7.552645206451416, Validation Loss: 9.995240211486816\n",
            "2/2 [==============================] - 0s 14ms/step\n",
            "Training with 20 years, batch_size 64, epochs 60 - MAE: 4.642419966440352\n",
            "Training Loss: 7.195827960968018, Validation Loss: 8.0298490524292\n",
            "2/2 [==============================] - 0s 11ms/step\n",
            "Training with 20 years, batch_size 64, epochs 80 - MAE: 6.464615110367063\n",
            "Training Loss: 6.75825834274292, Validation Loss: 12.798515319824219\n",
            "2/2 [==============================] - 0s 15ms/step\n",
            "Training with 10 years, batch_size 32, epochs 20 - MAE: 14.8127196781219\n",
            "Training Loss: 42.99951171875, Validation Loss: 12.23073673248291\n",
            "2/2 [==============================] - 0s 14ms/step\n",
            "Training with 10 years, batch_size 32, epochs 40 - MAE: 3.058441162109375\n",
            "Training Loss: 17.655637741088867, Validation Loss: 8.540093421936035\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Training with 10 years, batch_size 32, epochs 60 - MAE: 7.196675376286582\n",
            "Training Loss: 14.925933837890625, Validation Loss: 7.909217357635498\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Training with 10 years, batch_size 32, epochs 80 - MAE: 9.308069138299851\n",
            "Training Loss: 16.234962463378906, Validation Loss: 13.098267555236816\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Training with 10 years, batch_size 64, epochs 20 - MAE: 16.73516337076823\n",
            "Training Loss: 71.48731994628906, Validation Loss: 33.55276107788086\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Training with 10 years, batch_size 64, epochs 40 - MAE: 9.352145240420388\n",
            "Training Loss: 39.88129425048828, Validation Loss: 12.258262634277344\n",
            "2/2 [==============================] - 0s 13ms/step\n",
            "Training with 10 years, batch_size 64, epochs 60 - MAE: 8.257129487537203\n",
            "Training Loss: 23.69642448425293, Validation Loss: 10.079850196838379\n",
            "2/2 [==============================] - 0s 12ms/step\n",
            "Training with 10 years, batch_size 64, epochs 80 - MAE: 5.2581014481801835\n",
            "Training Loss: 25.169952392578125, Validation Loss: 7.874772548675537\n",
            "Best Configuration - Years: 10, Batch Size: 32, Epochs: 40 - MAE: 3.058441162109375\n"
          ]
        }
      ],
      "source": [
        "# Include overfitting parameter\n",
        "# No EARLY STOP\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "##########################################################################\n",
        "# Configuration for the LSTM model (GRID Search  - ******IMPORTANT******)\n",
        "##########################################################################\n",
        "config = {'units': 50, 'dropout': 0.1, 'learning_rate': 0.01}\n",
        "\n",
        "\n",
        "# Download the data\n",
        "start_date = '1974-01-01'\n",
        "end_date = '2024-08-10'\n",
        "all_stocks = ['BA', 'MSI', 'DE', 'SPGI']\n",
        "df = yf.download(all_stocks, start=start_date, end=end_date)['Adj Close']\n",
        "df = df.reset_index()\n",
        "\n",
        "# Prepare the BA stock data\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "df.set_index('Date', inplace=True)\n",
        "df = df['BA']\n",
        "\n",
        "# Preprocess Data\n",
        "def preprocess_data(df, window_size):\n",
        "    scaled_data = df.values.reshape(-1, 1)\n",
        "\n",
        "    X, y, dates = [], [], []\n",
        "    for i in range(window_size, len(df)):\n",
        "        X.append(scaled_data[i-window_size:i, 0])\n",
        "        y.append(scaled_data[i, 0])\n",
        "        dates.append(df.index[i])  # Collect dates for plotting\n",
        "\n",
        "    X, y = np.array(X), np.array(y)\n",
        "    X = np.reshape(X, (X.shape[0], X.shape[1], 1))  # Reshape for LSTM [samples, time steps, features]\n",
        "\n",
        "    return X, y, None, dates\n",
        "\n",
        "# Define LSTM Model\n",
        "def create_lstm_model(units, dropout, learning_rate):\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(units=units, return_sequences=False, input_shape=(90, 1)))\n",
        "    model.add(Dropout(dropout))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_absolute_error')\n",
        "    return model\n",
        "\n",
        "# Train and evaluate each model configuration\n",
        "def train_and_evaluate(X_train, y_train, X_val, y_val, X_test, y_test, hyperparams, epochs, batch_size):\n",
        "        model = create_lstm_model(hyperparams['units'], hyperparams['dropout'], hyperparams['learning_rate'])\n",
        "\n",
        "        # Adding EarlyStopping callback to prevent overfitting\n",
        "        early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "        history = model.fit(\n",
        "            X_train, y_train,\n",
        "            validation_data=(X_val, y_val),\n",
        "            epochs=epochs,\n",
        "            batch_size=batch_size,\n",
        "            verbose=0\n",
        "        )\n",
        "\n",
        "        train_loss = history.history['loss']\n",
        "        val_loss = history.history['val_loss']\n",
        "        predictions = model.predict(X_test)\n",
        "\n",
        "        mae = mean_absolute_error(y_test, predictions)\n",
        "\n",
        "        return mae, predictions, train_loss, val_loss\n",
        "\n",
        "\n",
        "\n",
        "# Possible values for epochs and batch size\n",
        "batch_sizes = [32, 64]\n",
        "epochs_list = [20, 40, 60, 80]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Varying training periods\n",
        "# Ex- If we are using a 20-year training period, we will use data from 20 years before\n",
        "# the end date of your training set. The end date of the training data is fixed at 2023-12-31,\n",
        "# and the start date is calculated by subtracting 20 years from this end date.\n",
        "\n",
        "training_years = [50, 40, 30, 20, 10]\n",
        "results = {}\n",
        "\n",
        "# Test data (2024)\n",
        "test_start = '2024-01-01'\n",
        "test_end = '2024-08-10'\n",
        "df_test = df[test_start:test_end]\n",
        "\n",
        "# Grid Search for best batch size and epochs\n",
        "best_config = None\n",
        "best_mae = float('inf')\n",
        "best_predictions = None\n",
        "best_dates = None\n",
        "\n",
        "for years in training_years:\n",
        "    train_end = pd.to_datetime('2023-12-31')\n",
        "    train_start = train_end - pd.DateOffset(years=years)\n",
        "    df_train = df[train_start:train_end]\n",
        "\n",
        "    # Split into training and validation sets (80/20 split)\n",
        "    val_size = int(len(df_train) * 0.2)\n",
        "    df_val = df_train[-val_size:]\n",
        "    df_train = df_train[:-val_size]\n",
        "\n",
        "    window_size = 90\n",
        "    X_train, y_train, scaler, train_dates = preprocess_data(df_train, window_size)\n",
        "    X_val, y_val, _, val_dates = preprocess_data(df_val, window_size)\n",
        "    X_test, y_test, _, test_dates = preprocess_data(df_test, window_size)\n",
        "\n",
        "    for batch_size in batch_sizes:\n",
        "        for epochs in epochs_list:\n",
        "            mae, predictions, train_loss, val_loss = train_and_evaluate(\n",
        "                X_train, y_train, X_val, y_val, X_test, y_test, config, epochs, batch_size)\n",
        "\n",
        "            if mae < best_mae:\n",
        "                best_mae = mae\n",
        "                best_config = {'years': years, 'batch_size': batch_size, 'epochs': epochs}\n",
        "                best_predictions = predictions\n",
        "                best_dates = test_dates\n",
        "            print(f\"Training with {years} years, batch_size {batch_size}, epochs {epochs} - MAE: {mae}\")\n",
        "            print(f\"Training Loss: {train_loss[-1]}, Validation Loss: {val_loss[-1]}\")\n",
        "\n",
        "print(f\"Best Configuration - Years: {best_config['years']}, Batch Size: {best_config['batch_size']}, Epochs: {best_config['epochs']} - MAE: {best_mae}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 596
        },
        "id": "pITh3gwughr0",
        "outputId": "1d522b80-c8c5-40b1-f145-c5355e35ed3a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[*********************100%%**********************]  4 of 4 completed\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2/2 [==============================] - 0s 33ms/step\n",
            "MAE on Test Data: 3.274235316685268\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.24.1.min.js\"></script>                <div id=\"1be79384-124b-4f16-a435-c465c709b29d\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"1be79384-124b-4f16-a435-c465c709b29d\")) {                    Plotly.newPlot(                        \"1be79384-124b-4f16-a435-c465c709b29d\",                        [{\"line\":{\"color\":\"black\",\"dash\":\"dash\"},\"mode\":\"lines\",\"name\":\"Actual Values\",\"x\":[\"2024-05-10T00:00:00\",\"2024-05-13T00:00:00\",\"2024-05-14T00:00:00\",\"2024-05-15T00:00:00\",\"2024-05-16T00:00:00\",\"2024-05-17T00:00:00\",\"2024-05-20T00:00:00\",\"2024-05-21T00:00:00\",\"2024-05-22T00:00:00\",\"2024-05-23T00:00:00\",\"2024-05-24T00:00:00\",\"2024-05-28T00:00:00\",\"2024-05-29T00:00:00\",\"2024-05-30T00:00:00\",\"2024-05-31T00:00:00\",\"2024-06-03T00:00:00\",\"2024-06-04T00:00:00\",\"2024-06-05T00:00:00\",\"2024-06-06T00:00:00\",\"2024-06-07T00:00:00\",\"2024-06-10T00:00:00\",\"2024-06-11T00:00:00\",\"2024-06-12T00:00:00\",\"2024-06-13T00:00:00\",\"2024-06-14T00:00:00\",\"2024-06-17T00:00:00\",\"2024-06-18T00:00:00\",\"2024-06-20T00:00:00\",\"2024-06-21T00:00:00\",\"2024-06-24T00:00:00\",\"2024-06-25T00:00:00\",\"2024-06-26T00:00:00\",\"2024-06-27T00:00:00\",\"2024-06-28T00:00:00\",\"2024-07-01T00:00:00\",\"2024-07-02T00:00:00\",\"2024-07-03T00:00:00\",\"2024-07-05T00:00:00\",\"2024-07-08T00:00:00\",\"2024-07-09T00:00:00\",\"2024-07-10T00:00:00\",\"2024-07-11T00:00:00\",\"2024-07-12T00:00:00\",\"2024-07-15T00:00:00\",\"2024-07-16T00:00:00\",\"2024-07-17T00:00:00\",\"2024-07-18T00:00:00\",\"2024-07-19T00:00:00\",\"2024-07-22T00:00:00\",\"2024-07-23T00:00:00\",\"2024-07-24T00:00:00\",\"2024-07-25T00:00:00\",\"2024-07-26T00:00:00\",\"2024-07-29T00:00:00\",\"2024-07-30T00:00:00\",\"2024-07-31T00:00:00\",\"2024-08-01T00:00:00\",\"2024-08-02T00:00:00\",\"2024-08-05T00:00:00\",\"2024-08-06T00:00:00\",\"2024-08-07T00:00:00\",\"2024-08-08T00:00:00\",\"2024-08-09T00:00:00\"],\"y\":[178.50999450683594,178.44000244140625,180.75999450683594,176.99000549316406,182.9600067138672,184.9499969482422,186.61000061035156,184.77999877929688,186.27999877929688,172.2100067138672,174.52000427246094,175.0800018310547,171.64999389648438,172.75,177.61000061035156,184.57000732421875,188.6199951171875,189.85000610351562,191.4199981689453,190.24000549316406,190.1199951171875,185.5,182.6699981689453,180.6999969482422,177.27000427246094,178.38999938964844,174.99000549316406,176.3000030517578,176.55999755859375,179.10000610351562,175.10000610351562,178.5,182.50999450683594,182.00999450683594,186.6999969482422,185.4199981689453,184.30999755859375,184.8300018310547,185.83999633789062,183.24000549316406,183.72999572753906,183.91000366210938,182.30999755859375,179.11000061035156,186.0500030517578,184.83999633789062,180.22999572753906,179.6699981689453,178.89999389648438,186.47999572753906,180.07000732421875,184.35000610351562,186.88999938964844,185.42999267578125,186.86000061035156,190.60000610351562,178.30999755859375,169.9499969482422,167.02999877929688,165.0399932861328,163.24000549316406,168.8699951171875,167.91000366210938],\"type\":\"scatter\"},{\"line\":{\"color\":\"blue\"},\"mode\":\"lines\",\"name\":\"Predictions (MAE: 3.2742)\",\"x\":[\"2024-05-10T00:00:00\",\"2024-05-13T00:00:00\",\"2024-05-14T00:00:00\",\"2024-05-15T00:00:00\",\"2024-05-16T00:00:00\",\"2024-05-17T00:00:00\",\"2024-05-20T00:00:00\",\"2024-05-21T00:00:00\",\"2024-05-22T00:00:00\",\"2024-05-23T00:00:00\",\"2024-05-24T00:00:00\",\"2024-05-28T00:00:00\",\"2024-05-29T00:00:00\",\"2024-05-30T00:00:00\",\"2024-05-31T00:00:00\",\"2024-06-03T00:00:00\",\"2024-06-04T00:00:00\",\"2024-06-05T00:00:00\",\"2024-06-06T00:00:00\",\"2024-06-07T00:00:00\",\"2024-06-10T00:00:00\",\"2024-06-11T00:00:00\",\"2024-06-12T00:00:00\",\"2024-06-13T00:00:00\",\"2024-06-14T00:00:00\",\"2024-06-17T00:00:00\",\"2024-06-18T00:00:00\",\"2024-06-20T00:00:00\",\"2024-06-21T00:00:00\",\"2024-06-24T00:00:00\",\"2024-06-25T00:00:00\",\"2024-06-26T00:00:00\",\"2024-06-27T00:00:00\",\"2024-06-28T00:00:00\",\"2024-07-01T00:00:00\",\"2024-07-02T00:00:00\",\"2024-07-03T00:00:00\",\"2024-07-05T00:00:00\",\"2024-07-08T00:00:00\",\"2024-07-09T00:00:00\",\"2024-07-10T00:00:00\",\"2024-07-11T00:00:00\",\"2024-07-12T00:00:00\",\"2024-07-15T00:00:00\",\"2024-07-16T00:00:00\",\"2024-07-17T00:00:00\",\"2024-07-18T00:00:00\",\"2024-07-19T00:00:00\",\"2024-07-22T00:00:00\",\"2024-07-23T00:00:00\",\"2024-07-24T00:00:00\",\"2024-07-25T00:00:00\",\"2024-07-26T00:00:00\",\"2024-07-29T00:00:00\",\"2024-07-30T00:00:00\",\"2024-07-31T00:00:00\",\"2024-08-01T00:00:00\",\"2024-08-02T00:00:00\",\"2024-08-05T00:00:00\",\"2024-08-06T00:00:00\",\"2024-08-07T00:00:00\",\"2024-08-08T00:00:00\",\"2024-08-09T00:00:00\"],\"y\":[181.39749145507812,176.96563720703125,177.79000854492188,181.1905975341797,175.2400665283203,184.29795837402344,185.64144897460938,186.16079711914062,183.4964599609375,185.724365234375,166.70350646972656,173.96804809570312,175.6892852783203,169.54638671875,173.01820373535156,179.46205139160156,186.76809692382812,189.6305694580078,189.319091796875,190.71197509765625,188.91815185546875,188.88351440429688,183.2340545654297,180.60491943359375,179.4063262939453,175.43898010253906,178.26461791992188,173.54190063476562,176.1505126953125,176.62002563476562,179.57740783691406,173.41928100585938,179.0076446533203,183.90753173828125,181.33880615234375,187.4060516357422,184.68136596679688,182.73077392578125,184.24298095703125,185.48085021972656,181.70529174804688,182.96884155273438,183.45660400390625,181.06698608398438,177.25350952148438,187.68853759765625,184.48834228515625,177.337646484375,178.53189086914062,178.0631103515625,188.4043731689453,178.0579071044922,184.14593505859375,187.55799865722656,183.9939422607422,186.2649383544922,191.029052734375,173.73155212402344,164.79837036132812,165.28343200683594,163.53773498535156,162.38832092285156,171.7071990966797],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"title\":{\"text\":\"LSTM Model Predictions vs Actual Values\"},\"xaxis\":{\"title\":{\"text\":\"Date\"}},\"yaxis\":{\"title\":{\"text\":\"Stock Price\"}},\"legend\":{\"title\":{\"text\":\"Legend\"}},\"hovermode\":\"x unified\"},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('1be79384-124b-4f16-a435-c465c709b29d');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "# Best Configuration - Years: 20, Batch Size: 64, Epochs: 40 - MAE: 3.7216394818018355\n",
        "\n",
        "\n",
        "##################################\n",
        "# Define the best configuration\n",
        "#################################\n",
        "# IMPORTANT\n",
        "#################################\n",
        "best_years = 20\n",
        "best_batch_size = 64\n",
        "best_epochs = 40\n",
        "\n",
        "##########################################################################\n",
        "# Configuration for the LSTM model (GRID Search  - ******IMPORTANT******)\n",
        "##########################################################################\n",
        "config = {'units': 50, 'dropout': 0.1, 'learning_rate': 0.01}\n",
        "\n",
        "# Download the data\n",
        "start_date = '1974-01-01'\n",
        "end_date = '2024-08-10'\n",
        "all_stocks = ['BA', 'MSI', 'DE', 'SPGI']\n",
        "df = yf.download(all_stocks, start=start_date, end=end_date)['Adj Close']\n",
        "df = df.reset_index()\n",
        "\n",
        "test_start = '2024-01-01'\n",
        "test_end = '2024-08-10'\n",
        "\n",
        "# Prepare the BA stock data\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "df.set_index('Date', inplace=True)\n",
        "df = df['BA']\n",
        "\n",
        "# Preprocess Data\n",
        "def preprocess_data(df, window_size):\n",
        "    #scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "    #scaled_data = scaler.fit_transform(df.values.reshape(-1, 1))\n",
        "\n",
        "    scaled_data = df.values.reshape(-1, 1)\n",
        "\n",
        "    X, y, dates = [], [], []\n",
        "    for i in range(window_size, len(df)):\n",
        "        X.append(scaled_data[i-window_size:i, 0])\n",
        "        y.append(scaled_data[i, 0])\n",
        "        dates.append(df.index[i])  # Collect dates for plotting\n",
        "\n",
        "    X, y = np.array(X), np.array(y)\n",
        "    X = np.reshape(X, (X.shape[0], X.shape[1], 1))  # Reshape for LSTM [samples, time steps, features]\n",
        "\n",
        "    return X, y, None, dates\n",
        "\n",
        "# Define LSTM Model\n",
        "def create_lstm_model(units, dropout, learning_rate):\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(units=units, return_sequences=False, input_shape=(90, 1)))\n",
        "    model.add(Dropout(dropout))\n",
        "    model.add(Dense(1))\n",
        "    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mean_absolute_error')\n",
        "    return model\n",
        "\n",
        "# Train and evaluate each model configuration\n",
        "def train_and_evaluate(X_train, y_train, X_test, y_test, hyperparams, epochs, batch_size):\n",
        "        model = create_lstm_model(hyperparams['units'], hyperparams['dropout'], hyperparams['learning_rate'])\n",
        "        model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size, verbose=0)\n",
        "        predictions = model.predict(X_test)\n",
        "        mae = mean_absolute_error(y_test, predictions)\n",
        "        return mae, predictions\n",
        "\n",
        "# Configuration for the LSTM model\n",
        "config = {'units': 150, 'dropout': 0.2, 'learning_rate': 0.001}\n",
        "\n",
        "\n",
        "\n",
        "# Prepare the data using the best configuration\n",
        "train_end = pd.to_datetime('2023-12-31')\n",
        "train_start = train_end - pd.DateOffset(years=best_years)\n",
        "df_train = df[train_start:train_end]\n",
        "df_test = df[test_start:test_end]\n",
        "\n",
        "# Prepare data\n",
        "window_size = 90\n",
        "X_train, y_train, scaler, train_dates = preprocess_data(df_train, window_size)\n",
        "X_test, y_test, _, test_dates = preprocess_data(df_test, window_size)\n",
        "\n",
        "# Train the model using the best configuration\n",
        "model = create_lstm_model(units=150, dropout=0.2, learning_rate=0.001)\n",
        "model.fit(X_train, y_train, epochs=best_epochs, batch_size=best_batch_size, verbose=0)\n",
        "\n",
        "# Make predictions on the test data\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "# Inverse transform the predictions and true values using the scaler\n",
        "#predictions = scaler.inverse_transform(predictions)\n",
        "#y_test = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
        "\n",
        "# Calculate MAE\n",
        "mae = mean_absolute_error(y_test, predictions)\n",
        "print(f\"MAE on Test Data: {mae}\")\n",
        "\n",
        "# Create a Plotly plot to show the predictions and actual values\n",
        "fig = go.Figure()\n",
        "\n",
        "# Plot actual values\n",
        "fig.add_trace(go.Scatter(x=test_dates, y=y_test.flatten(), mode='lines', name='Actual Values', line=dict(color='black', dash='dash')))\n",
        "\n",
        "# Plot predictions\n",
        "fig.add_trace(go.Scatter(x=test_dates, y=predictions.flatten(), mode='lines', name=f'Predictions (MAE: {mae:.4f})', line=dict(color='blue')))\n",
        "\n",
        "# Add layout details\n",
        "fig.update_layout(\n",
        "    title=\"LSTM Model Predictions vs Actual Values\",\n",
        "    xaxis_title=\"Date\",\n",
        "    yaxis_title=\"Stock Price\",\n",
        "    legend_title=\"Legend\",\n",
        "    hovermode=\"x unified\"\n",
        ")\n",
        "\n",
        "# Show the plot\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NkTo26Ev6kjN"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
